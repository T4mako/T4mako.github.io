<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.66" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://T4mako.github.io/code/python/Machine%20Learning/Pytorch/6%E3%80%81Pytorch%20%E4%B8%AD%E9%98%B6%20API.html"><meta property="og:site_name" content="T4mako"><meta property="og:title" content="6、Pytorch 中阶 API"><meta property="og:description" content="6、Pytorch 中阶 API 6.1、Dataset 和 DataLoader Pytorch 使用 Dataset 和 DataLoader 这两个工具类来构建数据管道。它们的作用是将数据整理成适合训练模型的格式，一个 batch 一个 batch 的取出给模型 6.1.1、Dataset 和 DataLoader 原理 获取一个 batch 的步骤 假定数据集的特征和标签分别表示为张量X和Y，数据集可以表+示为 (X,Y), 假定 batch 大小为 m"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="T4mako"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"6、Pytorch 中阶 API","image":[""],"dateModified":null,"author":[{"@type":"Person","name":"T4mako","url":"https://github.com/T4mako/T4mako.github.io"}]}</script><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin=""><link href="https://fonts.googleapis.com/css2?family=Zen+Maru+Gothic:wght@500&display=swap" rel="stylesheet"><link rel="alternate" type="application/rss+xml" href="https://T4mako.github.io/rss.xml" title="T4mako RSS Feed"><title>6、Pytorch 中阶 API | T4mako</title><meta name="description" content="6、Pytorch 中阶 API 6.1、Dataset 和 DataLoader Pytorch 使用 Dataset 和 DataLoader 这两个工具类来构建数据管道。它们的作用是将数据整理成适合训练模型的格式，一个 batch 一个 batch 的取出给模型 6.1.1、Dataset 和 DataLoader 原理 获取一个 batch 的步骤 假定数据集的特征和标签分别表示为张量X和Y，数据集可以表+示为 (X,Y), 假定 batch 大小为 m">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/assets/style-c43249a7.css" as="style"><link rel="stylesheet" href="/assets/style-c43249a7.css">
    <link rel="modulepreload" href="/assets/app-392a5c85.js"><link rel="modulepreload" href="/assets/6、Pytorch 中阶 API.html-d48a0b3d.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/assets/6、Pytorch 中阶 API.html-12d00ec0.js"><link rel="prefetch" href="/assets/index.html-28de6f14.js" as="script"><link rel="prefetch" href="/assets/index.html-f5f264d6.js" as="script"><link rel="prefetch" href="/assets/index.html-30a1b7c2.js" as="script"><link rel="prefetch" href="/assets/Elasticsearch.html-cdfa620a.js" as="script"><link rel="prefetch" href="/assets/MongoDB.html-26953f36.js" as="script"><link rel="prefetch" href="/assets/MySQL基础.html-142c566d.js" as="script"><link rel="prefetch" href="/assets/Redis.html-00ca8519.js" as="script"><link rel="prefetch" href="/assets/IDEA、Eclipse快捷键.html-15e3dc66.js" as="script"><link rel="prefetch" href="/assets/JDBC.html-8ec2fd12.js" as="script"><link rel="prefetch" href="/assets/JVM.html-fa0d3015.js" as="script"><link rel="prefetch" href="/assets/JVM入门.html-5f1781e4.js" as="script"><link rel="prefetch" href="/assets/JWT.html-7c0fd9d7.js" as="script"><link rel="prefetch" href="/assets/JavaWeb基础.html-af51d1ad.js" as="script"><link rel="prefetch" href="/assets/Maven基础.html-ad80dbfe.js" as="script"><link rel="prefetch" href="/assets/MyBatis.html-1642006c.js" as="script"><link rel="prefetch" href="/assets/MybatisPlus.html-07f9ea4d.js" as="script"><link rel="prefetch" href="/assets/RabbitMQ.html-5aae586f.js" as="script"><link rel="prefetch" href="/assets/Shiro.html-fafd6510.js" as="script"><link rel="prefetch" href="/assets/SpringBoot.html-6d206ed6.js" as="script"><link rel="prefetch" href="/assets/SpringBoot常用注解总结.html-c19e968e.js" as="script"><link rel="prefetch" href="/assets/SpringBoot自动装配原理.html-83356abe.js" as="script"><link rel="prefetch" href="/assets/SpringCloud基础.html-081659a3.js" as="script"><link rel="prefetch" href="/assets/SpringCloud基础2024.html-5c5733dc.js" as="script"><link rel="prefetch" href="/assets/SpringMVC.html-20c731bf.js" as="script"><link rel="prefetch" href="/assets/Spring基础.html-7fdff257.js" as="script"><link rel="prefetch" href="/assets/index.html-8b1856b4.js" as="script"><link rel="prefetch" href="/assets/Matplotlib.html-d2aba437.js" as="script"><link rel="prefetch" href="/assets/Numpy.html-ffba3c01.js" as="script"><link rel="prefetch" href="/assets/Pandas.html-febe9de7.js" as="script"><link rel="prefetch" href="/assets/python 基础语法.html-d33402d6.js" as="script"><link rel="prefetch" href="/assets/python 项目注意事项.html-f13637d7.js" as="script"><link rel="prefetch" href="/assets/Ajax.html-73e0d2ee.js" as="script"><link rel="prefetch" href="/assets/Axios.html-6a0f7031.js" as="script"><link rel="prefetch" href="/assets/CSS3.html-fbf9bd49.js" as="script"><link rel="prefetch" href="/assets/HTML.html-dba9796c.js" as="script"><link rel="prefetch" href="/assets/Node.js笔记.html-edd6db0d.js" as="script"><link rel="prefetch" href="/assets/Promise.html-83aba73d.js" as="script"><link rel="prefetch" href="/assets/Git.html-ef4b5f14.js" as="script"><link rel="prefetch" href="/assets/Markdown语法基础.html-2825f872.js" as="script"><link rel="prefetch" href="/assets/操作系统.html-8bed0f34.js" as="script"><link rel="prefetch" href="/assets/数据结构.html-6f1a0269.js" as="script"><link rel="prefetch" href="/assets/正则表达式.html-be164830.js" as="script"><link rel="prefetch" href="/assets/计算机组成原理.html-2668b30e.js" as="script"><link rel="prefetch" href="/assets/计算机网络.html-b11deb88.js" as="script"><link rel="prefetch" href="/assets/设计模式.html-35cadb50.js" as="script"><link rel="prefetch" href="/assets/面向对象设计基本原则.html-edb429fa.js" as="script"><link rel="prefetch" href="/assets/Docker.html-9f4e5ab9.js" as="script"><link rel="prefetch" href="/assets/GitHub Actions.html-bc1f64fb.js" as="script"><link rel="prefetch" href="/assets/基于centos部署java项目.html-b44ac3d3.js" as="script"><link rel="prefetch" href="/assets/一些插件.html-7333e5f0.js" as="script"><link rel="prefetch" href="/assets/快捷键.html-c14413e6.js" as="script"><link rel="prefetch" href="/assets/CSGO饰品图.html-41566c4f.js" as="script"><link rel="prefetch" href="/assets/基础.html-376a9d53.js" as="script"><link rel="prefetch" href="/assets/快捷键.html-ff145bec.js" as="script"><link rel="prefetch" href="/assets/Guitar.html-88bc41a7.js" as="script"><link rel="prefetch" href="/assets/hlae.html-5be4d809.js" as="script"><link rel="prefetch" href="/assets/settings.html-ed8baefd.js" as="script"><link rel="prefetch" href="/assets/Japanese.html-1b499c3a.js" as="script"><link rel="prefetch" href="/assets/index.html-71b38232.js" as="script"><link rel="prefetch" href="/assets/index.html-648f386b.js" as="script"><link rel="prefetch" href="/assets/30天JavaScript挑战.html-90b1a895.js" as="script"><link rel="prefetch" href="/assets/高频 SQL 50 题（基础版）.html-405179ee.js" as="script"><link rel="prefetch" href="/assets/多线程.html-46ffbab2.js" as="script"><link rel="prefetch" href="/assets/noob-Rg-better.html-cc14aa26.js" as="script"><link rel="prefetch" href="/assets/noob-Rg.html-3a2fdcc8.js" as="script"><link rel="prefetch" href="/assets/1、JavaScript基础语法笔记.html-69b3cd61.js" as="script"><link rel="prefetch" href="/assets/2、WebAPIs笔记.html-a8c50328.js" as="script"><link rel="prefetch" href="/assets/3、JavaScript进阶.html-180a19d8.js" as="script"><link rel="prefetch" href="/assets/1、TypeScript语言简介.html-ecee5a2a.js" as="script"><link rel="prefetch" href="/assets/2、基本用法.html-8e7f792b.js" as="script"><link rel="prefetch" href="/assets/3、any，unknown ，never 类型.html-92d935e7.js" as="script"><link rel="prefetch" href="/assets/4、系统类型.html-564a8c20.js" as="script"><link rel="prefetch" href="/assets/5、数组.html-74cacfb8.js" as="script"><link rel="prefetch" href="/assets/6、元祖.html-df4609ec.js" as="script"><link rel="prefetch" href="/assets/7、symbol 类型.html-459544bf.js" as="script"><link rel="prefetch" href="/assets/8、函数.html-c5c268c2.js" as="script"><link rel="prefetch" href="/assets/index.html-9b0abc8d.js" as="script"><link rel="prefetch" href="/assets/leetcode228_汇总区间.html-cddb8f7a.js" as="script"><link rel="prefetch" href="/assets/leetcode_001_两数之和.html-4d2935f3.js" as="script"><link rel="prefetch" href="/assets/leetcode_002_两数相加.html-fe20f202.js" as="script"><link rel="prefetch" href="/assets/leetcode_003_无重复字符的最长子串.html-267e65ef.js" as="script"><link rel="prefetch" href="/assets/leetcode_004_寻找两个正序数组的中位数.html-9efd45da.js" as="script"><link rel="prefetch" href="/assets/leetcode_005_最长回文子串.html-39e1fea9.js" as="script"><link rel="prefetch" href="/assets/leetcode_006_N 字形变换.html-b4f920fe.js" as="script"><link rel="prefetch" href="/assets/leetcode_007_整数反转.html-4dd1c7a5.js" as="script"><link rel="prefetch" href="/assets/leetcode_008_字符串转换整数 (atoi).html-fa0a27da.js" as="script"><link rel="prefetch" href="/assets/leetcode_009_回文数.html-55377a9f.js" as="script"><link rel="prefetch" href="/assets/leetcode_010_正则表达式匹配.html-674b100d.js" as="script"><link rel="prefetch" href="/assets/leetcode_011_盛最多水的容器.html-c8a38d1c.js" as="script"><link rel="prefetch" href="/assets/leetcode_012_整数转罗马数字.html-6b38adef.js" as="script"><link rel="prefetch" href="/assets/leetcode_013_罗马数字转整数.html-22c1adfb.js" as="script"><link rel="prefetch" href="/assets/leetcode_014_最长公共前缀.html-c172cec6.js" as="script"><link rel="prefetch" href="/assets/leetcode_015_三数之和.html-875a5ed0.js" as="script"><link rel="prefetch" href="/assets/leetcode_016_最接近的三数之和.html-b931e3c9.js" as="script"><link rel="prefetch" href="/assets/leetcode_017_电话号码的字母组合.html-6d899101.js" as="script"><link rel="prefetch" href="/assets/leetcode_018_四数之和.html-087b71dc.js" as="script"><link rel="prefetch" href="/assets/leetcode_019_删除链表的倒数第 N 个结点.html-8241fe9c.js" as="script"><link rel="prefetch" href="/assets/leetcode_020_有效的括号.html-e07e2181.js" as="script"><link rel="prefetch" href="/assets/leetcode_021_合并两个有序链表.html-3e5fdc85.js" as="script"><link rel="prefetch" href="/assets/leetcode_022_括号生成.html-34027bae.js" as="script"><link rel="prefetch" href="/assets/leetcode_023_合并 K 个升序链表.html-2830aabf.js" as="script"><link rel="prefetch" href="/assets/leetcode_024_两两交换链表中的节点.html-aa44f045.js" as="script"><link rel="prefetch" href="/assets/leetcode_025_K 个一组翻转链表.html-1cb0de2d.js" as="script"><link rel="prefetch" href="/assets/leetcode_026_删除有序数组中的重复项.html-bf359f60.js" as="script"><link rel="prefetch" href="/assets/leetcode_027_移除元素.html-529831e9.js" as="script"><link rel="prefetch" href="/assets/leetcode_028_找出字符串中第一个匹配项的下标.html-dc2e4f17.js" as="script"><link rel="prefetch" href="/assets/leetcode_029_两数相除.html-03436b7a.js" as="script"><link rel="prefetch" href="/assets/leetcode_030_串联所有单词的子串.html-faa41612.js" as="script"><link rel="prefetch" href="/assets/leetcode_031_下一个排列.html-1bf62ff0.js" as="script"><link rel="prefetch" href="/assets/leetcode_032_最长有效括号.html-dbd4329f.js" as="script"><link rel="prefetch" href="/assets/leetcode_033_搜索旋转排序数组.html-b3f9f227.js" as="script"><link rel="prefetch" href="/assets/leetcode_034_在排序数组中查找元素的第一个和最后一个位置.html-df6d34be.js" as="script"><link rel="prefetch" href="/assets/leetcode_035_搜索插入位置.html-f3871315.js" as="script"><link rel="prefetch" href="/assets/leetcode_036_有效的数独.html-5f2bc80d.js" as="script"><link rel="prefetch" href="/assets/leetcode_038_外观数列.html-6e3a5b1a.js" as="script"><link rel="prefetch" href="/assets/leetcode_039_ 组合总和.html-b0a7cd1a.js" as="script"><link rel="prefetch" href="/assets/leetcode_040_ 组合总和II.html-87cd31b2.js" as="script"><link rel="prefetch" href="/assets/leetcode_041_ 缺失的第一个正数.html-c08437a5.js" as="script"><link rel="prefetch" href="/assets/leetcode_042_接雨水.html-4f929eb5.js" as="script"><link rel="prefetch" href="/assets/leetcode_043_字符串相乘.html-81486896.js" as="script"><link rel="prefetch" href="/assets/leetcode_045_跳跃游戏II.html-a0a07e13.js" as="script"><link rel="prefetch" href="/assets/leetcode_046_全排列.html-8e33255c.js" as="script"><link rel="prefetch" href="/assets/leetcode_047_全排列II.html-bc7cb387.js" as="script"><link rel="prefetch" href="/assets/leetcode_048_旋转图像.html-ec0a782c.js" as="script"><link rel="prefetch" href="/assets/leetcode_049_字母异位词分组.html-dc410cab.js" as="script"><link rel="prefetch" href="/assets/leetcode_050_Pow(x_n).html-29adea25.js" as="script"><link rel="prefetch" href="/assets/leetcode_051_N 皇后.html-54792019.js" as="script"><link rel="prefetch" href="/assets/leetcode_052_N 皇后 II.html-cae35ca8.js" as="script"><link rel="prefetch" href="/assets/leetcode_053_最大子数组和.html-f84751f2.js" as="script"><link rel="prefetch" href="/assets/leetcode_054_螺旋矩阵.html-7c2f26d1.js" as="script"><link rel="prefetch" href="/assets/leetcode_055_跳跃游戏.html-196b4f14.js" as="script"><link rel="prefetch" href="/assets/leetcode_056_合并区间.html-3c8d49b7.js" as="script"><link rel="prefetch" href="/assets/leetcode_058_最后一个单词的长度.html-c74c97db.js" as="script"><link rel="prefetch" href="/assets/leetcode_061_旋转链表.html-c1b9f0bb.js" as="script"><link rel="prefetch" href="/assets/leetcode_062_不同路径.html-5571caa7.js" as="script"><link rel="prefetch" href="/assets/leetcode_063_不同路径II.html-761d387b.js" as="script"><link rel="prefetch" href="/assets/leetcode_064_最小路径和.html-cecd8a72.js" as="script"><link rel="prefetch" href="/assets/leetcode_066_加一.html-12eefde8.js" as="script"><link rel="prefetch" href="/assets/leetcode_067_二进制求和.html-555cfc00.js" as="script"><link rel="prefetch" href="/assets/leetcode_068_文本左右对齐.html-7a5c575d.js" as="script"><link rel="prefetch" href="/assets/leetcode_069_x的平方根.html-b20ef54c.js" as="script"><link rel="prefetch" href="/assets/leetcode_070_爬楼梯.html-d52a7e95.js" as="script"><link rel="prefetch" href="/assets/leetcode_072_编辑距离.html-00a22365.js" as="script"><link rel="prefetch" href="/assets/leetcode_073_矩阵置零.html-2bf295e3.js" as="script"><link rel="prefetch" href="/assets/leetcode_075_颜色分类.html-5d29d6eb.js" as="script"><link rel="prefetch" href="/assets/leetcode_076_最小覆盖子串 copy.html-dba3ee02.js" as="script"><link rel="prefetch" href="/assets/leetcode_076_最小覆盖子串.html-58de2813.js" as="script"><link rel="prefetch" href="/assets/leetcode_079_单词搜索.html-c3ecad05.js" as="script"><link rel="prefetch" href="/assets/leetcode_080_删除有序数组中的重复项.html-ad954572.js" as="script"><link rel="prefetch" href="/assets/leetcode_082_删除排序链表中的重复元素 II.html-ffc51573.js" as="script"><link rel="prefetch" href="/assets/leetcode_083_删除排序链表中的重复元素.html-70bf5bb7.js" as="script"><link rel="prefetch" href="/assets/leetcode_086_分隔链表.html-79aea5ef.js" as="script"><link rel="prefetch" href="/assets/leetcode_088_合并两个有序数组.html-ba540966.js" as="script"><link rel="prefetch" href="/assets/leetcode_092_反转链表 II.html-23caaca0.js" as="script"><link rel="prefetch" href="/assets/leetcode_094_二叉树的中序遍历.html-8288cdbe.js" as="script"><link rel="prefetch" href="/assets/leetcode_095_不同的二叉搜索树 II.html-47652036.js" as="script"><link rel="prefetch" href="/assets/leetcode_096_不同的二叉搜索树.html-ae75ff17.js" as="script"><link rel="prefetch" href="/assets/leetcode_098_验证二叉搜索树.html-03e5a7b1.js" as="script"><link rel="prefetch" href="/assets/leetcode_1004_最大连续1的个数 III.html-5533e255.js" as="script"><link rel="prefetch" href="/assets/leetcode_100_相同的树.html-b3027eaa.js" as="script"><link rel="prefetch" href="/assets/leetcode_101_对称二叉树.html-ce76548e.js" as="script"><link rel="prefetch" href="/assets/leetcode_102_二叉树的层序遍历.html-bf767249.js" as="script"><link rel="prefetch" href="/assets/leetcode_103_二叉树的锯齿形层序遍历.html-c877409c.js" as="script"><link rel="prefetch" href="/assets/leetcode_104_二叉树的最大深度.html-1b32579f.js" as="script"><link rel="prefetch" href="/assets/leetcode_105_从前序与中序遍历序列构造二叉树.html-d13a65a1.js" as="script"><link rel="prefetch" href="/assets/leetcode_106_从中序与后序遍历序列构造二叉树.html-a6327af2.js" as="script"><link rel="prefetch" href="/assets/leetcode_1071_字符串的最大公因子.html-5476c3fe.js" as="script"><link rel="prefetch" href="/assets/leetcode_107_二叉树的层序遍历 II.html-8d1b9daf.js" as="script"><link rel="prefetch" href="/assets/leetcode_108_将有序数组转换为二叉搜索树.html-56c9fcfb.js" as="script"><link rel="prefetch" href="/assets/leetcode_112_路径总和.html-bde85dfa.js" as="script"><link rel="prefetch" href="/assets/leetcode_113_路径总和II.html-8f74fe65.js" as="script"><link rel="prefetch" href="/assets/leetcode_1143_最长公共子序列.html-99c668c7.js" as="script"><link rel="prefetch" href="/assets/leetcode_114_二叉树展开为链表.html-bdd88a01.js" as="script"><link rel="prefetch" href="/assets/leetcode_1161_最大层内元素和.html-fcb209f5.js" as="script"><link rel="prefetch" href="/assets/leetcode_117_填充每个节点的下一个右侧节点指针 II.html-764ce2a2.js" as="script"><link rel="prefetch" href="/assets/leetcode_118_杨辉三角.html-479e30a4.js" as="script"><link rel="prefetch" href="/assets/leetcode_119_杨辉三角 II.html-a59406df.js" as="script"><link rel="prefetch" href="/assets/leetcode_1207_独一无二的出现次数.html-16dda148.js" as="script"><link rel="prefetch" href="/assets/leetcode_120_三角形最小路径和.html-86703591.js" as="script"><link rel="prefetch" href="/assets/leetcode_121_买卖股票的最佳时机 I.html-13626864.js" as="script"><link rel="prefetch" href="/assets/leetcode_122_买卖股票的最佳时机 II.html-bb37bd6e.js" as="script"><link rel="prefetch" href="/assets/leetcode_124_二叉树中的最大路径和.html-8e211f55.js" as="script"><link rel="prefetch" href="/assets/leetcode_125_验证回文串.html-90dfd796.js" as="script"><link rel="prefetch" href="/assets/leetcode_128_最长连续序列.html-8b798fa6.js" as="script"><link rel="prefetch" href="/assets/leetcode_129_求根节点到叶节点数字之和.html-8758fc10.js" as="script"><link rel="prefetch" href="/assets/leetcode_130_被围绕的区域.html-31108440.js" as="script"><link rel="prefetch" href="/assets/leetcode_1318_或运算的最小翻转次数.html-b1d00d09.js" as="script"><link rel="prefetch" href="/assets/leetcode_134_加油站.html-aa290400.js" as="script"><link rel="prefetch" href="/assets/leetcode_135_分发糖果.html-169212a1.js" as="script"><link rel="prefetch" href="/assets/leetcode_136_只出现一次的数字.html-efac1a41.js" as="script"><link rel="prefetch" href="/assets/leetcode_1372_二叉树中的最长交错路径.html-1f702f8e.js" as="script"><link rel="prefetch" href="/assets/leetcode_139_单词拆分.html-60a3eec6.js" as="script"><link rel="prefetch" href="/assets/leetcode_141_环形链表.html-caf4560c.js" as="script"><link rel="prefetch" href="/assets/leetcode_1431_拥有最多糖果的孩子.html-c46bc216.js" as="script"><link rel="prefetch" href="/assets/leetcode_1448_统计二叉树中好节点的数目.html-badf2d7e.js" as="script"><link rel="prefetch" href="/assets/leetcode_144_二叉树的前序遍历.html-782a3002.js" as="script"><link rel="prefetch" href="/assets/leetcode_1456_定长子串中元音的最大数目.html-0bebd911.js" as="script"><link rel="prefetch" href="/assets/leetcode_145_二叉树的后序遍历.html-f423c8ea.js" as="script"><link rel="prefetch" href="/assets/leetcode_1466_重新规划路线.html-61eb288d.js" as="script"><link rel="prefetch" href="/assets/leetcode_146_LRU 缓存.html-ef476ae2.js" as="script"><link rel="prefetch" href="/assets/leetcode_148_排序链表.html-6835afd8.js" as="script"><link rel="prefetch" href="/assets/leetcode_1493_删掉一个元素以后全为 1 的最长子数组.html-1c9feb8e.js" as="script"><link rel="prefetch" href="/assets/leetcode_150_逆波兰表达式求值.html-8c4709cc.js" as="script"><link rel="prefetch" href="/assets/leetcode_151_反转字符串中的单词.html-f91b8216.js" as="script"><link rel="prefetch" href="/assets/leetcode_155_最小栈.html-20c00992.js" as="script"><link rel="prefetch" href="/assets/leetcode_160_相交链表.html-5627acf7.js" as="script"><link rel="prefetch" href="/assets/leetcode_162_寻找峰值 copy.html-f24beb30.js" as="script"><link rel="prefetch" href="/assets/leetcode_162_寻找峰值.html-42e7d40f.js" as="script"><link rel="prefetch" href="/assets/leetcode_1657_确定两个字符串是否接近.html-8a42e6de.js" as="script"><link rel="prefetch" href="/assets/leetcode_1679_K 和数对的最大数目.html-75040006.js" as="script"><link rel="prefetch" href="/assets/leetcode_169_多数元素.html-b7c60141.js" as="script"><link rel="prefetch" href="/assets/leetcode_1732_找到最高海拔.html-7ecbb8c1.js" as="script"><link rel="prefetch" href="/assets/leetcode_173_二叉搜索树迭代器.html-5f9afb5f.js" as="script"><link rel="prefetch" href="/assets/leetcode_1768_交替合并字符串.html-fa23ddd6.js" as="script"><link rel="prefetch" href="/assets/leetcode_189_轮转数组.html-c42a0832.js" as="script"><link rel="prefetch" href="/assets/leetcode_1926_迷宫中离入口最近的出口.html-ba13a452.js" as="script"><link rel="prefetch" href="/assets/leetcode_198_打家劫舍.html-01ddc8ed.js" as="script"><link rel="prefetch" href="/assets/leetcode_199. 二叉树的右视图.html-b4044b3b.js" as="script"><link rel="prefetch" href="/assets/leetcode_202_快乐数 copy.html-4128158a.js" as="script"><link rel="prefetch" href="/assets/leetcode_202_快乐数.html-df50f511.js" as="script"><link rel="prefetch" href="/assets/leetcode_203_移除链表元素.html-e917fc2e.js" as="script"><link rel="prefetch" href="/assets/leetcode_206_反转链表.html-c20741fc.js" as="script"><link rel="prefetch" href="/assets/leetcode_208_实现 Trie (前缀树).html-b62d7f4f.js" as="script"><link rel="prefetch" href="/assets/leetcode_2095_删除链表的中间节点.html-deb3033e.js" as="script"><link rel="prefetch" href="/assets/leetcode_209_长度最小的子数组.html-b4fb933f.js" as="script"><link rel="prefetch" href="/assets/leetcode_2130_链表最大孪生和.html-d152e296.js" as="script"><link rel="prefetch" href="/assets/leetcode_215_数组中的第K个最大元素.html-facb19af.js" as="script"><link rel="prefetch" href="/assets/leetcode_216_组合总和 III.html-449b0a0b.js" as="script"><link rel="prefetch" href="/assets/leetcode_219_存在重复元素 II.html-426b13a6.js" as="script"><link rel="prefetch" href="/assets/leetcode_2215_找出两数组的不同.html-8f3c9866.js" as="script"><link rel="prefetch" href="/assets/leetcode_222_完全二叉树的节点个数.html-e247e501.js" as="script"><link rel="prefetch" href="/assets/leetcode_226_翻转二叉树.html-a6fe8a1b.js" as="script"><link rel="prefetch" href="/assets/leetcode_2300. 咒语和药水的成功对数.html-451a870e.js" as="script"><link rel="prefetch" href="/assets/leetcode_230_二叉搜索树中第K小的元素.html-8f9aff36.js" as="script"><link rel="prefetch" href="/assets/leetcode_2336_无限集中的最小数字.html-0186d2f3.js" as="script"><link rel="prefetch" href="/assets/leetcode_234_回文链表.html-5ffc51ac.js" as="script"><link rel="prefetch" href="/assets/leetcode_2352_相等行列对.html-aa32ea65.js" as="script"><link rel="prefetch" href="/assets/leetcode_236_二叉树的最近公共祖先.html-067dc510.js" as="script"><link rel="prefetch" href="/assets/leetcode_238_除自身以外数组的乘积.html-d416e8f0.js" as="script"><link rel="prefetch" href="/assets/leetcode_2390_从字符串中移除星号.html-f97fadc9.js" as="script"><link rel="prefetch" href="/assets/leetcode_242_有效的字母异位词.html-50564962.js" as="script"><link rel="prefetch" href="/assets/leetcode_2542_最大序列的分数.html-f5e8bf7d.js" as="script"><link rel="prefetch" href="/assets/leetcode_274_H指数.html-219397c4.js" as="script"><link rel="prefetch" href="/assets/leetcode_283_移动0 copy 2.html-2ece538f.js" as="script"><link rel="prefetch" href="/assets/leetcode_283_移动0 copy.html-b2dcfc16.js" as="script"><link rel="prefetch" href="/assets/leetcode_283_移动0.html-28326917.js" as="script"><link rel="prefetch" href="/assets/leetcode_300_最长递增子序列.html-7bc25e46.js" as="script"><link rel="prefetch" href="/assets/leetcode_322_零钱兑换.html-338820d0.js" as="script"><link rel="prefetch" href="/assets/leetcode_328_奇偶链表.html-c221c5e3.js" as="script"><link rel="prefetch" href="/assets/leetcode_334_递增的三元子序列.html-eaa7359c.js" as="script"><link rel="prefetch" href="/assets/leetcode_338_比特位计数.html-1ca3e264.js" as="script"><link rel="prefetch" href="/assets/leetcode_345_反转字符串中的元音字母.html-a53a56ed.js" as="script"><link rel="prefetch" href="/assets/leetcode_374_猜数字大小.html-838e47cd.js" as="script"><link rel="prefetch" href="/assets/leetcode_380_O(1) 时间插入、删除和获取随机元素.html-809dfc48.js" as="script"><link rel="prefetch" href="/assets/leetcode_383_赎金信.html-b140905d.js" as="script"><link rel="prefetch" href="/assets/leetcode_392_判断子序列.html-0f679959.js" as="script"><link rel="prefetch" href="/assets/leetcode_394_字符串解码.html-34dbf0a9.js" as="script"><link rel="prefetch" href="/assets/leetcode_435_无重叠区间.html-a86bed80.js" as="script"><link rel="prefetch" href="/assets/leetcode_437_路径总和 III.html-03d25af8.js" as="script"><link rel="prefetch" href="/assets/leetcode_443_压缩字符串.html-04e19523.js" as="script"><link rel="prefetch" href="/assets/leetcode_450_删除二叉搜索树中的节点.html-b96c20de.js" as="script"><link rel="prefetch" href="/assets/leetcode_452_用最少数量的箭引爆气球.html-38e27162.js" as="script"><link rel="prefetch" href="/assets/leetcode_530_二叉搜索树的最小绝对差.html-67141eeb.js" as="script"><link rel="prefetch" href="/assets/leetcode_547_省份数量.html-b3b6bac4.js" as="script"><link rel="prefetch" href="/assets/leetcode_605_种花问题.html-031d3116.js" as="script"><link rel="prefetch" href="/assets/leetcode_637_二叉树的层平均值.html-a7a36db9.js" as="script"><link rel="prefetch" href="/assets/leetcode_643_子数组最大平均数 I.html-a8f43e75.js" as="script"><link rel="prefetch" href="/assets/leetcode_649_Dota2 参议院.html-57e2951e.js" as="script"><link rel="prefetch" href="/assets/leetcode_700_二叉搜索树中的搜索.html-023dc167.js" as="script"><link rel="prefetch" href="/assets/leetcode_714_买卖股票的最佳时机含手续费.html-7ad01ab6.js" as="script"><link rel="prefetch" href="/assets/leetcode_724_寻找数组的中心下标.html-71388c1f.js" as="script"><link rel="prefetch" href="/assets/leetcode_735_行星碰撞.html-79670bf6.js" as="script"><link rel="prefetch" href="/assets/leetcode_739_每日温度.html-00bbc948.js" as="script"><link rel="prefetch" href="/assets/leetcode_746_使用最小花费爬楼梯.html-568fe2b8.js" as="script"><link rel="prefetch" href="/assets/leetcode_790_多米诺和托米诺平铺.html-e3681c3f.js" as="script"><link rel="prefetch" href="/assets/leetcode_841_钥匙和房间.html-75e3c562.js" as="script"><link rel="prefetch" href="/assets/leetcode_872_叶子相似的树.html-4ff19ccc.js" as="script"><link rel="prefetch" href="/assets/leetcode_875_爱吃香蕉的珂珂.html-c23d801c.js" as="script"><link rel="prefetch" href="/assets/leetcode_876_链表的中间结点.html-e827b29b.js" as="script"><link rel="prefetch" href="/assets/leetcode_901_股票价格跨度.html-6195f435.js" as="script"><link rel="prefetch" href="/assets/leetcode_933_最近的请求次数.html-b601ebe2.js" as="script"><link rel="prefetch" href="/assets/leetcode_994_腐烂的橘子.html-c9fcbc99.js" as="script"><link rel="prefetch" href="/assets/leetcode_LCP68_美丽的花束.html-bea99396.js" as="script"><link rel="prefetch" href="/assets/Arrays类.html-5cff467d.js" as="script"><link rel="prefetch" href="/assets/BigInteger和BigDecimal.html-b00a56b8.js" as="script"><link rel="prefetch" href="/assets/Collections类.html-6e3e4ec0.js" as="script"><link rel="prefetch" href="/assets/Java比较器.html-f75c5cab.js" as="script"><link rel="prefetch" href="/assets/Math类.html-66218b19.js" as="script"><link rel="prefetch" href="/assets/Object类.html-43eeba34.js" as="script"><link rel="prefetch" href="/assets/Pattern 与 Matcher 类.html-ee7d45a1.js" as="script"><link rel="prefetch" href="/assets/Stream API.html-9b418bda.js" as="script"><link rel="prefetch" href="/assets/String、Scanner相关类.html-ab539520.js" as="script"><link rel="prefetch" href="/assets/System类.html-0388d0ad.js" as="script"><link rel="prefetch" href="/assets/日期时间API.html-abcd5dc6.js" as="script"><link rel="prefetch" href="/assets/10、多线程.html-35e659db.js" as="script"><link rel="prefetch" href="/assets/11、枚举类.html-0bf87ec6.js" as="script"><link rel="prefetch" href="/assets/12、注解.html-50944a71.js" as="script"><link rel="prefetch" href="/assets/13、集合.html-9439bdbe.js" as="script"><link rel="prefetch" href="/assets/14、泛型.html-d7592650.js" as="script"><link rel="prefetch" href="/assets/15、IO流.html-515b20a8.js" as="script"><link rel="prefetch" href="/assets/16、网络编程.html-d4102567.js" as="script"><link rel="prefetch" href="/assets/17、反射.html-539859f0.js" as="script"><link rel="prefetch" href="/assets/18、新特性.html-ef7a4719.js" as="script"><link rel="prefetch" href="/assets/19、格式化输入输出.html-9690fc0f.js" as="script"><link rel="prefetch" href="/assets/1、基础概念.html-8e40e2f0.js" as="script"><link rel="prefetch" href="/assets/2、数据类型.html-7994f88b.js" as="script"><link rel="prefetch" href="/assets/3、运算符.html-457a2f56.js" as="script"><link rel="prefetch" href="/assets/4、程序流程控制.html-7c29dc6c.js" as="script"><link rel="prefetch" href="/assets/5、数组.html-a4c355fe.js" as="script"><link rel="prefetch" href="/assets/6、面向对象.html-ad3a75f9.js" as="script"><link rel="prefetch" href="/assets/7、单元测试Junit.html-6c06e896.js" as="script"><link rel="prefetch" href="/assets/8、包装类.html-91494c0b.js" as="script"><link rel="prefetch" href="/assets/9、异常处理.html-efeedf1f.js" as="script"><link rel="prefetch" href="/assets/网络架构.html-971a3185.js" as="script"><link rel="prefetch" href="/assets/1、Pytorch 建模流程.html-853004be.js" as="script"><link rel="prefetch" href="/assets/2、张量数据结构.html-369a6bfb.js" as="script"><link rel="prefetch" href="/assets/3、自动微分机制、优化器.html-a51e51d3.js" as="script"><link rel="prefetch" href="/assets/4、动态计算图.html-6751486b.js" as="script"><link rel="prefetch" href="/assets/5、Pytorch 低阶 API.html-384aa62f.js" as="script"><link rel="prefetch" href="/assets/7、构建、训练模型.html-4dc44cad.js" as="script"><link rel="prefetch" href="/assets/Pytorch-20.html-26d96f13.js" as="script"><link rel="prefetch" href="/assets/Pytorch.html-d1d2957f.js" as="script"><link rel="prefetch" href="/assets/强化学习.html-936d4c17.js" as="script"><link rel="prefetch" href="/assets/Vue笔记1-核心.html-e989c925.js" as="script"><link rel="prefetch" href="/assets/Vue笔记2-组件.html-8621d7cf.js" as="script"><link rel="prefetch" href="/assets/Vue笔记3-脚手架.html-de68fd83.js" as="script"><link rel="prefetch" href="/assets/Vue笔记4-Vue中的ajax.html-df24fe68.js" as="script"><link rel="prefetch" href="/assets/Vue笔记5-插槽.html-3155ce14.js" as="script"><link rel="prefetch" href="/assets/Vue笔记6-vuex.html-90ee75fa.js" as="script"><link rel="prefetch" href="/assets/Vue笔记7-路由.html-9023e075.js" as="script"><link rel="prefetch" href="/assets/Vue笔记8-element-ui.html-a3b54a99.js" as="script"><link rel="prefetch" href="/assets/Vue3快速上手.html-943296f4.js" as="script"><link rel="prefetch" href="/assets/markdown扩展.html-1b4673b3.js" as="script"><link rel="prefetch" href="/assets/404.html-cc91ab3d.js" as="script"><link rel="prefetch" href="/assets/index.html-4d071ae6.js" as="script"><link rel="prefetch" href="/assets/index.html-a6b9d739.js" as="script"><link rel="prefetch" href="/assets/index.html-673bc96c.js" as="script"><link rel="prefetch" href="/assets/index.html-a7a29c4c.js" as="script"><link rel="prefetch" href="/assets/index.html-194b0620.js" as="script"><link rel="prefetch" href="/assets/index.html-a899c6cf.js" as="script"><link rel="prefetch" href="/assets/index.html-e2494c9f.js" as="script"><link rel="prefetch" href="/assets/index.html-61c0a09c.js" as="script"><link rel="prefetch" href="/assets/index.html-be2636ab.js" as="script"><link rel="prefetch" href="/assets/index.html-67b37d25.js" as="script"><link rel="prefetch" href="/assets/index.html-2f703168.js" as="script"><link rel="prefetch" href="/assets/index.html-ed384e30.js" as="script"><link rel="prefetch" href="/assets/index.html-49eef8d8.js" as="script"><link rel="prefetch" href="/assets/index.html-2d0de498.js" as="script"><link rel="prefetch" href="/assets/index.html-2fa96ec7.js" as="script"><link rel="prefetch" href="/assets/index.html-d9cf3019.js" as="script"><link rel="prefetch" href="/assets/index.html-6d51d871.js" as="script"><link rel="prefetch" href="/assets/index.html-d782a376.js" as="script"><link rel="prefetch" href="/assets/index.html-848ebbe8.js" as="script"><link rel="prefetch" href="/assets/index.html-1ba1d180.js" as="script"><link rel="prefetch" href="/assets/index.html-9e55ee4d.js" as="script"><link rel="prefetch" href="/assets/index.html-106d954c.js" as="script"><link rel="prefetch" href="/assets/index.html-5fbc2a0d.js" as="script"><link rel="prefetch" href="/assets/index.html-c03d1584.js" as="script"><link rel="prefetch" href="/assets/index.html-9c648683.js" as="script"><link rel="prefetch" href="/assets/index.html-1c24935a.js" as="script"><link rel="prefetch" href="/assets/index.html-99af66e2.js" as="script"><link rel="prefetch" href="/assets/index.html-ee02ddf0.js" as="script"><link rel="prefetch" href="/assets/index.html-20b834a5.js" as="script"><link rel="prefetch" href="/assets/index.html-3a4d6e36.js" as="script"><link rel="prefetch" href="/assets/index.html-139f2f06.js" as="script"><link rel="prefetch" href="/assets/index.html-a935f094.js" as="script"><link rel="prefetch" href="/assets/index.html-e92ce08c.js" as="script"><link rel="prefetch" href="/assets/index.html-63bc6b2b.js" as="script"><link rel="prefetch" href="/assets/index.html-94957f61.js" as="script"><link rel="prefetch" href="/assets/index.html-91bcd141.js" as="script"><link rel="prefetch" href="/assets/index.html-e5933818.js" as="script"><link rel="prefetch" href="/assets/index.html-abd11527.js" as="script"><link rel="prefetch" href="/assets/index.html-d6c5a433.js" as="script"><link rel="prefetch" href="/assets/index.html-ec18a7df.js" as="script"><link rel="prefetch" href="/assets/index.html-d3624e38.js" as="script"><link rel="prefetch" href="/assets/index.html-13f98761.js" as="script"><link rel="prefetch" href="/assets/index.html-0cce8e2d.js" as="script"><link rel="prefetch" href="/assets/index.html-7193a535.js" as="script"><link rel="prefetch" href="/assets/index.html-0e85c606.js" as="script"><link rel="prefetch" href="/assets/index.html-41fd6d63.js" as="script"><link rel="prefetch" href="/assets/index.html-faa3c911.js" as="script"><link rel="prefetch" href="/assets/index.html-bfb66f86.js" as="script"><link rel="prefetch" href="/assets/index.html-f5020150.js" as="script"><link rel="prefetch" href="/assets/index.html-5c2c76d5.js" as="script"><link rel="prefetch" href="/assets/index.html-a2feaac9.js" as="script"><link rel="prefetch" href="/assets/index.html-b7b8c4b6.js" as="script"><link rel="prefetch" href="/assets/index.html-ff6eb04f.js" as="script"><link rel="prefetch" href="/assets/index.html-a3550230.js" as="script"><link rel="prefetch" href="/assets/index.html-7a3254b0.js" as="script"><link rel="prefetch" href="/assets/index.html-c5715831.js" as="script"><link rel="prefetch" href="/assets/index.html-e9a4a4e1.js" as="script"><link rel="prefetch" href="/assets/index.html-9d2fd3be.js" as="script"><link rel="prefetch" href="/assets/index.html-8af2a3d1.js" as="script"><link rel="prefetch" href="/assets/index.html-cc614db6.js" as="script"><link rel="prefetch" href="/assets/index.html-b772e465.js" as="script"><link rel="prefetch" href="/assets/index.html-b1a752fe.js" as="script"><link rel="prefetch" href="/assets/index.html-4209efbb.js" as="script"><link rel="prefetch" href="/assets/index.html-4ff744ba.js" as="script"><link rel="prefetch" href="/assets/index.html-c424e42c.js" as="script"><link rel="prefetch" href="/assets/index.html-7f168d0f.js" as="script"><link rel="prefetch" href="/assets/index.html-56de2f95.js" as="script"><link rel="prefetch" href="/assets/index.html-f7cb97d3.js" as="script"><link rel="prefetch" href="/assets/index.html-c546e8a1.js" as="script"><link rel="prefetch" href="/assets/index.html-0009964a.js" as="script"><link rel="prefetch" href="/assets/index.html-00b637a2.js" as="script"><link rel="prefetch" href="/assets/index.html-fb5f6e90.js" as="script"><link rel="prefetch" href="/assets/index.html-ac4d56df.js" as="script"><link rel="prefetch" href="/assets/index.html-6aa2d78e.js" as="script"><link rel="prefetch" href="/assets/index.html-d462cdeb.js" as="script"><link rel="prefetch" href="/assets/index.html-6b698fb1.js" as="script"><link rel="prefetch" href="/assets/index.html-acbfbf53.js" as="script"><link rel="prefetch" href="/assets/index.html-905f1804.js" as="script"><link rel="prefetch" href="/assets/index.html-16d7d340.js" as="script"><link rel="prefetch" href="/assets/index.html-46271911.js" as="script"><link rel="prefetch" href="/assets/index.html-b604d4ee.js" as="script"><link rel="prefetch" href="/assets/index.html-14c05c19.js" as="script"><link rel="prefetch" href="/assets/index.html-f310c092.js" as="script"><link rel="prefetch" href="/assets/index.html-ccaf9ee9.js" as="script"><link rel="prefetch" href="/assets/index.html-e22bc441.js" as="script"><link rel="prefetch" href="/assets/index.html-1c66cf59.js" as="script"><link rel="prefetch" href="/assets/index.html-8208559f.js" as="script"><link rel="prefetch" href="/assets/index.html-2aea38ab.js" as="script"><link rel="prefetch" href="/assets/index.html-9c6eba41.js" as="script"><link rel="prefetch" href="/assets/index.html-38a7a2af.js" as="script"><link rel="prefetch" href="/assets/index.html-c8b179d7.js" as="script"><link rel="prefetch" href="/assets/index.html-7e1863fb.js" as="script"><link rel="prefetch" href="/assets/index.html-8968a188.js" as="script"><link rel="prefetch" href="/assets/index.html-64ee687b.js" as="script"><link rel="prefetch" href="/assets/index.html-80fe2383.js" as="script"><link rel="prefetch" href="/assets/index.html-fea946c3.js" as="script"><link rel="prefetch" href="/assets/index.html-f17f8ab5.js" as="script"><link rel="prefetch" href="/assets/index.html-1a5be529.js" as="script"><link rel="prefetch" href="/assets/index.html-0e767346.js" as="script"><link rel="prefetch" href="/assets/index.html-3ca31bd8.js" as="script"><link rel="prefetch" href="/assets/index.html-b133f8c8.js" as="script"><link rel="prefetch" href="/assets/index.html-7195b3a2.js" as="script"><link rel="prefetch" href="/assets/index.html-1beacc22.js" as="script"><link rel="prefetch" href="/assets/index.html-88e5e4ce.js" as="script"><link rel="prefetch" href="/assets/index.html-210be80e.js" as="script"><link rel="prefetch" href="/assets/index.html-f33d538b.js" as="script"><link rel="prefetch" href="/assets/index.html-4941c0fe.js" as="script"><link rel="prefetch" href="/assets/index.html-c736be09.js" as="script"><link rel="prefetch" href="/assets/index.html-03c9f91d.js" as="script"><link rel="prefetch" href="/assets/index.html-0b4b9ae3.js" as="script"><link rel="prefetch" href="/assets/index.html-229119d0.js" as="script"><link rel="prefetch" href="/assets/index.html-bc2fb4cf.js" as="script"><link rel="prefetch" href="/assets/index.html-1c4a3288.js" as="script"><link rel="prefetch" href="/assets/index.html-842d27e2.js" as="script"><link rel="prefetch" href="/assets/index.html-b4206ac4.js" as="script"><link rel="prefetch" href="/assets/index.html-73e0d1d2.js" as="script"><link rel="prefetch" href="/assets/index.html-72b69436.js" as="script"><link rel="prefetch" href="/assets/index.html-3477f208.js" as="script"><link rel="prefetch" href="/assets/index.html-7c416790.js" as="script"><link rel="prefetch" href="/assets/index.html-2fa8126d.js" as="script"><link rel="prefetch" href="/assets/index.html-c4278da5.js" as="script"><link rel="prefetch" href="/assets/index.html-97b358be.js" as="script"><link rel="prefetch" href="/assets/index.html-a3c429a2.js" as="script"><link rel="prefetch" href="/assets/index.html-e28d3b12.js" as="script"><link rel="prefetch" href="/assets/index.html-e18dcbc2.js" as="script"><link rel="prefetch" href="/assets/index.html-9cbad9b7.js" as="script"><link rel="prefetch" href="/assets/index.html-c02de52a.js" as="script"><link rel="prefetch" href="/assets/index.html-375eef32.js" as="script"><link rel="prefetch" href="/assets/index.html-3b8fb7db.js" as="script"><link rel="prefetch" href="/assets/index.html-2b8d0809.js" as="script"><link rel="prefetch" href="/assets/index.html-186dd8fe.js" as="script"><link rel="prefetch" href="/assets/index.html-62cabbed.js" as="script"><link rel="prefetch" href="/assets/index.html-15d05023.js" as="script"><link rel="prefetch" href="/assets/index.html-1cb2d226.js" as="script"><link rel="prefetch" href="/assets/index.html-0f822705.js" as="script"><link rel="prefetch" href="/assets/Elasticsearch.html-0ec42c07.js" as="script"><link rel="prefetch" href="/assets/MongoDB.html-9179b1d1.js" as="script"><link rel="prefetch" href="/assets/MySQL基础.html-c9c1792a.js" as="script"><link rel="prefetch" href="/assets/Redis.html-e502e15e.js" as="script"><link rel="prefetch" href="/assets/IDEA、Eclipse快捷键.html-dd186a92.js" as="script"><link rel="prefetch" href="/assets/JDBC.html-f686fffc.js" as="script"><link rel="prefetch" href="/assets/JVM.html-31fe88b9.js" as="script"><link rel="prefetch" href="/assets/JVM入门.html-d1e4e2df.js" as="script"><link rel="prefetch" href="/assets/JWT.html-44b85402.js" as="script"><link rel="prefetch" href="/assets/JavaWeb基础.html-53f36e29.js" as="script"><link rel="prefetch" href="/assets/Maven基础.html-8ca260d4.js" as="script"><link rel="prefetch" href="/assets/MyBatis.html-13331ba8.js" as="script"><link rel="prefetch" href="/assets/MybatisPlus.html-111585ab.js" as="script"><link rel="prefetch" href="/assets/RabbitMQ.html-e24ace30.js" as="script"><link rel="prefetch" href="/assets/Shiro.html-b7613862.js" as="script"><link rel="prefetch" href="/assets/SpringBoot.html-04b7d467.js" as="script"><link rel="prefetch" href="/assets/SpringBoot常用注解总结.html-28e17e23.js" as="script"><link rel="prefetch" href="/assets/SpringBoot自动装配原理.html-1e5d6ea2.js" as="script"><link rel="prefetch" href="/assets/SpringCloud基础.html-270458f1.js" as="script"><link rel="prefetch" href="/assets/SpringCloud基础2024.html-74ae8947.js" as="script"><link rel="prefetch" href="/assets/SpringMVC.html-04839bae.js" as="script"><link rel="prefetch" href="/assets/Spring基础.html-3d2dffd5.js" as="script"><link rel="prefetch" href="/assets/index.html-3b08c3fa.js" as="script"><link rel="prefetch" href="/assets/Matplotlib.html-59401557.js" as="script"><link rel="prefetch" href="/assets/Numpy.html-22085998.js" as="script"><link rel="prefetch" href="/assets/Pandas.html-37a6d250.js" as="script"><link rel="prefetch" href="/assets/python 基础语法.html-35094fbb.js" as="script"><link rel="prefetch" href="/assets/python 项目注意事项.html-3ce4cadc.js" as="script"><link rel="prefetch" href="/assets/Ajax.html-d1b5f272.js" as="script"><link rel="prefetch" href="/assets/Axios.html-1dd69401.js" as="script"><link rel="prefetch" href="/assets/CSS3.html-2e92d343.js" as="script"><link rel="prefetch" href="/assets/HTML.html-73eb42fc.js" as="script"><link rel="prefetch" href="/assets/Node.js笔记.html-a9ed55e9.js" as="script"><link rel="prefetch" href="/assets/Promise.html-0cdc0847.js" as="script"><link rel="prefetch" href="/assets/Git.html-2fe3c0b1.js" as="script"><link rel="prefetch" href="/assets/Markdown语法基础.html-b029b013.js" as="script"><link rel="prefetch" href="/assets/操作系统.html-0e6a3f71.js" as="script"><link rel="prefetch" href="/assets/数据结构.html-ef059ef3.js" as="script"><link rel="prefetch" href="/assets/正则表达式.html-c437b4de.js" as="script"><link rel="prefetch" href="/assets/计算机组成原理.html-b1b41a64.js" as="script"><link rel="prefetch" href="/assets/计算机网络.html-feaac010.js" as="script"><link rel="prefetch" href="/assets/设计模式.html-8a686ee7.js" as="script"><link rel="prefetch" href="/assets/面向对象设计基本原则.html-fe969115.js" as="script"><link rel="prefetch" href="/assets/Docker.html-fb86f273.js" as="script"><link rel="prefetch" href="/assets/GitHub Actions.html-ffa2e539.js" as="script"><link rel="prefetch" href="/assets/基于centos部署java项目.html-60e899d7.js" as="script"><link rel="prefetch" href="/assets/一些插件.html-dc4b35ee.js" as="script"><link rel="prefetch" href="/assets/快捷键.html-44cb4402.js" as="script"><link rel="prefetch" href="/assets/CSGO饰品图.html-174a078a.js" as="script"><link rel="prefetch" href="/assets/基础.html-578db97e.js" as="script"><link rel="prefetch" href="/assets/快捷键.html-4ab25ef9.js" as="script"><link rel="prefetch" href="/assets/Guitar.html-0288ec24.js" as="script"><link rel="prefetch" href="/assets/hlae.html-c498c121.js" as="script"><link rel="prefetch" href="/assets/settings.html-bb5b471e.js" as="script"><link rel="prefetch" href="/assets/Japanese.html-0ae8fa0a.js" as="script"><link rel="prefetch" href="/assets/index.html-3dc9a7f1.js" as="script"><link rel="prefetch" href="/assets/index.html-77d07788.js" as="script"><link rel="prefetch" href="/assets/30天JavaScript挑战.html-2e3efa68.js" as="script"><link rel="prefetch" href="/assets/高频 SQL 50 题（基础版）.html-30cdfb31.js" as="script"><link rel="prefetch" href="/assets/多线程.html-2bf96fd3.js" as="script"><link rel="prefetch" href="/assets/noob-Rg-better.html-da6ba649.js" as="script"><link rel="prefetch" href="/assets/noob-Rg.html-b99a8d92.js" as="script"><link rel="prefetch" href="/assets/1、JavaScript基础语法笔记.html-54ed1321.js" as="script"><link rel="prefetch" href="/assets/2、WebAPIs笔记.html-206dcbac.js" as="script"><link rel="prefetch" href="/assets/3、JavaScript进阶.html-9c36b76d.js" as="script"><link rel="prefetch" href="/assets/1、TypeScript语言简介.html-7dc2203e.js" as="script"><link rel="prefetch" href="/assets/2、基本用法.html-7e482044.js" as="script"><link rel="prefetch" href="/assets/3、any，unknown ，never 类型.html-25911b8e.js" as="script"><link rel="prefetch" href="/assets/4、系统类型.html-8273dd65.js" as="script"><link rel="prefetch" href="/assets/5、数组.html-a602bf42.js" as="script"><link rel="prefetch" href="/assets/6、元祖.html-918f5fc3.js" as="script"><link rel="prefetch" href="/assets/7、symbol 类型.html-a4b70cd4.js" as="script"><link rel="prefetch" href="/assets/8、函数.html-608609fb.js" as="script"><link rel="prefetch" href="/assets/index.html-ff072262.js" as="script"><link rel="prefetch" href="/assets/leetcode228_汇总区间.html-94d05681.js" as="script"><link rel="prefetch" href="/assets/leetcode_001_两数之和.html-5e493a62.js" as="script"><link rel="prefetch" href="/assets/leetcode_002_两数相加.html-47396bb5.js" as="script"><link rel="prefetch" href="/assets/leetcode_003_无重复字符的最长子串.html-8f89bc36.js" as="script"><link rel="prefetch" href="/assets/leetcode_004_寻找两个正序数组的中位数.html-d403a409.js" as="script"><link rel="prefetch" href="/assets/leetcode_005_最长回文子串.html-5c902db2.js" as="script"><link rel="prefetch" href="/assets/leetcode_006_N 字形变换.html-19936f81.js" as="script"><link rel="prefetch" href="/assets/leetcode_007_整数反转.html-740d0ac6.js" as="script"><link rel="prefetch" href="/assets/leetcode_008_字符串转换整数 (atoi).html-50f29b94.js" as="script"><link rel="prefetch" href="/assets/leetcode_009_回文数.html-91a4deac.js" as="script"><link rel="prefetch" href="/assets/leetcode_010_正则表达式匹配.html-b2efbb0b.js" as="script"><link rel="prefetch" href="/assets/leetcode_011_盛最多水的容器.html-f7c85466.js" as="script"><link rel="prefetch" href="/assets/leetcode_012_整数转罗马数字.html-ca6c9c23.js" as="script"><link rel="prefetch" href="/assets/leetcode_013_罗马数字转整数.html-d456b85e.js" as="script"><link rel="prefetch" href="/assets/leetcode_014_最长公共前缀.html-0205b4e3.js" as="script"><link rel="prefetch" href="/assets/leetcode_015_三数之和.html-18170be5.js" as="script"><link rel="prefetch" href="/assets/leetcode_016_最接近的三数之和.html-a2d37b88.js" as="script"><link rel="prefetch" href="/assets/leetcode_017_电话号码的字母组合.html-6c638e74.js" as="script"><link rel="prefetch" href="/assets/leetcode_018_四数之和.html-8a40d385.js" as="script"><link rel="prefetch" href="/assets/leetcode_019_删除链表的倒数第 N 个结点.html-83745947.js" as="script"><link rel="prefetch" href="/assets/leetcode_020_有效的括号.html-5241c959.js" as="script"><link rel="prefetch" href="/assets/leetcode_021_合并两个有序链表.html-3d1f08f8.js" as="script"><link rel="prefetch" href="/assets/leetcode_022_括号生成.html-603d20ce.js" as="script"><link rel="prefetch" href="/assets/leetcode_023_合并 K 个升序链表.html-bf403d84.js" as="script"><link rel="prefetch" href="/assets/leetcode_024_两两交换链表中的节点.html-1ed208e7.js" as="script"><link rel="prefetch" href="/assets/leetcode_025_K 个一组翻转链表.html-68199bc9.js" as="script"><link rel="prefetch" href="/assets/leetcode_026_删除有序数组中的重复项.html-57071dc9.js" as="script"><link rel="prefetch" href="/assets/leetcode_027_移除元素.html-80566e7d.js" as="script"><link rel="prefetch" href="/assets/leetcode_028_找出字符串中第一个匹配项的下标.html-79f8653c.js" as="script"><link rel="prefetch" href="/assets/leetcode_029_两数相除.html-9b1c961e.js" as="script"><link rel="prefetch" href="/assets/leetcode_030_串联所有单词的子串.html-16408c15.js" as="script"><link rel="prefetch" href="/assets/leetcode_031_下一个排列.html-e7d2946a.js" as="script"><link rel="prefetch" href="/assets/leetcode_032_最长有效括号.html-a5e2bf39.js" as="script"><link rel="prefetch" href="/assets/leetcode_033_搜索旋转排序数组.html-b1c32d67.js" as="script"><link rel="prefetch" href="/assets/leetcode_034_在排序数组中查找元素的第一个和最后一个位置.html-e57a72b3.js" as="script"><link rel="prefetch" href="/assets/leetcode_035_搜索插入位置.html-fe9abdcc.js" as="script"><link rel="prefetch" href="/assets/leetcode_036_有效的数独.html-5c43e0ff.js" as="script"><link rel="prefetch" href="/assets/leetcode_038_外观数列.html-11c26962.js" as="script"><link rel="prefetch" href="/assets/leetcode_039_ 组合总和.html-c9890bba.js" as="script"><link rel="prefetch" href="/assets/leetcode_040_ 组合总和II.html-657f9742.js" as="script"><link rel="prefetch" href="/assets/leetcode_041_ 缺失的第一个正数.html-3ee2d4bf.js" as="script"><link rel="prefetch" href="/assets/leetcode_042_接雨水.html-31c2ad86.js" as="script"><link rel="prefetch" href="/assets/leetcode_043_字符串相乘.html-53a10118.js" as="script"><link rel="prefetch" href="/assets/leetcode_045_跳跃游戏II.html-730d122f.js" as="script"><link rel="prefetch" href="/assets/leetcode_046_全排列.html-8023359b.js" as="script"><link rel="prefetch" href="/assets/leetcode_047_全排列II.html-6753f4a7.js" as="script"><link rel="prefetch" href="/assets/leetcode_048_旋转图像.html-a5c1846d.js" as="script"><link rel="prefetch" href="/assets/leetcode_049_字母异位词分组.html-e6812402.js" as="script"><link rel="prefetch" href="/assets/leetcode_050_Pow(x_n).html-2d80bc8a.js" as="script"><link rel="prefetch" href="/assets/leetcode_051_N 皇后.html-f415143a.js" as="script"><link rel="prefetch" href="/assets/leetcode_052_N 皇后 II.html-90492013.js" as="script"><link rel="prefetch" href="/assets/leetcode_053_最大子数组和.html-8f2c51f5.js" as="script"><link rel="prefetch" href="/assets/leetcode_054_螺旋矩阵.html-c690c07c.js" as="script"><link rel="prefetch" href="/assets/leetcode_055_跳跃游戏.html-b06c376f.js" as="script"><link rel="prefetch" href="/assets/leetcode_056_合并区间.html-b62cb667.js" as="script"><link rel="prefetch" href="/assets/leetcode_058_最后一个单词的长度.html-4ebb87d2.js" as="script"><link rel="prefetch" href="/assets/leetcode_061_旋转链表.html-ee7cb23a.js" as="script"><link rel="prefetch" href="/assets/leetcode_062_不同路径.html-8662c2cd.js" as="script"><link rel="prefetch" href="/assets/leetcode_063_不同路径II.html-830880fa.js" as="script"><link rel="prefetch" href="/assets/leetcode_064_最小路径和.html-bfac9146.js" as="script"><link rel="prefetch" href="/assets/leetcode_066_加一.html-0f7e4033.js" as="script"><link rel="prefetch" href="/assets/leetcode_067_二进制求和.html-53511cf7.js" as="script"><link rel="prefetch" href="/assets/leetcode_068_文本左右对齐.html-ad0ba4c6.js" as="script"><link rel="prefetch" href="/assets/leetcode_069_x的平方根.html-2ce215e8.js" as="script"><link rel="prefetch" href="/assets/leetcode_070_爬楼梯.html-ad717657.js" as="script"><link rel="prefetch" href="/assets/leetcode_072_编辑距离.html-c1fd2a15.js" as="script"><link rel="prefetch" href="/assets/leetcode_073_矩阵置零.html-a7145276.js" as="script"><link rel="prefetch" href="/assets/leetcode_075_颜色分类.html-3758d736.js" as="script"><link rel="prefetch" href="/assets/leetcode_076_最小覆盖子串 copy.html-a9c82594.js" as="script"><link rel="prefetch" href="/assets/leetcode_076_最小覆盖子串.html-c98f724a.js" as="script"><link rel="prefetch" href="/assets/leetcode_079_单词搜索.html-e76daa1a.js" as="script"><link rel="prefetch" href="/assets/leetcode_080_删除有序数组中的重复项.html-ec37f850.js" as="script"><link rel="prefetch" href="/assets/leetcode_082_删除排序链表中的重复元素 II.html-86bd1d1d.js" as="script"><link rel="prefetch" href="/assets/leetcode_083_删除排序链表中的重复元素.html-3766f031.js" as="script"><link rel="prefetch" href="/assets/leetcode_086_分隔链表.html-ae31c926.js" as="script"><link rel="prefetch" href="/assets/leetcode_088_合并两个有序数组.html-d5dfb57f.js" as="script"><link rel="prefetch" href="/assets/leetcode_092_反转链表 II.html-ca156bae.js" as="script"><link rel="prefetch" href="/assets/leetcode_094_二叉树的中序遍历.html-c4a36c66.js" as="script"><link rel="prefetch" href="/assets/leetcode_095_不同的二叉搜索树 II.html-43811530.js" as="script"><link rel="prefetch" href="/assets/leetcode_096_不同的二叉搜索树.html-f542aa86.js" as="script"><link rel="prefetch" href="/assets/leetcode_098_验证二叉搜索树.html-d9875fc6.js" as="script"><link rel="prefetch" href="/assets/leetcode_1004_最大连续1的个数 III.html-699af011.js" as="script"><link rel="prefetch" href="/assets/leetcode_100_相同的树.html-1eca0de6.js" as="script"><link rel="prefetch" href="/assets/leetcode_101_对称二叉树.html-65daf9f0.js" as="script"><link rel="prefetch" href="/assets/leetcode_102_二叉树的层序遍历.html-1dd19229.js" as="script"><link rel="prefetch" href="/assets/leetcode_103_二叉树的锯齿形层序遍历.html-6f11792d.js" as="script"><link rel="prefetch" href="/assets/leetcode_104_二叉树的最大深度.html-7cbe041c.js" as="script"><link rel="prefetch" href="/assets/leetcode_105_从前序与中序遍历序列构造二叉树.html-eafdb993.js" as="script"><link rel="prefetch" href="/assets/leetcode_106_从中序与后序遍历序列构造二叉树.html-93392553.js" as="script"><link rel="prefetch" href="/assets/leetcode_1071_字符串的最大公因子.html-20cdb43a.js" as="script"><link rel="prefetch" href="/assets/leetcode_107_二叉树的层序遍历 II.html-c960b534.js" as="script"><link rel="prefetch" href="/assets/leetcode_108_将有序数组转换为二叉搜索树.html-5ab8923c.js" as="script"><link rel="prefetch" href="/assets/leetcode_112_路径总和.html-7667f460.js" as="script"><link rel="prefetch" href="/assets/leetcode_113_路径总和II.html-834ca183.js" as="script"><link rel="prefetch" href="/assets/leetcode_1143_最长公共子序列.html-9e70aac4.js" as="script"><link rel="prefetch" href="/assets/leetcode_114_二叉树展开为链表.html-c5a5b4cc.js" as="script"><link rel="prefetch" href="/assets/leetcode_1161_最大层内元素和.html-a95511c2.js" as="script"><link rel="prefetch" href="/assets/leetcode_117_填充每个节点的下一个右侧节点指针 II.html-f257e22b.js" as="script"><link rel="prefetch" href="/assets/leetcode_118_杨辉三角.html-ce38e44d.js" as="script"><link rel="prefetch" href="/assets/leetcode_119_杨辉三角 II.html-b99e6c12.js" as="script"><link rel="prefetch" href="/assets/leetcode_1207_独一无二的出现次数.html-9e4cbd22.js" as="script"><link rel="prefetch" href="/assets/leetcode_120_三角形最小路径和.html-e9fe7e95.js" as="script"><link rel="prefetch" href="/assets/leetcode_121_买卖股票的最佳时机 I.html-bef2ebfc.js" as="script"><link rel="prefetch" href="/assets/leetcode_122_买卖股票的最佳时机 II.html-e0d2bc8e.js" as="script"><link rel="prefetch" href="/assets/leetcode_124_二叉树中的最大路径和.html-d79f3452.js" as="script"><link rel="prefetch" href="/assets/leetcode_125_验证回文串.html-a0e305af.js" as="script"><link rel="prefetch" href="/assets/leetcode_128_最长连续序列.html-1da75d0d.js" as="script"><link rel="prefetch" href="/assets/leetcode_129_求根节点到叶节点数字之和.html-df313203.js" as="script"><link rel="prefetch" href="/assets/leetcode_130_被围绕的区域.html-8896ae3a.js" as="script"><link rel="prefetch" href="/assets/leetcode_1318_或运算的最小翻转次数.html-9da1f4f7.js" as="script"><link rel="prefetch" href="/assets/leetcode_134_加油站.html-71b9bed2.js" as="script"><link rel="prefetch" href="/assets/leetcode_135_分发糖果.html-44a9d684.js" as="script"><link rel="prefetch" href="/assets/leetcode_136_只出现一次的数字.html-f82bc5ad.js" as="script"><link rel="prefetch" href="/assets/leetcode_1372_二叉树中的最长交错路径.html-70d180be.js" as="script"><link rel="prefetch" href="/assets/leetcode_139_单词拆分.html-3caa55a3.js" as="script"><link rel="prefetch" href="/assets/leetcode_141_环形链表.html-40ff2dcb.js" as="script"><link rel="prefetch" href="/assets/leetcode_1431_拥有最多糖果的孩子.html-418180de.js" as="script"><link rel="prefetch" href="/assets/leetcode_1448_统计二叉树中好节点的数目.html-4796fd15.js" as="script"><link rel="prefetch" href="/assets/leetcode_144_二叉树的前序遍历.html-810dd50a.js" as="script"><link rel="prefetch" href="/assets/leetcode_1456_定长子串中元音的最大数目.html-06c90396.js" as="script"><link rel="prefetch" href="/assets/leetcode_145_二叉树的后序遍历.html-d8ffa8d0.js" as="script"><link rel="prefetch" href="/assets/leetcode_1466_重新规划路线.html-f19d80c9.js" as="script"><link rel="prefetch" href="/assets/leetcode_146_LRU 缓存.html-dd036bb3.js" as="script"><link rel="prefetch" href="/assets/leetcode_148_排序链表.html-bc899858.js" as="script"><link rel="prefetch" href="/assets/leetcode_1493_删掉一个元素以后全为 1 的最长子数组.html-b50ff4aa.js" as="script"><link rel="prefetch" href="/assets/leetcode_150_逆波兰表达式求值.html-afaed1f3.js" as="script"><link rel="prefetch" href="/assets/leetcode_151_反转字符串中的单词.html-3f89a620.js" as="script"><link rel="prefetch" href="/assets/leetcode_155_最小栈.html-087885f4.js" as="script"><link rel="prefetch" href="/assets/leetcode_160_相交链表.html-248fa152.js" as="script"><link rel="prefetch" href="/assets/leetcode_162_寻找峰值 copy.html-0a40385a.js" as="script"><link rel="prefetch" href="/assets/leetcode_162_寻找峰值.html-51e5dda3.js" as="script"><link rel="prefetch" href="/assets/leetcode_1657_确定两个字符串是否接近.html-3b56d30f.js" as="script"><link rel="prefetch" href="/assets/leetcode_1679_K 和数对的最大数目.html-3c16816a.js" as="script"><link rel="prefetch" href="/assets/leetcode_169_多数元素.html-f83ae97c.js" as="script"><link rel="prefetch" href="/assets/leetcode_1732_找到最高海拔.html-aec269b0.js" as="script"><link rel="prefetch" href="/assets/leetcode_173_二叉搜索树迭代器.html-afba1aad.js" as="script"><link rel="prefetch" href="/assets/leetcode_1768_交替合并字符串.html-26544815.js" as="script"><link rel="prefetch" href="/assets/leetcode_189_轮转数组.html-bb95e0b9.js" as="script"><link rel="prefetch" href="/assets/leetcode_1926_迷宫中离入口最近的出口.html-1e6c1401.js" as="script"><link rel="prefetch" href="/assets/leetcode_198_打家劫舍.html-7bcf6855.js" as="script"><link rel="prefetch" href="/assets/leetcode_199. 二叉树的右视图.html-04b0c2f7.js" as="script"><link rel="prefetch" href="/assets/leetcode_202_快乐数 copy.html-8c5e7cd5.js" as="script"><link rel="prefetch" href="/assets/leetcode_202_快乐数.html-0ca4653c.js" as="script"><link rel="prefetch" href="/assets/leetcode_203_移除链表元素.html-0587955f.js" as="script"><link rel="prefetch" href="/assets/leetcode_206_反转链表.html-dc024db5.js" as="script"><link rel="prefetch" href="/assets/leetcode_208_实现 Trie (前缀树).html-7fc74651.js" as="script"><link rel="prefetch" href="/assets/leetcode_2095_删除链表的中间节点.html-8e147c71.js" as="script"><link rel="prefetch" href="/assets/leetcode_209_长度最小的子数组.html-7c0829ed.js" as="script"><link rel="prefetch" href="/assets/leetcode_2130_链表最大孪生和.html-02587182.js" as="script"><link rel="prefetch" href="/assets/leetcode_215_数组中的第K个最大元素.html-62fe9634.js" as="script"><link rel="prefetch" href="/assets/leetcode_216_组合总和 III.html-10ca47fb.js" as="script"><link rel="prefetch" href="/assets/leetcode_219_存在重复元素 II.html-c12bfc6e.js" as="script"><link rel="prefetch" href="/assets/leetcode_2215_找出两数组的不同.html-a5027020.js" as="script"><link rel="prefetch" href="/assets/leetcode_222_完全二叉树的节点个数.html-c522125d.js" as="script"><link rel="prefetch" href="/assets/leetcode_226_翻转二叉树.html-931d34e2.js" as="script"><link rel="prefetch" href="/assets/leetcode_2300. 咒语和药水的成功对数.html-27098519.js" as="script"><link rel="prefetch" href="/assets/leetcode_230_二叉搜索树中第K小的元素.html-277d24e4.js" as="script"><link rel="prefetch" href="/assets/leetcode_2336_无限集中的最小数字.html-8505ed0d.js" as="script"><link rel="prefetch" href="/assets/leetcode_234_回文链表.html-23d92074.js" as="script"><link rel="prefetch" href="/assets/leetcode_2352_相等行列对.html-14a8ad08.js" as="script"><link rel="prefetch" href="/assets/leetcode_236_二叉树的最近公共祖先.html-92eef588.js" as="script"><link rel="prefetch" href="/assets/leetcode_238_除自身以外数组的乘积.html-91a79ffd.js" as="script"><link rel="prefetch" href="/assets/leetcode_2390_从字符串中移除星号.html-61a20cae.js" as="script"><link rel="prefetch" href="/assets/leetcode_242_有效的字母异位词.html-9d54d4ca.js" as="script"><link rel="prefetch" href="/assets/leetcode_2542_最大序列的分数.html-ab8aef67.js" as="script"><link rel="prefetch" href="/assets/leetcode_274_H指数.html-46cf10fc.js" as="script"><link rel="prefetch" href="/assets/leetcode_283_移动0 copy 2.html-38c05602.js" as="script"><link rel="prefetch" href="/assets/leetcode_283_移动0 copy.html-40315e0e.js" as="script"><link rel="prefetch" href="/assets/leetcode_283_移动0.html-7c232577.js" as="script"><link rel="prefetch" href="/assets/leetcode_300_最长递增子序列.html-a2da3d71.js" as="script"><link rel="prefetch" href="/assets/leetcode_322_零钱兑换.html-67eb2b4d.js" as="script"><link rel="prefetch" href="/assets/leetcode_328_奇偶链表.html-bf0cc09d.js" as="script"><link rel="prefetch" href="/assets/leetcode_334_递增的三元子序列.html-0ae4e17c.js" as="script"><link rel="prefetch" href="/assets/leetcode_338_比特位计数.html-173476e3.js" as="script"><link rel="prefetch" href="/assets/leetcode_345_反转字符串中的元音字母.html-2ffc29f7.js" as="script"><link rel="prefetch" href="/assets/leetcode_374_猜数字大小.html-9f6e731a.js" as="script"><link rel="prefetch" href="/assets/leetcode_380_O(1) 时间插入、删除和获取随机元素.html-c77ac87f.js" as="script"><link rel="prefetch" href="/assets/leetcode_383_赎金信.html-372d0c85.js" as="script"><link rel="prefetch" href="/assets/leetcode_392_判断子序列.html-e0cff9b7.js" as="script"><link rel="prefetch" href="/assets/leetcode_394_字符串解码.html-fa428464.js" as="script"><link rel="prefetch" href="/assets/leetcode_435_无重叠区间.html-c53662fd.js" as="script"><link rel="prefetch" href="/assets/leetcode_437_路径总和 III.html-de66b459.js" as="script"><link rel="prefetch" href="/assets/leetcode_443_压缩字符串.html-47e3cd69.js" as="script"><link rel="prefetch" href="/assets/leetcode_450_删除二叉搜索树中的节点.html-8b645106.js" as="script"><link rel="prefetch" href="/assets/leetcode_452_用最少数量的箭引爆气球.html-8156984b.js" as="script"><link rel="prefetch" href="/assets/leetcode_530_二叉搜索树的最小绝对差.html-2049b83b.js" as="script"><link rel="prefetch" href="/assets/leetcode_547_省份数量.html-51df30f3.js" as="script"><link rel="prefetch" href="/assets/leetcode_605_种花问题.html-6ce000bc.js" as="script"><link rel="prefetch" href="/assets/leetcode_637_二叉树的层平均值.html-f2f62194.js" as="script"><link rel="prefetch" href="/assets/leetcode_643_子数组最大平均数 I.html-73038873.js" as="script"><link rel="prefetch" href="/assets/leetcode_649_Dota2 参议院.html-19a5375a.js" as="script"><link rel="prefetch" href="/assets/leetcode_700_二叉搜索树中的搜索.html-89e9e4a9.js" as="script"><link rel="prefetch" href="/assets/leetcode_714_买卖股票的最佳时机含手续费.html-7e791860.js" as="script"><link rel="prefetch" href="/assets/leetcode_724_寻找数组的中心下标.html-87d40065.js" as="script"><link rel="prefetch" href="/assets/leetcode_735_行星碰撞.html-d9dc0676.js" as="script"><link rel="prefetch" href="/assets/leetcode_739_每日温度.html-5f857f26.js" as="script"><link rel="prefetch" href="/assets/leetcode_746_使用最小花费爬楼梯.html-6ce06866.js" as="script"><link rel="prefetch" href="/assets/leetcode_790_多米诺和托米诺平铺.html-e9624d18.js" as="script"><link rel="prefetch" href="/assets/leetcode_841_钥匙和房间.html-03697f53.js" as="script"><link rel="prefetch" href="/assets/leetcode_872_叶子相似的树.html-fb1119e3.js" as="script"><link rel="prefetch" href="/assets/leetcode_875_爱吃香蕉的珂珂.html-73fe357d.js" as="script"><link rel="prefetch" href="/assets/leetcode_876_链表的中间结点.html-247bc9f7.js" as="script"><link rel="prefetch" href="/assets/leetcode_901_股票价格跨度.html-1ece63f2.js" as="script"><link rel="prefetch" href="/assets/leetcode_933_最近的请求次数.html-5d85fffe.js" as="script"><link rel="prefetch" href="/assets/leetcode_994_腐烂的橘子.html-ab31cdfd.js" as="script"><link rel="prefetch" href="/assets/leetcode_LCP68_美丽的花束.html-efc9672a.js" as="script"><link rel="prefetch" href="/assets/Arrays类.html-c0fc84ce.js" as="script"><link rel="prefetch" href="/assets/BigInteger和BigDecimal.html-598701c0.js" as="script"><link rel="prefetch" href="/assets/Collections类.html-32b468d1.js" as="script"><link rel="prefetch" href="/assets/Java比较器.html-7030968f.js" as="script"><link rel="prefetch" href="/assets/Math类.html-78b46ec1.js" as="script"><link rel="prefetch" href="/assets/Object类.html-e8610cbc.js" as="script"><link rel="prefetch" href="/assets/Pattern 与 Matcher 类.html-7aff92f8.js" as="script"><link rel="prefetch" href="/assets/Stream API.html-1d5591d7.js" as="script"><link rel="prefetch" href="/assets/String、Scanner相关类.html-7ef8b988.js" as="script"><link rel="prefetch" href="/assets/System类.html-314ce5e7.js" as="script"><link rel="prefetch" href="/assets/日期时间API.html-477c0426.js" as="script"><link rel="prefetch" href="/assets/10、多线程.html-40dd955c.js" as="script"><link rel="prefetch" href="/assets/11、枚举类.html-85e15700.js" as="script"><link rel="prefetch" href="/assets/12、注解.html-96f1c0ae.js" as="script"><link rel="prefetch" href="/assets/13、集合.html-c35f7f7a.js" as="script"><link rel="prefetch" href="/assets/14、泛型.html-4b572e46.js" as="script"><link rel="prefetch" href="/assets/15、IO流.html-827dedb7.js" as="script"><link rel="prefetch" href="/assets/16、网络编程.html-8d4303b6.js" as="script"><link rel="prefetch" href="/assets/17、反射.html-3fe4539f.js" as="script"><link rel="prefetch" href="/assets/18、新特性.html-1a025170.js" as="script"><link rel="prefetch" href="/assets/19、格式化输入输出.html-09c332da.js" as="script"><link rel="prefetch" href="/assets/1、基础概念.html-67a0f9d4.js" as="script"><link rel="prefetch" href="/assets/2、数据类型.html-9e406abb.js" as="script"><link rel="prefetch" href="/assets/3、运算符.html-c17f7aeb.js" as="script"><link rel="prefetch" href="/assets/4、程序流程控制.html-d2b814fa.js" as="script"><link rel="prefetch" href="/assets/5、数组.html-85c1d0a1.js" as="script"><link rel="prefetch" href="/assets/6、面向对象.html-a9e6327d.js" as="script"><link rel="prefetch" href="/assets/7、单元测试Junit.html-99367f16.js" as="script"><link rel="prefetch" href="/assets/8、包装类.html-258c0d47.js" as="script"><link rel="prefetch" href="/assets/9、异常处理.html-f6d08065.js" as="script"><link rel="prefetch" href="/assets/网络架构.html-380b5995.js" as="script"><link rel="prefetch" href="/assets/1、Pytorch 建模流程.html-cfcae00c.js" as="script"><link rel="prefetch" href="/assets/2、张量数据结构.html-594dd367.js" as="script"><link rel="prefetch" href="/assets/3、自动微分机制、优化器.html-bffcff92.js" as="script"><link rel="prefetch" href="/assets/4、动态计算图.html-6f9a2386.js" as="script"><link rel="prefetch" href="/assets/5、Pytorch 低阶 API.html-702f435b.js" as="script"><link rel="prefetch" href="/assets/7、构建、训练模型.html-2369de49.js" as="script"><link rel="prefetch" href="/assets/Pytorch-20.html-fc0771e8.js" as="script"><link rel="prefetch" href="/assets/Pytorch.html-4871cab3.js" as="script"><link rel="prefetch" href="/assets/强化学习.html-5fe75460.js" as="script"><link rel="prefetch" href="/assets/Vue笔记1-核心.html-0a14b8f6.js" as="script"><link rel="prefetch" href="/assets/Vue笔记2-组件.html-687956a7.js" as="script"><link rel="prefetch" href="/assets/Vue笔记3-脚手架.html-db55f179.js" as="script"><link rel="prefetch" href="/assets/Vue笔记4-Vue中的ajax.html-08990e38.js" as="script"><link rel="prefetch" href="/assets/Vue笔记5-插槽.html-b7170ba4.js" as="script"><link rel="prefetch" href="/assets/Vue笔记6-vuex.html-0e1a9b5b.js" as="script"><link rel="prefetch" href="/assets/Vue笔记7-路由.html-05c5d25f.js" as="script"><link rel="prefetch" href="/assets/Vue笔记8-element-ui.html-7d9341f2.js" as="script"><link rel="prefetch" href="/assets/Vue3快速上手.html-93f038c3.js" as="script"><link rel="prefetch" href="/assets/markdown扩展.html-676135c8.js" as="script"><link rel="prefetch" href="/assets/404.html-e867304b.js" as="script"><link rel="prefetch" href="/assets/index.html-0945568d.js" as="script"><link rel="prefetch" href="/assets/index.html-8ce10acf.js" as="script"><link rel="prefetch" href="/assets/index.html-f98d7be5.js" as="script"><link rel="prefetch" href="/assets/index.html-e032b868.js" as="script"><link rel="prefetch" href="/assets/index.html-c3be45ea.js" as="script"><link rel="prefetch" href="/assets/index.html-27c96f44.js" as="script"><link rel="prefetch" href="/assets/index.html-5b7bd2d1.js" as="script"><link rel="prefetch" href="/assets/index.html-37f2f425.js" as="script"><link rel="prefetch" href="/assets/index.html-deaa8d74.js" as="script"><link rel="prefetch" href="/assets/index.html-3d6f3bf7.js" as="script"><link rel="prefetch" href="/assets/index.html-dd8a17be.js" as="script"><link rel="prefetch" href="/assets/index.html-e3c58128.js" as="script"><link rel="prefetch" href="/assets/index.html-60261d22.js" as="script"><link rel="prefetch" href="/assets/index.html-b0926616.js" as="script"><link rel="prefetch" href="/assets/index.html-95a98357.js" as="script"><link rel="prefetch" href="/assets/index.html-1f763f8f.js" as="script"><link rel="prefetch" href="/assets/index.html-d59c8bd4.js" as="script"><link rel="prefetch" href="/assets/index.html-ee26db22.js" as="script"><link rel="prefetch" href="/assets/index.html-cc180e40.js" as="script"><link rel="prefetch" href="/assets/index.html-6798fa36.js" as="script"><link rel="prefetch" href="/assets/index.html-17c0a97a.js" as="script"><link rel="prefetch" href="/assets/index.html-099fb7dd.js" as="script"><link rel="prefetch" href="/assets/index.html-d87a10d4.js" as="script"><link rel="prefetch" href="/assets/index.html-3c2b8f0f.js" as="script"><link rel="prefetch" href="/assets/index.html-d80a8eb2.js" as="script"><link rel="prefetch" href="/assets/index.html-b69d3a70.js" as="script"><link rel="prefetch" href="/assets/index.html-65ee11e8.js" as="script"><link rel="prefetch" href="/assets/index.html-676edc2e.js" as="script"><link rel="prefetch" href="/assets/index.html-c20a0bc9.js" as="script"><link rel="prefetch" href="/assets/index.html-6841bed6.js" as="script"><link rel="prefetch" href="/assets/index.html-d7d8527e.js" as="script"><link rel="prefetch" href="/assets/index.html-dc63e538.js" as="script"><link rel="prefetch" href="/assets/index.html-b24fd012.js" as="script"><link rel="prefetch" href="/assets/index.html-36ce6927.js" as="script"><link rel="prefetch" href="/assets/index.html-d18030c0.js" as="script"><link rel="prefetch" href="/assets/index.html-b788c264.js" as="script"><link rel="prefetch" href="/assets/index.html-d28167d3.js" as="script"><link rel="prefetch" href="/assets/index.html-f00065eb.js" as="script"><link rel="prefetch" href="/assets/index.html-b262a654.js" as="script"><link rel="prefetch" href="/assets/index.html-87b7cd84.js" as="script"><link rel="prefetch" href="/assets/index.html-e1659afe.js" as="script"><link rel="prefetch" href="/assets/index.html-c8774fb2.js" as="script"><link rel="prefetch" href="/assets/index.html-c7776923.js" as="script"><link rel="prefetch" href="/assets/index.html-1acac7b7.js" as="script"><link rel="prefetch" href="/assets/index.html-be6f2b0f.js" as="script"><link rel="prefetch" href="/assets/index.html-37a492e6.js" as="script"><link rel="prefetch" href="/assets/index.html-01d540e1.js" as="script"><link rel="prefetch" href="/assets/index.html-4c86b315.js" as="script"><link rel="prefetch" href="/assets/index.html-3823cae8.js" as="script"><link rel="prefetch" href="/assets/index.html-778a21dc.js" as="script"><link rel="prefetch" href="/assets/index.html-2f1cc695.js" as="script"><link rel="prefetch" href="/assets/index.html-81319eec.js" as="script"><link rel="prefetch" href="/assets/index.html-c21a78e3.js" as="script"><link rel="prefetch" href="/assets/index.html-d4ab30f8.js" as="script"><link rel="prefetch" href="/assets/index.html-a885306c.js" as="script"><link rel="prefetch" href="/assets/index.html-8256f710.js" as="script"><link rel="prefetch" href="/assets/index.html-7bc32e63.js" as="script"><link rel="prefetch" href="/assets/index.html-17421caa.js" as="script"><link rel="prefetch" href="/assets/index.html-4bbb5330.js" as="script"><link rel="prefetch" href="/assets/index.html-e2547dff.js" as="script"><link rel="prefetch" href="/assets/index.html-6ccba92b.js" as="script"><link rel="prefetch" href="/assets/index.html-dac87edc.js" as="script"><link rel="prefetch" href="/assets/index.html-a18593c5.js" as="script"><link rel="prefetch" href="/assets/index.html-8f864586.js" as="script"><link rel="prefetch" href="/assets/index.html-7f90b7ee.js" as="script"><link rel="prefetch" href="/assets/index.html-79c6ea87.js" as="script"><link rel="prefetch" href="/assets/index.html-e2a6096a.js" as="script"><link rel="prefetch" href="/assets/index.html-0044c72b.js" as="script"><link rel="prefetch" href="/assets/index.html-7ff61241.js" as="script"><link rel="prefetch" href="/assets/index.html-07a742bc.js" as="script"><link rel="prefetch" href="/assets/index.html-e387861e.js" as="script"><link rel="prefetch" href="/assets/index.html-fb68bc51.js" as="script"><link rel="prefetch" href="/assets/index.html-0c860654.js" as="script"><link rel="prefetch" href="/assets/index.html-a4e2ad4b.js" as="script"><link rel="prefetch" href="/assets/index.html-90f97136.js" as="script"><link rel="prefetch" href="/assets/index.html-7339ba52.js" as="script"><link rel="prefetch" href="/assets/index.html-0cb111fc.js" as="script"><link rel="prefetch" href="/assets/index.html-be5006c9.js" as="script"><link rel="prefetch" href="/assets/index.html-b60eed6d.js" as="script"><link rel="prefetch" href="/assets/index.html-e0ab4437.js" as="script"><link rel="prefetch" href="/assets/index.html-4737bd1e.js" as="script"><link rel="prefetch" href="/assets/index.html-e3220bc0.js" as="script"><link rel="prefetch" href="/assets/index.html-60c5d8e0.js" as="script"><link rel="prefetch" href="/assets/index.html-571444c5.js" as="script"><link rel="prefetch" href="/assets/index.html-553d7191.js" as="script"><link rel="prefetch" href="/assets/index.html-e4a8f984.js" as="script"><link rel="prefetch" href="/assets/index.html-b36a1c32.js" as="script"><link rel="prefetch" href="/assets/index.html-e19092c7.js" as="script"><link rel="prefetch" href="/assets/index.html-30c8db30.js" as="script"><link rel="prefetch" href="/assets/index.html-771524bd.js" as="script"><link rel="prefetch" href="/assets/index.html-82b1f16d.js" as="script"><link rel="prefetch" href="/assets/index.html-373ab305.js" as="script"><link rel="prefetch" href="/assets/index.html-5e21aa28.js" as="script"><link rel="prefetch" href="/assets/index.html-62371857.js" as="script"><link rel="prefetch" href="/assets/index.html-93b9ba51.js" as="script"><link rel="prefetch" href="/assets/index.html-91e913cb.js" as="script"><link rel="prefetch" href="/assets/index.html-dd8f9eb6.js" as="script"><link rel="prefetch" href="/assets/index.html-c6edb153.js" as="script"><link rel="prefetch" href="/assets/index.html-a0661932.js" as="script"><link rel="prefetch" href="/assets/index.html-c1cd0919.js" as="script"><link rel="prefetch" href="/assets/index.html-724074eb.js" as="script"><link rel="prefetch" href="/assets/index.html-bcc81930.js" as="script"><link rel="prefetch" href="/assets/index.html-539e6f48.js" as="script"><link rel="prefetch" href="/assets/index.html-0fd143cf.js" as="script"><link rel="prefetch" href="/assets/index.html-2d3b3b5b.js" as="script"><link rel="prefetch" href="/assets/index.html-e82d22e3.js" as="script"><link rel="prefetch" href="/assets/index.html-89a9df63.js" as="script"><link rel="prefetch" href="/assets/index.html-c0ca611f.js" as="script"><link rel="prefetch" href="/assets/index.html-4f8f6573.js" as="script"><link rel="prefetch" href="/assets/index.html-ff564c5e.js" as="script"><link rel="prefetch" href="/assets/index.html-542f611e.js" as="script"><link rel="prefetch" href="/assets/index.html-878683b2.js" as="script"><link rel="prefetch" href="/assets/index.html-d2083573.js" as="script"><link rel="prefetch" href="/assets/index.html-dada9714.js" as="script"><link rel="prefetch" href="/assets/index.html-8d240c9f.js" as="script"><link rel="prefetch" href="/assets/index.html-0149542d.js" as="script"><link rel="prefetch" href="/assets/index.html-5d6afacf.js" as="script"><link rel="prefetch" href="/assets/index.html-762a3ce2.js" as="script"><link rel="prefetch" href="/assets/index.html-e834b8b1.js" as="script"><link rel="prefetch" href="/assets/index.html-162a5122.js" as="script"><link rel="prefetch" href="/assets/index.html-cfcbe07e.js" as="script"><link rel="prefetch" href="/assets/index.html-7116cb26.js" as="script"><link rel="prefetch" href="/assets/index.html-58bac03c.js" as="script"><link rel="prefetch" href="/assets/index.html-116cb145.js" as="script"><link rel="prefetch" href="/assets/index.html-2095e4a4.js" as="script"><link rel="prefetch" href="/assets/index.html-cf14d5e6.js" as="script"><link rel="prefetch" href="/assets/index.html-6d5ab26e.js" as="script"><link rel="prefetch" href="/assets/index.html-df6eacca.js" as="script"><link rel="prefetch" href="/assets/index.html-39e44091.js" as="script"><link rel="prefetch" href="/assets/index.html-d12d6650.js" as="script"><link rel="prefetch" href="/assets/index.html-b3c28f37.js" as="script"><link rel="prefetch" href="/assets/index.html-0d5efc21.js" as="script"><link rel="prefetch" href="/assets/waline-meta-56fbc549.js" as="script"><link rel="prefetch" href="/assets/component-31c82188.js" as="script"><link rel="prefetch" href="/assets/auto-fa8841cf.js" as="script"><link rel="prefetch" href="/assets/index-a7d1ee58.js" as="script"><link rel="prefetch" href="/assets/flowchart-c441f34d.js" as="script"><link rel="prefetch" href="/assets/mermaid.core-59574a45.js" as="script"><link rel="prefetch" href="/assets/highlight.esm-75b11b9d.js" as="script"><link rel="prefetch" href="/assets/markdown.esm-abe06b83.js" as="script"><link rel="prefetch" href="/assets/math.esm-70a288c8.js" as="script"><link rel="prefetch" href="/assets/notes.esm-a106bb2c.js" as="script"><link rel="prefetch" href="/assets/reveal.esm-ec5549c1.js" as="script"><link rel="prefetch" href="/assets/search.esm-7e6792e2.js" as="script"><link rel="prefetch" href="/assets/zoom.esm-b83b91d0.js" as="script"><link rel="prefetch" href="/assets/VuePlayground-7eb658cd.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-5794cde2.js" as="script"><link rel="prefetch" href="/assets/pageview-c30b5411.js" as="script"><link rel="prefetch" href="/assets/style-e9220a04.js" as="script"><link rel="prefetch" href="/assets/docsearch-1d421ddb.js" as="script"><link rel="prefetch" href="/assets/index-5161ad19.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand" href="/"><img class="vp-nav-logo" src="/favicon.ico" alt="T4mako"><!----><span class="vp-site-name hide-in-pad">T4mako</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a class="vp-link nav-link" href="/"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="代码笔记"><span class="title"><span class="font-icon icon iconfont icon-code" style=""></span>代码笔记</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a class="vp-link nav-link" href="/code/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html"><!---->基础知识<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/java.html"><!---->Java<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91.html"><!---->前端开发<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/数据库.html"><!---->数据库<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/%E8%BF%90%E7%BB%B4%E4%B8%8E%E9%83%A8%E7%BD%B2.html"><!---->运维与部署<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link active" href="/code/python.html"><!---->Python<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/leetcode.html"><!---->Leetcode<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/project.html"><!---->项目笔记<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/code/其他.html"><!---->其他<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="兴趣使然"><span class="title"><span class="font-icon icon iconfont icon-view" style=""></span>兴趣使然</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a class="vp-link nav-link" href="/interests/AE.html"><!---->AE<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/interests/Blender.html"><!---->Blender<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/interests/HLAE.html"><!---->HLAE<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/interests/cook.html"><!---->吃饭糊弄学<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><div class="dropdown-wrapper"><button type="button" class="dropdown-title" aria-label="生活碎片"><span class="title"><span class="font-icon icon iconfont icon-note" style=""></span>生活碎片</span><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a class="vp-link nav-link" href="/life/随笔.html"><!---->随笔<!----></a></li><li class="dropdown-item"><a class="vp-link nav-link" href="/life/%E8%A7%82%E5%BD%B1%E5%8C%BA.html"><!---->观影区<!----></a></li></ul></button></div></div><div class="nav-item hide-in-mobile"><a class="vp-link nav-link" href="/friends.html"><span class="font-icon icon iconfont icon-group" style=""></span>友链<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/T4mako/T4mako.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->6、Pytorch 中阶 API</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/T4mako/T4mako.github.io" target="_blank" rel="noopener noreferrer">T4mako</a></span><span property="author" content="T4mako"></span></span><!----><!----><!----><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 30 分钟</span><meta property="timeRequired" content="PT30M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_6-1、dataset-和-dataloader">6.1、Dataset 和 DataLoader</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-1-1、dataset-和-dataloader-原理">6.1.1、Dataset 和 DataLoader 原理</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-1-2、dataset-使用">6.1.2、Dataset 使用</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-1-3、使用-dataloader-加载数据集">6.1.3、使用 DataLoader 加载数据集</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_6-2、模型层-torch-nn">6.2、模型层 torch.nn</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-2-1、基础层">6.2.1、基础层</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-2-2、卷积网络相关层">6.2.2、卷积网络相关层</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-2-3、循环网络相关层">6.2.3、循环网络相关层</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-2-4、transformer-相关层">6.2.4、Transformer 相关层</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-2-5、自定义模型层">6.2.5、自定义模型层</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_6-3、损失函数-losses">6.3、损失函数 losses</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-3-1、内置损失函数">6.3.1、内置损失函数</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-3-2、自定义损失函数">6.3.2、自定义损失函数</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-3-3、l1-和-l2正则化项">6.3.3、L1 和 L2正则化项</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-3-4、l1l2-正则项使用完整范例">6.3.4、L1L2 正则项使用完整范例</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-3-5、通过优化器实现-l2-正则化">6.3.5、通过优化器实现 L2 正则化</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/#_6-4、tensorboard-可视化">6.4、TensorBoard 可视化</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/#_6-4-0、tensorboard-可视化概述">6.4.0、Tensorboard 可视化概述</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="_6、pytorch-中阶-api" tabindex="-1"><a class="header-anchor" href="#_6、pytorch-中阶-api" aria-hidden="true">#</a> 6、Pytorch 中阶 API</h1><h2 id="_6-1、dataset-和-dataloader" tabindex="-1"><a class="header-anchor" href="#_6-1、dataset-和-dataloader" aria-hidden="true">#</a> 6.1、Dataset 和 DataLoader</h2><p>Pytorch 使用 Dataset 和 DataLoader 这两个工具类来构建数据管道。它们的作用是将数据整理成适合训练模型的格式，一个 batch 一个 batch 的取出给模型</p><h3 id="_6-1-1、dataset-和-dataloader-原理" tabindex="-1"><a class="header-anchor" href="#_6-1-1、dataset-和-dataloader-原理" aria-hidden="true">#</a> 6.1.1、Dataset 和 DataLoader 原理</h3><h4 id="获取一个-batch-的步骤" tabindex="-1"><a class="header-anchor" href="#获取一个-batch-的步骤" aria-hidden="true">#</a> 获取一个 batch 的步骤</h4><p>假定数据集的特征和标签分别表示为张量<code>X</code>和<code>Y</code>，数据集可以表+示为 <code>(X,Y)</code>, 假定 batch 大小为 <code>m</code></p><ol><li>首先确定数据集长度： <code>n</code></li><li>从 <code>0</code> 到 <code>n-1</code> 的范围中抽样出 <code>m</code> 个数（batch 大小）<br> 假定 <code>m=4</code>, 拿到的结果是一个列表，类似：<code>indices = [1,4,8,9]</code></li><li>从数据集中去取这<code>m</code>个数对应下标的元素<br> 拿到的结果是一个元组列表，类似：<code>samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]</code></li><li>将结果整理成两个张量作为输出<br> 类似 <code>batch = (features,labels)</code><br> 其中 <code>features = torch.stack([X[1],X[4],X[8],X[9]])</code>，<code>labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])</code></li></ol><h4 id="dataset-和-dataloader-的功能分工" tabindex="-1"><a class="header-anchor" href="#dataset-和-dataloader-的功能分工" aria-hidden="true">#</a> Dataset 和 DataLoader 的功能分工</h4><p><code>Dataset</code> 是一个抽象类，用于表示数据集</p><ul><li>上述步骤 1 确定数据集的长度是由 Dataset 的 **<code>__len__</code> ** 方法实现</li><li>步骤 2 从<code>0</code>到 <code>n-1</code> 的范围中抽样出 <code>m</code> 个数的方法是由 DataLoader 的 <code>sampler</code>和 <code>batch_sampler</code>参数指定 <ul><li><code>sampler</code> 参数指定单个元素抽样方法，一般无需用户设置，程序默认在 DataLoader 的参数 <code>shuffle=True</code> 时采用随机抽样，<code>shuffle=False</code> 时采用顺序抽样</li><li><code>batch_sampler</code> 参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在 DataLoader 的参数 <code>drop_last=True</code> 时会丢弃数据集最后一个长度不能被 batch 大小整除的批次，在 <code>drop_last=False</code> 时保留最后一个批次</li></ul></li><li>步骤 3 根据下标取数据集中的元素 是由 Dataset 的 **<code>__getitem__</code> **方法实现</li><li>步骤 4 的逻辑由DataLoader的参数<code>collate_fn</code>指定。一般情况下也无需用户设置。</li></ul><p>Dataset 和 DataLoader 的一般使用方式如下</p><ul><li>TensorDataset(data, labels)</li><li>DataLoader(ds,batch_size=4,drop_last,shuffle...)</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>Dataset<span class="token punctuation">,</span>DataLoader
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> RandomSampler<span class="token punctuation">,</span>BatchSampler 

<span class="token comment"># TensorDataset(data, labels)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token comment"># 获取数据加载器中的第一个批次（features和labels）</span>
features<span class="token punctuation">,</span>labels <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features = &quot;</span><span class="token punctuation">,</span>features <span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;labels = &quot;</span><span class="token punctuation">,</span>labels <span class="token punctuation">)</span>  
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>DataLoader 内部调用方式步骤拆解如下：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># step1: 确定数据集长度 (Dataset 的 __len__ 方法实现)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;n = &quot;</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># len(ds)等价于 ds.__len__()</span>

<span class="token comment"># step2: 确定抽样 indices (DataLoader 中的 Sampler 和 BatchSampler 实现)</span>
sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>data_source <span class="token operator">=</span> ds<span class="token punctuation">)</span> <span class="token comment"># 创建随机采样器</span>
batch_sampler <span class="token operator">=</span> BatchSampler<span class="token punctuation">(</span>sampler <span class="token operator">=</span> sampler<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span> drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># 创建批次采样器</span>
<span class="token comment"># 取出第一个批次的索引 indices</span>
<span class="token keyword">for</span> idxs <span class="token keyword">in</span> batch_sampler<span class="token punctuation">:</span>
    indices <span class="token operator">=</span> idxs
    <span class="token keyword">break</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;indices = &quot;</span><span class="token punctuation">,</span>indices<span class="token punctuation">)</span>

<span class="token comment"># step3: 取出一批样本 batch (Dataset 的 __getitem__ 方法实现)</span>
batch <span class="token operator">=</span> <span class="token punctuation">[</span>ds<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span>  indices<span class="token punctuation">]</span>  <span class="token comment">#  ds[i] 等价于 ds.__getitem__(i)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;batch = &quot;</span><span class="token punctuation">,</span> batch<span class="token punctuation">)</span>

<span class="token comment"># step4: 整理成 features 和 labels (DataLoader 的 collate_fn 方法实现)</span>
<span class="token keyword">def</span> <span class="token function">collate_fn</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    features <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch<span class="token punctuation">]</span><span class="token punctuation">)</span>
    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> features<span class="token punctuation">,</span>labels 

features<span class="token punctuation">,</span>labels <span class="token operator">=</span> collate_fn<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features = &quot;</span><span class="token punctuation">,</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;labels = &quot;</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-1-2、dataset-使用" tabindex="-1"><a class="header-anchor" href="#_6-1-2、dataset-使用" aria-hidden="true">#</a> 6.1.2、Dataset 使用</h3><p>Dataset 创建数据集的方法：</p><ul><li><p>使用 torch.utils.data.<strong>TensorDataset</strong> 根据 Tensor 创建数据集（numpy 的 array，Pandas 的 DataFrame 需要先转换成 Tensor）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset
train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集</p></li><li><p>继承 torch.utils.data.Dataset 创建自定义数据集</p></li></ul><p>常用手法：</p><ul><li>torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集</li><li>调用 Dataset 的加法运算符(<code>+</code>)将多个数据集合并成一个数据集</li></ul><h4 id="根据-tensor-创建数据集" tabindex="-1"><a class="header-anchor" href="#根据-tensor-创建数据集" aria-hidden="true">#</a> 根据 Tensor 创建数据集</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>Dataset<span class="token punctuation">,</span>DataLoader<span class="token punctuation">,</span>random_split 

<span class="token comment"># 根据 Tensor 创建数据集</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets 
iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
ds_iris <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 分割成训练集和预测集</span>
n_train <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_iris<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">)</span>
n_val <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ds_iris<span class="token punctuation">)</span> <span class="token operator">-</span> n_train
ds_train<span class="token punctuation">,</span>ds_val <span class="token operator">=</span> random_split<span class="token punctuation">(</span>ds_iris<span class="token punctuation">,</span><span class="token punctuation">[</span>n_train<span class="token punctuation">,</span>n_val<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 使用 DataLoader 加载数据集</span>
dl_train<span class="token punctuation">,</span>dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">break</span>

<span class="token comment"># 演示加法运算符（`+`）的合并作用</span>
ds_data <span class="token operator">=</span> ds_train <span class="token operator">+</span> ds_val

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;len(ds_train) = &#39;</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_train<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 120</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;len(ds_valid) = &#39;</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_val<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 30</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;len(ds_train+ds_valid) = &#39;</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_data<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 150</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>ds_data<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="根据图片目录创建图片数据集" tabindex="-1"><a class="header-anchor" href="#根据图片目录创建图片数据集" aria-hidden="true">#</a> 根据图片目录创建图片数据集</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token punctuation">,</span>datasets 

<span class="token comment"># 常用的图片增强操作</span>
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&#39;./data/cat.jpeg&#39;</span><span class="token punctuation">)</span>
<span class="token comment"># 随机数值翻转</span>
transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
<span class="token comment"># 随机旋转</span>
transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
<span class="token comment"># 定义图片增强操作，用于将多个图像变换（transforms）组合成一个单一的变换</span>
transform_train <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
   transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机水平翻转</span>
   transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机垂直翻转</span>
   transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 随机在45度角度内旋转</span>
   transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 转换成张量</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
transform_valid <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># 根据图片目录创建数据集</span>
<span class="token keyword">def</span> <span class="token function">transform_label</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

ds_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span><span class="token string">&quot;./eat_pytorch_datasets/cifar2/train/&quot;</span><span class="token punctuation">,</span>transform <span class="token operator">=</span> transform_train<span class="token punctuation">,</span>target_transform<span class="token operator">=</span> transform_label<span class="token punctuation">)</span>
ds_val <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span><span class="token string">&quot;./eat_pytorch_datasets/cifar2/test/&quot;</span><span class="token punctuation">,</span>transform <span class="token operator">=</span> transform_valid<span class="token punctuation">,</span>target_transform<span class="token operator">=</span> transform_label<span class="token punctuation">)</span>


<span class="token keyword">print</span><span class="token punctuation">(</span>ds_train<span class="token punctuation">.</span>class_to_idx<span class="token punctuation">)</span> <span class="token comment"># {&#39;0_airplane&#39;: 0, &#39;1_automobile&#39;: 1}</span>

<span class="token comment"># 使用 DataLoader 加载数据集</span>
dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 3, 32, 32])</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 1])</span>
    <span class="token keyword">break</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="创建自定义数据集" tabindex="-1"><a class="header-anchor" href="#创建自定义数据集" aria-hidden="true">#</a> 创建自定义数据集</h4><p>通过继承 torch.utils.data.Dataset 创建自定义数据集的方式来对 cifar2 构建数据管道</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path 
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image 

<span class="token keyword">class</span> <span class="token class-name">Cifar2Dataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>imgs_dir<span class="token punctuation">,</span>img_transform<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>files <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>Path<span class="token punctuation">(</span>imgs_dir<span class="token punctuation">)</span><span class="token punctuation">.</span>rglob<span class="token punctuation">(</span><span class="token string">&quot;*.jpg&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> img_transform
        
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>
        file_i <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
        img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>file_i<span class="token punctuation">)</span>
        tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span>  <span class="token string">&quot;1_automobile&quot;</span> <span class="token keyword">in</span> file_i <span class="token keyword">else</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tensor<span class="token punctuation">,</span>label 
    
    
train_dir <span class="token operator">=</span> <span class="token string">&quot;./eat_pytorch_datasets/cifar2/train/&quot;</span>
test_dir <span class="token operator">=</span> <span class="token string">&quot;./eat_pytorch_datasets/cifar2/test/&quot;</span>

<span class="token comment"># 定义图片增强</span>
transform_train <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
   transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机水平翻转</span>
   transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机垂直翻转</span>
   transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 随机在45度角度内旋转</span>
   transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 转换成张量</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

transform_val <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

ds_train <span class="token operator">=</span> Cifar2Dataset<span class="token punctuation">(</span>train_dir<span class="token punctuation">,</span>transform_train<span class="token punctuation">)</span>
ds_val <span class="token operator">=</span> Cifar2Dataset<span class="token punctuation">(</span>test_dir<span class="token punctuation">,</span>transform_val<span class="token punctuation">)</span>

dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 3, 32, 32])</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 1])</span>
    <span class="token keyword">break</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-1-3、使用-dataloader-加载数据集" tabindex="-1"><a class="header-anchor" href="#_6-1-3、使用-dataloader-加载数据集" aria-hidden="true">#</a> 6.1.3、使用 DataLoader 加载数据集</h3><p>DataLoader 能够控制 batch 的大小，batch 中元素的采样方法，以及将 batch 结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据</p><p>DataLoader 的函数签名如下</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>DataLoader<span class="token punctuation">(</span>
    dataset<span class="token punctuation">,</span> <span class="token comment"># 传入 Dataset</span>
    batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 取 Dataset 时，一次取多大</span>
    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    sampler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    batch_sampler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    collate_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    timeout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    worker_init_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    multiprocessing_context<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers,pin_memory, drop_last 这六个参数，</p><p>有时候对于一些复杂结构的数据集，还需要自定义 collate_fn 函数，其他参数一般使用默认值即可。</p><p>DataLoader 除了可以加载我们前面讲的 torch.utils.data.Dataset 外，还能够加载另外一种数据集 torch.utils.data.IterableDataset。</p><p>和 Dataset 数据集相当于一种列表结构不同，IterableDataset 相当于一种迭代器结构。 它更加复杂，一般较少使用。</p><ul><li>dataset : 数据集</li><li>batch_size: 批次大小</li><li>shuffle: 是否乱序</li><li>sampler: 样本采样函数，一般无需设置。</li><li>batch_sampler: 批次采样函数，一般无需设置。</li><li>num_workers: 使用多进程读取数据，设置的进程数。</li><li>collate_fn: 整理一个批次数据的函数。</li><li>pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。</li><li>drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。</li><li>timeout: 加载一个数据批次的最长等待时间，一般无需设置。</li><li>worker_init_fn: 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 构建输入数据管道</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span>
                batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span>
                shuffle<span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                drop_last <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 迭代数据</span>
<span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token keyword">in</span> dl<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
    
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([45, 49, 27,  7, 32, 48, 19, 38, 35, 30])
tensor([44, 37, 21, 39, 29, 13,  8, 31, 33,  5])
tensor([34, 28,  2, 23, 15, 42, 43, 40, 22,  6])
tensor([36,  3, 46,  9, 26, 16, 12, 17, 18,  1])
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_6-2、模型层-torch-nn" tabindex="-1"><a class="header-anchor" href="#_6-2、模型层-torch-nn" aria-hidden="true">#</a> 6.2、模型层 torch.nn</h2><p>深度学习模型由各种模型层组合</p><p><strong>torch.nn</strong> 中内置了非常丰富的各种模型层。它们都属于 nn.Module 的子类，具备参数管理功能</p><p>也可以通过继承 nn.Module 基类构建自定义的模型层</p><p>pytorch 不区分模型和模型层，都是通过继承 nn.Module 进行构建</p><p>只要继承 nn.Module 基类并实现 forward 方法即可自定义模型层</p><blockquote><p>torch.nn.function 中有很多功能，与 nn.Module 一样。<br> 一般情况下，如果模型有可学习的参数（w，b），最好用 nn.Module，其他情况（激活函数，loss function） nn.function 相对更简单些</p></blockquote><h3 id="_6-2-1、基础层" tabindex="-1"><a class="header-anchor" href="#_6-2-1、基础层" aria-hidden="true">#</a> 6.2.1、基础层</h3><ul><li><p>nn.Linear：全连接层。参数个数 = 输入层特征数 × 输出层特征数(weight)＋ 输出层特征数(bias)</p></li><li><p>nn.Embedding：嵌入层。一种比 Onehot 更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p></li><li><p>nn.Flatten：压平层，用于将多维张量样本压成一维张量样本。</p></li><li><p>nn.BatchNorm1d：一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。可以用 afine 参数设置该层是否含有可以训练的参数。</p></li><li><p>nn.BatchNorm2d：二维批标准化层。 常用于 CV 领域。</p></li><li><p>nn.BatchNorm3d：三维批标准化层。</p></li><li><p>nn.Dropout：一维随机丢弃层。一种正则化手段。</p></li><li><p>nn.Dropout2d：二维随机丢弃层。</p></li><li><p>nn.Dropout3d：三维随机丢弃层。</p></li><li><p>nn.Threshold：限幅层。当输入大于或小于阈值范围时，截断之。</p></li><li><p>nn.ConstantPad2d： 二维常数填充层。对二维张量样本填充常数扩展长度。</p></li><li><p>nn.ReplicationPad1d： 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</p></li><li><p>nn.ZeroPad2d：二维零值填充层。对二维张量样本在边缘填充0值.</p></li><li><p>nn.GroupNorm：组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。不受 batch 大小限制。</p></li><li><p>nn.LayerNorm：层归一化。常用于 NLP 领域，不受序列长度不一致影响。</p></li><li><p>nn.InstanceNorm2d: 样本归一化。一般在图像风格迁移任务中效果较好。</p></li></ul><p>各种归一化层：</p><ul><li>结构化数据 BatchNorm1D 归一化</li><li>图片数据的各种归一化（一般常用BatchNorm2D）</li><li>文本数据的 LayerNorm 归一化</li><li>可自适应学习的归一化 SwitchableNorm</li></ul><p>参考论文：<a href="https://arxiv.org/pdf/1806.10779.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1806.10779.pdf<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>对 BatchNorm 需要注意的几点：</p><ul><li>原始论文认为将 BatchNorm 放在激活函数前效果较好，后面的研究一般认为将 BatchNorm 放在激活函数之后更好</li><li>BatchNorm在训练过程和推理过程的逻辑不一样，训练过程 BatchNorm 的均值和方差和根据 mini-batch 中的数据估计的，而推理过程中 BatchNorm 的均值和方差是用的训练过程中的全体样本估计的。因此预测过程是稳定的，相同的样本不会因为所在批次的差异得到不同的结果，但训练过程中则会受到批次中其他样本的影响所以有正则化效果</li><li>如果受到 GPU 内存限制，不得不使用很小的 batch_size，训练阶段时使用的 mini-batch 上的均值和方差的估计和预测阶段时使用的全体样本上的均值和方差的估计差异可能会较大，效果会变差。这时候，可以尝试 LayerNorm 或者 GroupNorm 等归一化方法</li></ul><p>BatchNorm 使用：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
batch_size<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token operator">*</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">128</span><span class="token operator">*</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 
<span class="token comment"># 创建了 2D 批量归一化层</span>
bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>channel<span class="token punctuation">,</span>affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
bn_out <span class="token operator">=</span> bn<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
channel_mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>bn_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 提取张量中第一个通道的所有像素，计算该通道像素的均值和标准差</span>
channel_std <span class="token operator">=</span> torch<span class="token punctuation">.</span>std<span class="token punctuation">(</span>bn_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;channel mean:&quot;</span><span class="token punctuation">,</span>channel_mean<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 1.043081283569336e-07</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;channel std:&quot;</span><span class="token punctuation">,</span>channel_std<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 1.0000009536743164</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-2、卷积网络相关层" tabindex="-1"><a class="header-anchor" href="#_6-2-2、卷积网络相关层" aria-hidden="true">#</a> 6.2.2、卷积网络相关层</h3><ul><li>nn.Conv1d：普通一维卷积，常用于文本。参数个数 = 输入通道数 × 卷积核尺寸(如3)×卷积核个数 + 卷积核尺寸(如3）</li><li>nn.Conv2d：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数 + 卷积核尺寸(如3乘3)。) 通过调整 dilation 参数大于 1，可以变成空洞卷积，增加感受野。 通过调整 groups 参数不为1，可以变成分组卷积。分组卷积中每个卷积核仅对其对应的一个分组进行操作。 当 groups 参数数量等于输入通道数时，相当于 tensorflow 中的二维深度卷积层tf.keras.layers.DepthwiseConv2D。 利用分组卷积和1乘1卷积的组合操作，可以构造相当于 Keras 中的二维深度可分离卷积层tf.keras.layers.SeparableConv2D。</li><li>nn.Conv3d：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数 + 卷积核尺寸(如3乘3乘3) 。</li><li>nn.MaxPool1d: 一维最大池化。</li><li>nn.MaxPool2d：二维最大池化。一种下采样方式。没有需要训练的参数。</li><li>nn.MaxPool3d：三维最大池化。</li><li>nn.AdaptiveMaxPool2d：二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的尺寸来反向推算池化算子的padding,stride等参数。</li><li>nn.FractionalMaxPool2d：二维分数最大池化。普通最大池化通常输入尺寸是输出的整数倍。而分数最大池化则可以不必是整数。分数最大池化使用了一些随机采样策略，有一定的正则效果，可以用它来代替普通最大池化和 Dropout 层。</li><li>nn.AvgPool2d：二维平均池化。</li><li>nn.AdaptiveAvgPool2d：二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</li><li>nn.ConvTranspose2d：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</li><li>nn.Upsample：上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略为&quot;nearest&quot;最邻近策略或&quot;linear&quot;线性插值策略。</li><li>nn.Unfold：滑动窗口提取层。其参数和卷积操作nn.Conv2d相同。实际上，卷积操作可以等价于nn.Unfold和nn.Linear以及nn.Fold的一个组合。 其中nn.Unfold操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。利用nn.Linear将nn.Unfold的输出和卷积核做乘法后，再使用 nn.Fold操作将结果转换成输出图片形状。</li><li>nn.Fold：逆滑动窗口提取层。</li></ul><p>各种常用的卷积层和上采样层：</p><ul><li><p>普通卷积</p><p>![image-20240723134724344](E:\Study=my repo\vuepress-hope-bloc\my-docs\src\code\人工智能\Pytorch\assets\image-20240723134724344.png)</p></li><li><p>空洞卷积</p><p>![image-20240723134732374](E:\Study=my repo\vuepress-hope-bloc\my-docs\src\code\人工智能\Pytorch\assets\image-20240723134732374.png)</p></li><li><p>分组卷积</p></li><li><p>深度可分离卷积</p></li><li><p>转置卷积</p></li><li><p>上采样层</p></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

<span class="token comment"># 卷积输出尺寸计算公式 o = (i + 2*p -k&#39;)//s  + 1 </span>
<span class="token comment"># 对空洞卷积 k&#39; = d(k-1) + 1</span>
<span class="token comment"># o 是输出尺寸，i 是输入尺寸，p 是 padding 大小， k 是卷积核尺寸， s是 stride 步长, d是 dilation 空洞参数</span>

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">25</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># i = 5</span>
filters <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># k = 2</span>

outputs <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> filters<span class="token punctuation">)</span> <span class="token comment"># o = (5+2*0-2)//1+1 = 4</span>
outputs_s2 <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment">#o = (5+2*0-2)//2+1 = 2</span>
outputs_p1 <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#o = (5+2*1-2)//1+1 = 6</span>
outputs_d2 <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>filters<span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment">#o = (5+2*0-(2(2-1)+1))//1+1 = 3</span>


<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

features <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features.shape:&quot;</span><span class="token punctuation">,</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 普通卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--conv--&quot;</span><span class="token punctuation">)</span>
conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
conv_out <span class="token operator">=</span> conv<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv_out.shape:&quot;</span><span class="token punctuation">,</span>conv_out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv.weight.shape:&quot;</span><span class="token punctuation">,</span>conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 分组卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--group conv--&quot;</span><span class="token punctuation">)</span>
conv_group <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
group_out <span class="token operator">=</span> conv_group<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;group_out.shape:&quot;</span><span class="token punctuation">,</span>group_out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv_group.weight.shape:&quot;</span><span class="token punctuation">,</span>conv_group<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 深度可分离卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--separable conv--&quot;</span><span class="token punctuation">)</span>
depth_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>
oneone_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
separable_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>depth_conv<span class="token punctuation">,</span>oneone_conv<span class="token punctuation">)</span>
separable_out <span class="token operator">=</span> separable_conv<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;separable_out.shape:&quot;</span><span class="token punctuation">,</span>separable_out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;depth_conv.weight.shape:&quot;</span><span class="token punctuation">,</span>depth_conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;oneone_conv.weight.shape:&quot;</span><span class="token punctuation">,</span>oneone_conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 转置卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--conv transpose--&quot;</span><span class="token punctuation">)</span>
conv_t <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
features_like <span class="token operator">=</span> conv_t<span class="token punctuation">(</span>conv_out<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features_like.shape:&quot;</span><span class="token punctuation">,</span>features_like<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv_t.weight.shape:&quot;</span><span class="token punctuation">,</span>conv_t<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;inputs:&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>

nearest <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;nearest&#39;</span><span class="token punctuation">)</span>
bilinear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>mode<span class="token operator">=</span><span class="token string">&quot;bilinear&quot;</span><span class="token punctuation">,</span>align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;nearest(inputs)：&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>nearest<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;bilinear(inputs)：&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>bilinear<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-3、循环网络相关层" tabindex="-1"><a class="header-anchor" href="#_6-2-3、循环网络相关层" aria-hidden="true">#</a> 6.2.3、循环网络相关层</h3><ul><li>nn.LSTM：长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置 bidirectional = True 时可以得到双向 LSTM。需要注意的时，默认的输入和输出形状是(seq,batch,feature), 如果需要将 batch 维度放在第 0 维，则要设置 batch_first 参数设置为 True</li><li>nn.GRU：门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快</li><li>nn.RNN：简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用</li><li>nn.LSTMCell：长短记忆循环网络单元。和 nn.LSTM 在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用</li><li>nn.GRUCell：门控循环网络单元。和 nn.GRU 在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用</li><li>nn.RNNCell：简单循环网络单元。和 nn.RNN 在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用</li></ul><p>各种 RNN 序列模型层(RNN,GRU,LSTM 等)可以用函数表示如下: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = f(h_{t-1},x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><p>公式的含义：t 时刻循环神经网络的输出向量 ℎ𝑡 由 t-1 时刻的输出向量 ℎ<sub>𝑡−1</sub> 和 t 时刻的输入 𝑖<sub>𝑡</sub> 变换而来</p><ul><li><p>LSTM 结构解析（参考文章：《人人都能看懂的 LSTM》<a href="https://zhuanlan.zhihu.com/p/32085405%EF%BC%89" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/32085405）<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>LSTM 通过引入了三个门来控制信息的传递，分别是遗忘门，输入门 和输出门 。三个门的作用为：</p><ol><li>遗忘门: 遗忘门 𝑓<sub>𝑡</sub> 控制上一时刻的内部状态 需要遗忘多少信息；</li><li>输入门: 输入门 𝑖<sub>𝑡</sub> 控制当前时刻的候选状态 有多少信息需要保存；</li><li>输出门: 输出门 𝑜<sub>𝑡</sub> 控制当前时刻的内部状态 有多少信息需要输出给外部状态 ；</li></ol></li><li><p>GRU 结构解析（参考文章：《人人都能看懂的 GRU》<a href="https://zhuanlan.zhihu.com/p/32481747%EF%BC%89" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/32481747）<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>GRU 的结构比 LSTM 更为简单一些，GRU 只有两个门，更新门和重置门</p><ol><li>更新门：更新门用于控制每一步ℎ𝑡ℎ𝑡被更新的比例，更新门越大，ℎ<sub>𝑡</sub> 更新幅度越大。</li><li>重置门：重置门用于控制更新候选向量 ℎ̃<sub>𝑡</sub> 中前一步的状态 ℎ<sub>𝑡−1</sub> 被重新放入的比例，重置门越大，更新候选向量中 ℎ<sub>𝑡−1</sub> 被重新放进来的比例越大</li></ol><p>公式中的小圈表示哈达玛积，也就是两个向量逐位相乘</p><p>其中 1式和 2式计算的是更新门 𝑢<sub>𝑡</sub> 和重置门 𝑟<sub>𝑡</sub>，是两个长度和 ℎ<sub>𝑡</sub> 相同的向量。</p><p>注意到 4式实际上和 ResNet 的残差结构是相似的，都是 f(x) = x + g(x) 的形式，可以有效地防止长序列学习反向传播过程中梯度消失问题。</p></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span> <span class="token comment">#batch_size, seq_length, features</span>

gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
gru_output<span class="token punctuation">,</span>gru_hn <span class="token operator">=</span> gru<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--GRU--&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;gru_output.shape:&quot;</span><span class="token punctuation">,</span>gru_output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;gru_hn.shape:&quot;</span><span class="token punctuation">,</span>gru_hn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\n&quot;</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--LSTM--&quot;</span><span class="token punctuation">)</span>
lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
lstm_output<span class="token punctuation">,</span><span class="token punctuation">(</span>lstm_hn<span class="token punctuation">,</span>lstm_cn<span class="token punctuation">)</span> <span class="token operator">=</span> lstm<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;lstm_output.shape:&quot;</span><span class="token punctuation">,</span>lstm_output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;lstm_hn.shape:&quot;</span><span class="token punctuation">,</span>lstm_hn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;lstm_cn.shape:&quot;</span><span class="token punctuation">,</span>lstm_cn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary
summary<span class="token punctuation">(</span>gru<span class="token punctuation">,</span>input_data<span class="token operator">=</span>inputs<span class="token punctuation">)</span><span class="token punctuation">;</span>
summary<span class="token punctuation">(</span>lstm<span class="token punctuation">,</span>input_data<span class="token operator">=</span>inputs<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-4、transformer-相关层" tabindex="-1"><a class="header-anchor" href="#_6-2-4、transformer-相关层" aria-hidden="true">#</a> 6.2.4、Transformer 相关层</h3><ul><li>nn.Transformer：Transformer网络结构。Transformer网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。它是目前NLP任务的主流模型的主要构成部分。</li><li>nn.TransformerEncoder：Transformer编码器结构。由多个 nn.TransformerEncoderLayer编码器层组成。</li><li>nn.TransformerDecoder：Transformer解码器结构。由多个 nn.TransformerDecoderLayer解码器层组成。</li><li>nn.TransformerEncoderLayer：Transformer的编码器层。主要由Multi-Head self-Attention, Feed-Forward前馈网络, LayerNorm归一化层, 以及残差连接层组成。</li><li>nn.TransformerDecoderLayer：Transformer的解码器层。主要由Masked Multi-Head self-Attention, Multi-Head cross-Attention, Feed-Forward前馈网络, LayerNorm归一化层, 以及残差连接层组成。</li><li>nn.MultiheadAttention：多头注意力层。用于在序列方向上融合特征。使用的是Scaled Dot Production Attention，并引入了多个注意力头。</li></ul><p>参考阅读材料：<br> Transformer知乎原理讲解：<a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/48508221<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><br> Transformer哈佛博客代码讲解：<a href="http://nlp.seas.harvard.edu/annotated-transformer/" target="_blank" rel="noopener noreferrer">http://nlp.seas.harvard.edu/annotated-transformer/<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

<span class="token comment"># 验证 MultiheadAttention 和 head 数量无关</span>
inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span> <span class="token comment"># batch_size, seq_length, features</span>
attention_h8 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>
    embed_dim <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span>
    num_heads <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>
    bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    batch_first<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
attention_h16 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>
    embed_dim <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span>
    num_heads <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>
    bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    batch_first<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
out_h8 <span class="token operator">=</span> attention_h8<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span>
out_h16 <span class="token operator">=</span> attention_h16<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span>

<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary 
summary<span class="token punctuation">(</span>attention_h8<span class="token punctuation">,</span>input_data_args<span class="token operator">=</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
summary<span class="token punctuation">(</span>attention_h16<span class="token punctuation">,</span>input_data_args<span class="token operator">=</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">from</span> copy <span class="token keyword">import</span> deepcopy

<span class="token comment"># 多头注意力的一种简洁实现</span>

<span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Compute &#39;Scaled Dot Product Attention&#39;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        d_k <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        scores <span class="token operator">=</span> query@key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_k<span class="token operator">**</span><span class="token number">0.5</span>     
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e20</span><span class="token punctuation">)</span>
        p_attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            p_attn <span class="token operator">=</span> dropout<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>
        <span class="token keyword">return</span> p_attn@value<span class="token punctuation">,</span> p_attn
    
<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> h<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Take in model size and number of heads.&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> d_model <span class="token operator">%</span> h <span class="token operator">==</span> <span class="token number">0</span>
        <span class="token comment"># We assume d_v always equals d_k</span>
        self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> d_model <span class="token operator">//</span> h
        self<span class="token punctuation">.</span>h <span class="token operator">=</span> h
        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>deepcopy<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Implements Figure 2&quot;</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># Same mask applied to all h heads.</span>
            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        nbatches <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span>
        query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value <span class="token operator">=</span> \
            <span class="token punctuation">[</span>l<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
             <span class="token keyword">for</span> l<span class="token punctuation">,</span> x <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>linears<span class="token punctuation">,</span> <span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 2) Apply attention on all the projected vectors in batch. </span>
        x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">,</span> 
                                 dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
        
        <span class="token comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> \
             <span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h <span class="token operator">*</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-5、自定义模型层" tabindex="-1"><a class="header-anchor" href="#_6-2-5、自定义模型层" aria-hidden="true">#</a> 6.2.5、自定义模型层</h3><p>如果 Pytorch 的内置模型层不能够满足需求，我们也可以通过继承 nn.Module 基类构建自定义的模型层</p><p>实际上，pytorch 不区分模型和模型层，都是通过继承 nn.Module 进行构建。</p><p>因此，我们只要继承 nn.Module 基类并实现 forward 方法即可自定义模型层。</p><p>下面是 Pytorch 的 nn.Linear 层的源码，我们可以仿照它来自定义模型层。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    __constants__ <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;in_features&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;out_features&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> bias<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">&#39;bias&#39;</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            fan_in<span class="token punctuation">,</span> _ <span class="token operator">=</span> nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>_calculate_fan_in_and_fan_out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
            bound <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan_in<span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">extra_repr</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string">&#39;in_features={}, out_features={}, bias={}&#39;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_6-3、损失函数-losses" tabindex="-1"><a class="header-anchor" href="#_6-3、损失函数-losses" aria-hidden="true">#</a> 6.3、损失函数 losses</h2><p>一般来说，监督学习的目标函数由损失函数和正则化项组成。(Objective = Loss + Regularization)</p><p>Pytorch 中的损失函数一般在训练模型时候指定</p><blockquote><p>Pytorch 中内置的损失函数的参数和 tensorflow 不同，是 y_pred 在前，y_true 在后，而 Tensorflow 是 y_true 在前，y_pred 在后</p></blockquote><ul><li><p>对于 <strong>回归模型</strong>，通常使用的内置损失函数是 <strong>均方损失函数 nn.MSELoss</strong></p></li><li><p>对于 <strong>二分类模型</strong>，通常使用的是 <strong>二元交叉熵损失函数nn.BCELoss</strong> (输入已经是 sigmoid 激活函数之后的结果) 或者 nn.BCEWithLogitsLoss (输入尚未经过 nn.Sigmoid 激活函数)</p></li><li><p>对于 <strong>多分类模型</strong>，一般推荐使用 <strong>交叉熵损失函数 nn.CrossEntropyLoss</strong> (y_true 需要是一维的，是类别编码。y_pred 未经过 nn.Softmax 激活)</p><p>此外，如果多分类的 y_pred 经过了 nn.LogSoftmax 激活，可以使用 nn.NLLLoss 损失函数(The negative log likelihood loss)。 这种方法和直接使用 nn.CrossEntropyLoss 等价</p></li></ul><p>如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量 y_pred，y_true 作为输入参数，并输出一个标量作为损失函数值</p><p>Pytorch 中的正则化项一般通过自定义的方式和损失函数一起添加作为目标函数</p><p>如果仅仅使用 L2 正则化，也可以利用优化器的 weight_decay 参数来实现相同的效果</p><h3 id="_6-3-1、内置损失函数" tabindex="-1"><a class="header-anchor" href="#_6-3-1、内置损失函数" aria-hidden="true">#</a> 6.3.1、内置损失函数</h3><p>内置的损失函数一般有类的实现和函数的实现两种形式</p><p>类的实现形式通常是调用函数的实现形式并用 nn.Module 封装后得到的</p><p>我们常用的是类的实现形式。它们封装在 torch.nn 模块下，并且类名以 Loss 结尾</p><p>常用的一些内置损失函数说明如下：</p><ul><li>nn.MSELoss（均方误差损失，也叫做 L2 损失，用于回归）</li><li>nn.L1Loss （L1 损失，也叫做绝对值误差损失，用于回归）</li><li>nn.SmoothL1Loss (平滑 L1 损失，当输入在 -1 到 1 之间时，平滑为 L2 损失，用于回归)</li><li>nn.BCELoss (二元交叉熵，用于二分类，输入已经过 nn.Sigmoid 激活，对不平衡数据集可以用 weigths 参数调整类别权重)</li><li>nn.BCEWithLogitsLoss (二元交叉熵，用于二分类，输入未经过 nn.Sigmoid 激活)</li><li>nn.CrossEntropyLoss (交叉熵，用于多分类，要求label为稀疏编码，输入未经过 nn.Softmax 激活，对不平衡数据集可以用 weigths 参数调整类别权重)</li><li>nn.NLLLoss (负对数似然损失，用于多分类，要求 label 为稀疏编码，输入经过 nn.LogSoftmax 激活)</li><li>nn.KLDivLoss (KL 散度损失，也叫相对熵，等于交叉熵减去信息熵，用于标签为概率值的多分类，要求输入经过 nn.LogSoftmax 激活)</li><li>nn.CosineSimilarity(余弦相似度，可用于多分类)</li><li>nn.AdaptiveLogSoftmaxWithLoss (一种适合非常多类别且类别分布很不均衡的损失函数，会自适应地将多个小类别合成一个 cluster)</li></ul><p>二元交叉熵、多元交叉熵、对数损失 LogLoss、负对数似然损失 NLLLoss、KL 散度之间的区别和联系：略</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

<span class="token comment"># nn.BCELoss() 和 nn.BCEWithLogitsLoss() 关系</span>
y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

bce <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span><span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>bce<span class="token punctuation">)</span>


bce_logits <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>bce_logits<span class="token punctuation">)</span>

<span class="token comment"># nn.CrossEntropyLoss() 和  nn.NLLLoss() 关系</span>
y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 直接调用交叉熵损失</span>
ce <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ce<span class="token punctuation">)</span>

<span class="token comment"># 等价于先计算 nn.LogSoftmax 激活，再调用 nn.NLLLoss</span>
y_pred_logsoftmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span>
nll <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred_logsoftmax<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>nll<span class="token punctuation">)</span>

<span class="token comment"># nn.CrossEntropyLoss() 和  KLDivLoss 关系</span>
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y_true <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

ce <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">&quot;mean&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ce<span class="token punctuation">)</span>


<span class="token comment">#KLDivLoss要求target为向量形式编码且preds经过LogSoftmax激活</span>
pred <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
target <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>y_true<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
kl <span class="token operator">=</span> nn<span class="token punctuation">.</span>KLDivLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">&quot;batchmean&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>kl<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-2、自定义损失函数" tabindex="-1"><a class="header-anchor" href="#_6-3-2、自定义损失函数" aria-hidden="true">#</a> 6.3.2、自定义损失函数</h3><p>自定义损失函数接收两个张量 y_pred , y_true 作为输入参数，并输出一个标量作为损失函数值</p><p>也可以对 nn.Module 进行子类化，重写 forward 方法实现损失的计算逻辑，从而得到损失函数的类的实现</p><h4 id="自定义损失函数-focalloss-范例" tabindex="-1"><a class="header-anchor" href="#自定义损失函数-focalloss-范例" aria-hidden="true">#</a> 自定义损失函数 FocalLoss 范例</h4><p>Focal Loss是一种对 binary_crossentropy 的改进损失函数形式</p><p>它在样本不均衡和存在较多易分类的样本时相比 binary_crossentropy 具有明显的优势</p><p>它有两个可调参数，alpha 参数和 gamma 参数。其中 alpha 参数主要用于衰减负样本的权重，gamma 参数主要用于衰减容易训练样本的权重。从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做 Focal Loss。</p><p>![image-20240724160609623](E:\Study=my repo\vuepress-hope-bloc\my-docs\src\code\人工智能\Pytorch\assets\image-20240724160609623.png)</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">class</span> <span class="token class-name">FocalLoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> alpha

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span><span class="token punctuation">:</span>
        bce <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>reduction <span class="token operator">=</span> <span class="token string">&quot;none&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
        p_t <span class="token operator">=</span> <span class="token punctuation">(</span>y_true <span class="token operator">*</span> y_pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_true<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        alpha_factor <span class="token operator">=</span> y_true <span class="token operator">*</span> self<span class="token punctuation">.</span>alpha <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_true<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>alpha<span class="token punctuation">)</span>
        modulating_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> p_t<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>alpha_factor <span class="token operator">*</span> modulating_factor <span class="token operator">*</span> bce<span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss
    
    
<span class="token comment"># 困难样本</span>
y_pred_hard <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true_hard <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 容易样本</span>
y_pred_easy <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true_easy <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

focal_loss <span class="token operator">=</span> FocalLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
bce_loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;focal_loss(easy samples):&quot;</span><span class="token punctuation">,</span> focal_loss<span class="token punctuation">(</span>y_pred_easy<span class="token punctuation">,</span>y_true_easy<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.0005)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;bce_loss(easy samples):&quot;</span><span class="token punctuation">,</span> bce_loss<span class="token punctuation">(</span>y_pred_easy<span class="token punctuation">,</span>y_true_easy<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.1054)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;focal_loss(hard samples):&quot;</span><span class="token punctuation">,</span> focal_loss<span class="token punctuation">(</span>y_pred_hard<span class="token punctuation">,</span>y_true_hard<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.0866)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;bce_loss(hard samples):&quot;</span><span class="token punctuation">,</span> bce_loss<span class="token punctuation">(</span>y_pred_hard<span class="token punctuation">,</span>y_true_hard<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.6931)</span>


<span class="token comment">#可见 focal_loss 让容易样本的权重衰减到原来的 0.0005/0.1054 = 0.00474</span>
<span class="token comment">#而让困难样本的权重只衰减到原来的 0.0866/0.6931=0.12496</span>

<span class="token comment"># 因此相对而言，focal_loss可以衰减容易样本的权重。</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="sceloss-案例" tabindex="-1"><a class="header-anchor" href="#sceloss-案例" aria-hidden="true">#</a> SCELoss 案例</h4><p>Symmetric Cross Entropy Loss 也是一种对交叉熵损失的改进损失，主要用在标签中存在明显噪声的场景。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">ce</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span>
    p <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>p<span class="token punctuation">,</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">1e-4</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>y<span class="token punctuation">,</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">1e-4</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>y<span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>y<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>p<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">rce</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ce<span class="token punctuation">(</span>p<span class="token punctuation">,</span>y<span class="token punctuation">)</span>

<span class="token comment"># 正常标签</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.8</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token operator">/</span>ce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(8.2502)</span>


<span class="token comment"># 噪声标签</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.8</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token operator">/</span>ce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(4.5786)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span>  torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

<span class="token keyword">class</span> <span class="token class-name">SCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SCELoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes
        self<span class="token punctuation">.</span>a <span class="token operator">=</span> a <span class="token comment">#两个超参数</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b
        self<span class="token punctuation">.</span>cross_entropy <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># CE 部分，正常的交叉熵损失</span>
        ce <span class="token operator">=</span> self<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
        <span class="token comment"># RCE</span>
        pred <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        label_one_hot <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        label_one_hot <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>label_one_hot<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token comment">#最小设为 1e-4，即 A 取 -4</span>
        rce <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>pred <span class="token operator">*</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>label_one_hot<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>a <span class="token operator">*</span> ce <span class="token operator">+</span> self<span class="token punctuation">.</span>b <span class="token operator">*</span> rce<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss
    
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-3、l1-和-l2正则化项" tabindex="-1"><a class="header-anchor" href="#_6-3-3、l1-和-l2正则化项" aria-hidden="true">#</a> 6.3.3、L1 和 L2正则化项</h3><p>L1 正则、L2 正则、Dropout、Early_stopping 是神经网络常用的正则化方法（正则化：防止模型在训练数据上过拟合）</p><p>通常认为 L1 正则化可以产生稀疏权值矩阵，即产生一个参数稀疏的模型。而 L2 正则化可以让模型的参数取绝对值较小的数</p><p>考虑两种正则化函数的等值面与原始 Loss 函数的等值面的关系。</p><ul><li><p>以二维情况为例，L1 正则化函数的等值面是个菱形，L2 正则化函数的等值面是个圆形。最优参数必定取在正则化函数的某条等值面和原始Loss函数的某条等值面的切点处。</p></li><li><p>从求导角度考虑，最优参数是个极值点，要求该处 正则化函数的梯度等于 原始Loss函数的梯度的负数。</p><p>而梯度方向必定垂直于等值面的切线方向，所以可以推断必定极值点必定在正则化函数某条等值面和原始Loss函数的某条等值面的切点处。</p></li><li><p>从数值角度考虑，如果该极值点不在两个等值面的切点，那么沿着原始函数Loss的等值面(原始Loss不变)，一定可以找到一个点正则化函数取值更小。</p><p>这样就用反证法证明了最优参数必定取在正则化函数的某条等值面和原始Loss函数的某条等值面的切点处。</p></li></ul><p>由于 L1 正则化函数的等值面是个菱形，更容易和凸的 Loss 函数的等值面相切在坐标轴上，所以倾向于取得参数稀疏的模型，而 L2 正则化则更倾向于使得极小点到坐标原点的距离更近，但不会导致参数稀疏。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token comment"># L2 正则化</span>
<span class="token keyword">def</span> <span class="token function">L2Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>alpha<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l2_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span> <span class="token comment"># 一般不对偏置项使用正则</span>
            l2_loss <span class="token operator">=</span> l2_loss <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> alpha <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l2_loss

<span class="token comment"># L1 正则化</span>
<span class="token keyword">def</span> <span class="token function">L1Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>beta<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l1_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
            l1_loss <span class="token operator">=</span> l1_loss <span class="token operator">+</span>  beta <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l1_loss
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-4、l1l2-正则项使用完整范例" tabindex="-1"><a class="header-anchor" href="#_6-3-4、l1l2-正则项使用完整范例" aria-hidden="true">#</a> 6.3.4、L1L2 正则项使用完整范例</h3><p>以一个二分类问题为例，演示给模型的目标函数添加自定义L1和L2正则化项的方法。</p><p>这个范例同时演示了以下 FocalLoss 的使用。</p><ol><li><p>准备数据</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd 
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span>DataLoader<span class="token punctuation">,</span>TensorDataset
<span class="token keyword">import</span> torchkeras 
<span class="token operator">%</span>matplotlib inline
<span class="token operator">%</span>config InlineBackend<span class="token punctuation">.</span>figure_format <span class="token operator">=</span> <span class="token string">&#39;svg&#39;</span>

<span class="token comment"># 正负样本数量</span>
n_positive<span class="token punctuation">,</span>n_negative <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">6000</span>

<span class="token comment"># 生成正样本, 小圆环分布</span>
r_p <span class="token operator">=</span> <span class="token number">5.0</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">,</span>size <span class="token operator">=</span> <span class="token punctuation">[</span>n_positive<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
theta_p <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>pi<span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span>n_positive<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xp <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>r_p<span class="token operator">*</span>torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>theta_p<span class="token punctuation">)</span><span class="token punctuation">,</span>r_p<span class="token operator">*</span>torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>theta_p<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
Yp <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>r_p<span class="token punctuation">)</span>

<span class="token comment"># 生成负样本, 大圆环分布</span>
r_n <span class="token operator">=</span> <span class="token number">8.0</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">,</span>size <span class="token operator">=</span> <span class="token punctuation">[</span>n_negative<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
theta_n <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>pi<span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span>n_negative<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xn <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>r_n<span class="token operator">*</span>torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>theta_n<span class="token punctuation">)</span><span class="token punctuation">,</span>r_n<span class="token operator">*</span>torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>theta_n<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
Yn <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>r_n<span class="token punctuation">)</span>

<span class="token comment"># 汇总样本</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>Xp<span class="token punctuation">,</span>Xn<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>Yp<span class="token punctuation">,</span>Yn<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>


<span class="token comment"># 可视化</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;r&quot;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;positive&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;negative&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment"># 创建 TensorDataset，用于存储特征和标签</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span>Y<span class="token punctuation">)</span>
<span class="token comment"># 将数据集按 7:3 比例划分为训练集和验证集，并创建 DataLoader 用于批量数据加载</span>
ds_train<span class="token punctuation">,</span>ds_val <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>ds<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token operator">-</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token comment">#获取一个批次的数据</span>
features<span class="token punctuation">,</span>labels <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>dl_train<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>定义模型</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 简单的三层全连接神经网络。fc1、fc2 和 fc3 分别是三层的全连接层</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y
        
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span> 

<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary

summary<span class="token punctuation">(</span>net<span class="token punctuation">,</span>features<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>训练模型</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># L2正则化</span>
<span class="token keyword">def</span> <span class="token function">L2Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>alpha<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l2_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span> <span class="token comment">#一般不对偏置项使用正则</span>
            l2_loss <span class="token operator">=</span> l2_loss <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> alpha <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l2_loss

<span class="token comment"># L1正则化</span>
<span class="token keyword">def</span> <span class="token function">L1Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>beta<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l1_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
            l1_loss <span class="token operator">=</span> l1_loss <span class="token operator">+</span>  beta <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l1_loss
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> KerasModel
<span class="token keyword">from</span> torchkeras<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> AUC

net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 将L2正则和L1正则添加到FocalLoss损失，一起作为目标函数</span>
<span class="token keyword">def</span> <span class="token function">focal_loss_with_regularization</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span><span class="token punctuation">:</span>
    y_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span>
    focal <span class="token operator">=</span> FocalLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_probs<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span> 
    l2_loss <span class="token operator">=</span> L2Loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span><span class="token number">0.001</span><span class="token punctuation">)</span> <span class="token comment">#注意设置正则化项系数</span>
    l1_loss <span class="token operator">=</span> L1Loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span><span class="token number">0.001</span><span class="token punctuation">)</span>
    total_loss <span class="token operator">=</span> focal <span class="token operator">+</span> l2_loss <span class="token operator">+</span> l1_loss
    <span class="token keyword">return</span> total_loss


optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.002</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> KerasModel<span class="token punctuation">(</span>net<span class="token operator">=</span>net<span class="token punctuation">,</span>
                   loss_fn <span class="token operator">=</span> focal_loss_with_regularization <span class="token punctuation">,</span>
                   metrics_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;auc&quot;</span><span class="token punctuation">:</span>AUC<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                   optimizer<span class="token operator">=</span> optimizer <span class="token punctuation">)</span>


dfhistory <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_data<span class="token operator">=</span>dl_train<span class="token punctuation">,</span>
      val_data<span class="token operator">=</span>dl_val<span class="token punctuation">,</span>
      epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
      ckpt_path<span class="token operator">=</span><span class="token string">&#39;checkpoint&#39;</span><span class="token punctuation">,</span>
      patience<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
      monitor<span class="token operator">=</span><span class="token string">&#39;val_auc&#39;</span><span class="token punctuation">,</span>
      mode<span class="token operator">=</span><span class="token string">&#39;max&#39;</span><span class="token punctuation">,</span>
      plot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
      cpu<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 结果可视化</span>
fig<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax1<span class="token punctuation">,</span>ax2<span class="token punctuation">)</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>ncols<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token string">&quot;r&quot;</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;positive&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;negative&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
ax1<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">&quot;y_true&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

Xp_pred <span class="token operator">=</span> X<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
Xn_pred <span class="token operator">=</span> X<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

ax2<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xp_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xp_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;r&quot;</span><span class="token punctuation">)</span>
ax2<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xn_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xn_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">)</span>
ax2<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;positive&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;negative&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
ax2<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">&quot;y_pred&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol><h3 id="_6-3-5、通过优化器实现-l2-正则化" tabindex="-1"><a class="header-anchor" href="#_6-3-5、通过优化器实现-l2-正则化" aria-hidden="true">#</a> 6.3.5、通过优化器实现 L2 正则化</h3><p>如果仅仅需要使用 L2 正则化，那么也可以利用优化器的 weight_decay 参数来实现。</p><p>weight_decay 参数可以设置参数在训练过程中的衰减，这和 L2 正则化的作用效果等价</p><p>Pytorch 的优化器支持一种称之为 Per-parameter options 的操作，就是对每一个参数进行特定的学习率，权重衰减率指定，以满足更为细致的要求。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>weight_params <span class="token operator">=</span> <span class="token punctuation">[</span>param <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token string">&quot;bias&quot;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">]</span>
bias_params <span class="token operator">=</span> <span class="token punctuation">[</span>param <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token string">&quot;bias&quot;</span> <span class="token keyword">in</span> name<span class="token punctuation">]</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">&#39;params&#39;</span><span class="token punctuation">:</span> weight_params<span class="token punctuation">,</span> <span class="token string">&#39;weight_decay&#39;</span><span class="token punctuation">:</span><span class="token number">1e-5</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                             <span class="token punctuation">{</span><span class="token string">&#39;params&#39;</span><span class="token punctuation">:</span> bias_params<span class="token punctuation">,</span> <span class="token string">&#39;weight_decay&#39;</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                            lr<span class="token operator">=</span><span class="token number">1e-2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_6-4、tensorboard-可视化" tabindex="-1"><a class="header-anchor" href="#_6-4、tensorboard-可视化" aria-hidden="true">#</a> 6.4、TensorBoard 可视化</h2><p>在炼丹过程中，如果能够使用丰富的图像来展示模型的结构，指标的变化，参数的分布，输入的形态等信息，无疑会提升我们对问题的洞察力，并增加许多炼丹的乐趣。</p><p>ensorBoard 正是这样一个神奇的炼丹可视化辅助工具。它原是 TensorFlow 的小弟，但它也能够很好地和 Pytorch 进行配合。甚至在 Pytorch 中使用 TensorBoard 比 TensorFlow 中使用 TensorBoard 还要来的更加简单和自然。</p><h3 id="_6-4-0、tensorboard-可视化概述" tabindex="-1"><a class="header-anchor" href="#_6-4-0、tensorboard-可视化概述" aria-hidden="true">#</a> 6.4.0、Tensorboard 可视化概述</h3><p>pytorch 中利用 TensorBoard 可视化的大概过程：</p><ul><li>首先在 Pytorch 中指定一个目录创建一个 torch.utils.tensorboard.SummaryWriter 日志写入器</li><li>据需要可视化的信息，利用日志写入器将相应信息日志写入我们指定的目录</li><li>最后就可以传入日志目录作为参数启动 TensorBoard</li></ul><p>主要介绍 Pytorch 中利用 TensorBoard 进行如下方面信息的可视化的方法</p><ul><li>可视化模型结构： writer.add_graph</li><li>可视化指标变化： writer.add_scalar</li><li>可视化参数分布： writer.add_histogram</li><li>可视化原始图像： writer.add_image 或 writer.add_images</li><li>可视化人工绘图： writer.add_figure</li></ul><p>作者在 torchkeras 库中集成了一个 torchkeras.callback.TensorBoard 回调函数工具，</p><p>利用该工具配合 torchkeras.LightModel 可以用极少的代码在 TensorBoard 中实现绝大部分常用的可视化功能。</p><p>包括：</p><ul><li>可视化模型结构</li><li>可视化指标变化</li><li>可视化参数分布</li><li>可视化超参调整</li></ul></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/T4mako/T4mako.github.io/edit/main/src/code/python/Machine Learning/Pytorch/6、Pytorch 中阶 API.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><!----><!----></div></footer><!----><div id="comment" class="waline-wrapper" darkmode="false" style="display:block;"><div data-waline provider="Waline" pageview="false"><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">昵称</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">邮箱</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">网址</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="请留言。(填写邮箱可在被回复时收到邮件提醒)"></textarea><div class="wl-preview" style="display:none;"><hr><h4>预览:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="表情" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="表情包"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="上传图片"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="预览"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-captcha-container"></div><div class="wl-text-number">0 <!--v-if-->  字</div><button type="button" class="wl-btn">登录</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->提交<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="搜索表情包"><!--v-if--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> 评论</div><ul class="wl-sort"><!--[--><li class="active">按正序</li><li class="">按倒序</li><li class="">按热度</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.15.5</div></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">T4mako's blog</div><div class="vp-copyright">Copyright © 2024 T4mako</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app-392a5c85.js" defer></script>
  </body>
</html>
