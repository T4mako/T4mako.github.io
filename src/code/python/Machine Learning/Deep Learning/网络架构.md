---
title: 机器学习网络层
---
## 全连接层 fc

### 线性回归

```py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 生成示例数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3 * X + 噪声

# 将数据转换为 PyTorch 张量
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# 定义线性回归模型
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1) #（输入维度，输出维度）输入x，输出y

    def forward(self, x):
        return self.linear(x)

# 实例化模型
model = LinearRegressionModel()

# 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差损失
optimizer = optim.SGD(model.parameters(), lr=0.01)  # 随机梯度下降优化器

# 训练模型
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    
    # 前向传播
    y_pred = model(X_tensor)
    
    # 计算损失
    loss = criterion(y_pred, y_tensor)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

# 绘制结果
model.eval()
with torch.no_grad():
    predicted = model(X_tensor).numpy()
plt.scatter(X, y, color='blue', label='实际数据')
plt.plot(X, predicted, color='red', label='拟合直线')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('线性回归示例')
plt.show()
```

### Softmax 分类

Softmax 是多分类任务，隐藏层为线性层，输出为多个。

为了使输出为概率，使用 **softmax**  $sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, 2, \ldots, K$ 函数，通过真实值与预测值的概率经过 **交叉熵损失** 得到 0,1 值

```py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# 1. 数据准备：下载MNIST数据集，并进行预处理
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# 2. 定义模型：一个简单的两层神经网络
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)  # 输入层 (28*28像素)
        self.fc2 = nn.Linear(128, 10)     # 输出层 (10类)

    def forward(self, x):
        x = x.view(-1, 28*28)            # 将输入展平
        x = torch.relu(self.fc1(x))      # 第一个全连接层和ReLU激活函数
        x = self.fc2(x)                  # 第二个全连接层
        return torch.softmax(x, dim=1)   # 使用softmax函数生成概率分布

# 3. 初始化模型、损失函数和优化器
model = SimpleNet()
criterion = nn.CrossEntropyLoss()        # 交叉熵损失函数 (softmax和损失结合)
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 4. 训练模型
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for images, labels in train_loader:
        outputs = model(images)          # 前向传播
        loss = criterion(outputs, labels) # 计算损失

        optimizer.zero_grad()            # 梯度清零
        loss.backward()                  # 反向传播
        optimizer.step()                 # 更新模型参数

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 5. 测试模型
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)  # 预测最大概率的类
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')
```

### 多层感知机 MLP

感知机为二分类。给定输入 x，权重 w，偏移 b，感知机输出 0 或 1（有时为 -1 或 1）。它不能拟合 XOR 函数

多层感知机（MLP）的简单例子：

```py
import torch
import torch.nn as nn
import torch.optim as optim

# 定义MLP模型
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        # 定义第一层全连接层
        self.fc1 = nn.Linear(input_size, hidden_size)
        # 激活函数ReLU
        self.relu = nn.ReLU()
        # 定义第二层全连接层
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # 第一层 + 激活函数
        out = self.fc1(x)
        out = self.relu(out)
        # 输出层
        out = self.fc2(out)
        return out

# 模型实例化
input_size = 10   # 输入特征数
hidden_size = 20  # 隐藏层神经元数
output_size = 3   # 输出类别数

model = MLP(input_size, hidden_size, output_size)

# 损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 假设我们有一个大小为10的输入张量
x = torch.randn(5, input_size)  # 批次大小为5
labels = torch.randint(0, output_size, (5,))  # 随机生成5个类别标签

# 前向传播
outputs = model(x)

# 计算损失
loss = criterion(outputs, labels)

# 反向传播和优化
optimizer.zero_grad()  # 梯度清零
loss.backward()        # 反向传播
optimizer.step()       # 更新参数

print(f'Loss: {loss.item()}')
```

## 卷积神经网络 CNN

卷积神经网络通常用于 **图像、文本** 处理

整体架构大致为：conv，relu，conv，relu，pool，conv....，fc

> n 层神经网络：有 n 层带参数的层（pool，relu 不算层）

### 输入层

### 卷积层 conv（convolution）

卷积层中的 w 是卷积核，b 是偏置

![image-20240821222130729](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240821222130729.png)

计算的记过成为 **特征图**

一次卷积可以有多个 Filter，卷积后的深度就为 Filter 的个数（上述 7\*3\*3 经过 2 个 3\*3\*3 的 filter 变为 3\*3\*2 的特征图）卷积结果计算公式：

![image-20240822133826745](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240822133826745.png)

可以对一次卷积后的特征图再卷积再卷积

**卷积层** 涉及 **参数**：

- 滑动窗口步长
- 卷积核尺寸
- 边缘填充
- 卷积核个数

卷积层的权重（w + b）参数：所有卷积核的像素数 + 卷积核个数（偏置）

### 池化层 pool

用于 **压缩特征图**

例如 MAX POOLING： 2*2 的特征为一组，筛选最大的值

### 全连接层 fc

通过前面 conv 和 pool 得到最后的特征图（假设为 32\*32\*10），任务为 n 分类任务

将特征图拉成特征向量 [1,32\*32\*10] ，则全连接层参数为 [32\*32\*10,n]

## 经典多层神经网路

### AlexNet（2012 年）

### Vgg（2014 年） 

### Resnet（2015年）

vgg 在层数更多时，训练效果反而不好

resnet 将好的层保留，不好的层跳过。做法是通过对层数堆叠的值与同等映射的值进行比较

- 若果层数堆叠的值不好，将层数堆叠权重设为 0

  ![image-20240822141714082](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240822141714082.png)

## 迁移学习

深度学习的常见问题

- 训练数据量过少。导致欠拟合、过拟合
  - 使用数据增强
- 参数调节过多、时间成本大

**迁移学习** 就是使用 **他人的** 与自己项1目相似（数据集相似，参数相似）的项目的网络的 **w，b** 参数

- 继续训练 w，b 参数
- 微调网络（特别是 fc），继续训练一部分 w，b 参数

迁移学习的速度非常快、pytorch 官网有迁移学习的例子

## RNN 递归神经网络

RNN 的特点是带有时间序列，常用于 NLP，RNN 网络会将之前的结果全部记下来（h~0~，h~1~ .......），即输入过多，效果并不好

![image-20240826143426351](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240826143426351.png)

## LSTM 网络

相较于 RNN，添加控制参数 c，决定什么样的信息会被保留，什么样的会被遗忘

## 词向量模型 Word2Vec

文本向量化：

- 使用一定维度的向量描述词语
- 数据维度越高，能提供的信息越多
- 相似的词在特征表达中比较相似

词向量模型训练过程：

- 输入： 一个一个词，根据词向量大表（随机初始化）转换为向量
- 输出：一个词，输出可能性最高的词，类似多分类任务
- 更新词向量大表中输入数据的词向量

不同架构模型：

- CBOW
- Skip-gram

如果一个语料库很大，可能的结果就很多，最后一层 softmax 计算起来十分耗时。解决方案:

- 输入两个单词，看他们是不是前后对应的输入和输出，相当于一个二分类任务
- 由于训练数据来自文本，上述方法的结果都为 1，因此引入 **负采样**，即认为添加结果为 0 的输入

## GAN 对抗生成网络

对抗生成网络有 **生成器** 与 **判别器**

- 生成器生成想要的东西
- 判别器用于判别

![image-20240828214817355](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240828214817355.png)

## CycleGan

PatchGAN 的作用：

- 输入一个 N * N 的矩阵个，基于感受野来计算损失
- 基于感受野在特征图上的预测结果和标签（也需设置成 N * N）计算损失

## KNN

## 随机森林
