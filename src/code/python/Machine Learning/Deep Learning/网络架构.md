---
title: 深度学习
---
## 基础概念

### 机器学习概念

#### 梯度下降

#### 误差

- 训练误差：模型在训练数据上的误差
- 泛化误差：模型在新数据上的误差

#### k 折交叉验证

#### 过拟合与欠拟合

#### 权重衰退

- 使用均方范数作为硬性限制，控制模型复杂度，减少过拟合 

  通过限制参数值 w 的选择范围来控制模型容量$||w||^2 \le  \theta$ ，通常不限制 bias，小的 theta 意味着更强的正则项

#### dropout 丢弃法 

动机：一个好的模型需要对输入数据的扰动鲁棒，减少过拟合

- 使用有噪音的数据等价于 Tikhonov 正则
- dropout：在层之间加入噪音，丢弃一部分前一层的输入、后一层的输出

通常将 dropout 作用在 **隐藏全连接层的输出** 上，将其中的一些值随机设为 **0** 来控制模型复杂度，丢弃概率为 **超参数**，其他数会相应变大，保证均值方差一样

注意：dropout 用在训练模型上减少复杂性，在使用模型时一般不用 dropout

![image-20241010195927901](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241010195927901.png)

#### 数值稳定性

数值稳定性常见的两个问题：

- 梯度爆炸
  - 值超出阈值
  - 对学习率敏感
- 梯度消失
  - 梯度值变为 0
  - 训练无进展
  - 对于底部层尤为严重

因此，合理的权重初始值和激活函数的选取可以提升数值稳定性

### 数据集操作

数据集可分为

- 训练集
- 验证集
- 测试集

#### 数据增广

数据增强：则国家已有数据集，使得有更多的多样性。如在语音中加入背景噪声、改变图片的颜色和形状（翻转、切割、改颜色）

数据增广一般使用 torchvision

### 硬件、计算性能

https://zh.d2l.ai/chapter_computational-performance/index.html

### 微调

使用已训练好的模型的特征提取层与权重，调整自己崔侯的全连接层

![image-20241018164921787](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241018164921787.png)

## 全连接层 fc

### 线性回归

```py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 生成示例数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3 * X + 噪声

# 将数据转换为 PyTorch 张量
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# 定义线性回归模型
class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(1, 1) #（输入维度，输出维度）输入x，输出y

    def forward(self, x):
        return self.linear(x)

# 实例化模型
model = LinearRegressionModel()

# 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差损失
optimizer = optim.SGD(model.parameters(), lr=0.01)  # 随机梯度下降优化器

# 训练模型
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    
    # 前向传播
    y_pred = model(X_tensor)
    
    # 计算损失
    loss = criterion(y_pred, y_tensor)
    
    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

# 绘制结果
model.eval()
with torch.no_grad():
    predicted = model(X_tensor).numpy()
plt.scatter(X, y, color='blue', label='实际数据')
plt.plot(X, predicted, color='red', label='拟合直线')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('线性回归示例')
plt.show()
```

### Softmax 分类

Softmax 是多分类任务，隐藏层为线性层，输出为多个。

为了使输出为概率，使用 **softmax**  $sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, 2, \ldots, K$ 函数，通过真实值与预测值的概率经过 **交叉熵损失** 得到 0,1 值

```py
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# 1. 数据准备：下载MNIST数据集，并进行预处理
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# 2. 定义模型：一个简单的两层神经网络
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)  # 输入层 (28*28像素)
        self.fc2 = nn.Linear(128, 10)     # 输出层 (10类)

    def forward(self, x):
        x = x.view(-1, 28*28)            # 将输入展平
        x = torch.relu(self.fc1(x))      # 第一个全连接层和ReLU激活函数
        x = self.fc2(x)                  # 第二个全连接层
        return torch.softmax(x, dim=1)   # 使用softmax函数生成概率分布

# 3. 初始化模型、损失函数和优化器
model = SimpleNet()
criterion = nn.CrossEntropyLoss()        # 交叉熵损失函数 (softmax和损失结合)
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 4. 训练模型
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for images, labels in train_loader:
        outputs = model(images)          # 前向传播
        loss = criterion(outputs, labels) # 计算损失

        optimizer.zero_grad()            # 梯度清零
        loss.backward()                  # 反向传播
        optimizer.step()                 # 更新模型参数

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 5. 测试模型
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)  # 预测最大概率的类
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')
```

### 多层感知机 MLP

感知机为二分类。给定输入 x，权重 w，偏移 b，感知机输出 0 或 1（有时为 -1 或 1）。它不能拟合 XOR 函数

多层感知机（MLP）的简单例子：

```py
import torch
import torch.nn as nn
import torch.optim as optim

# 定义MLP模型
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        # 定义第一层全连接层
        self.fc1 = nn.Linear(input_size, hidden_size)
        # 激活函数ReLU
        self.relu = nn.ReLU()
        # 定义第二层全连接层
        self.fc2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # 第一层 + 激活函数
        out = self.fc1(x)
        out = self.relu(out)
        # 输出层
        out = self.fc2(out)
        return out

# 模型实例化
input_size = 10   # 输入特征数
hidden_size = 20  # 隐藏层神经元数
output_size = 3   # 输出类别数

model = MLP(input_size, hidden_size, output_size)

# 损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 假设我们有一个大小为10的输入张量
x = torch.randn(5, input_size)  # 批次大小为5
labels = torch.randint(0, output_size, (5,))  # 随机生成5个类别标签

# 前向传播
outputs = model(x)

# 计算损失
loss = criterion(outputs, labels)

# 反向传播和优化
optimizer.zero_grad()  # 梯度清零
loss.backward()        # 反向传播
optimizer.step()       # 更新参数

print(f'Loss: {loss.item()}')
```

## 卷积神经网络 CNN

卷积神经网络通常用于 **图像、文本** 处理

整体架构大致为：conv，relu，conv，relu，pool，conv....，fc

> n 层神经网络：有 n 层带参数的层（pool，relu 不算层）

### 多输入输出通道

彩色图片由 RGB 三个通道，每个输入通道通常有独立的二维卷积核

可以使用多个三维卷积核（下图核函数粘贴复制），每个核生成一个输出通道

![image-20241018160507227](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241018160507227.png)

- 计算的结果为 **特征图**
- 一次卷积可以有多个 Filter，卷积后的深度就为 Filter 的个数（上述 7\*7\*3 经过 2 个 3\*3\*3 的 filter 变为 3\*3\*2 的特征图）

1 * 1 的卷积核不识别空间模式，只是融合通道，以 **c~i~ 个输入** 值转换为 **c~o~ 个输出值** 

![image-20241018160829623](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241018160829623.png)

### 卷积层 conv（convolution）

卷积层中的 w 是卷积核，b 是偏置，w、b 是可学习参数

- 卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出
- 核矩阵和偏移是可学习的参数

**卷积层** 涉及 **超参数**：

- 滑动窗口步长
- 卷积核尺寸
- 是否边缘填充
- 卷积核个数

卷积层的权重（w + b）参数：所有卷积核的像素数 + 卷积核个数（偏置）

![image-20240821222130729](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240821222130729.png)

卷积结果计算公式：

![image-20240822133826745](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240822133826745.png)

可以对一次卷积后的特征图再卷积再卷积

### 池化层 pool

用于 **压缩特征图**

例如： MAX POOLING 最大池化层： 2*2 的特征为一组，筛选最大的值。平均池化层，将最大操作替换为平均

- 池化层与卷积层类似，都具有填充和步幅
- 池化层没有可学习的参数
- 在每个输入通道应用池化层以获得相应的输出通道
- 输出通道数 = 输入通道数

### 全连接层 fc

通过前面 conv 和 pool 得到最后的特征图（假设为 32\*32\*10），任务为 n 分类任务

将特征图拉成特征向量 [1,32\*32\*10] ，则全连接层参数为 [32\*32\*10,n]

## 经典多层神经网路

### LeNet （1980s）

卷积、全连接

### AlexNet（2012 年）

更大卷积，全连接

### Vgg（2014 年） 

提出 vgg 块，更大更深的 AlexNet，赋值粘贴 AlexNet

![image-20241012111726737](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241012111726737.png)

### NiN

提出 NiN 块，一个卷积层后跟两个全连接层（1*1 的卷积核）

![image-20241012111744823](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241012111744823.png)

### GoogLeNet

提出  Inception 块

### Resnet（2015年）

vgg 在层数更多时，训练效果反而不好

resnet 将 **好的层保留，不好的层跳过**。做法是通过对层数堆叠的值与同等映射的值进行比较

- 若果层数堆叠的值不好，将层数堆叠权重设为 0

  ![image-20240822141714082](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240822141714082.png)

### DenseNet

## 批量归一化层

- 损失在最后，后面的层训练比加快，前面的层训练比较慢。
- 前面的层一变化，所有层都得跟这边，最后的那些层需要重新学习多次，导致收敛边慢

方法：

- 固定小批量 batch 里的均值和方差，然后再做额外调整

  ![image-20241016182432423](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241016182432423.png)

  ![image-20241016182512399](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\Deep Learning\assets\image-20241016182512399.png)

  - 批量归一化层可学习的参数为 γ 和 β
  - 作用在
    - 全连接层和卷积层输出上，激活函数前
    - 全连接层和卷积层输入上
  - 对前连接层，作用在特征维
  - 对于卷积层，作用在通道维

## 目标检测算法

锚框：一类目标检测算法是基于锚框

- 提出多个被称为锚框的区域(边缘框)
- 预测每个锚框里是否含有关注的物体
- 如果是，预测从这个锚框到真实边缘框的偏移

loU 交并比：

- IoU 用来计算两个框之间的相似度
- 0 表示无重叠，1 表示重合，具体值通过集合计算

赋予锚框标号

- 每个锚框是一个训练样本
- 将每个锚框，要么标注成背景，要么关联上一个真实边缘框
- 我们可能会生成大量的锚框，这个导致大量的负类样本

非极大值抑制输出(NMS)

- 每个锚框预测一个边缘框
- NMS可以合并相似的预测
  - 选中是非背景类的最大预测值
  - 去掉所有其它和它IoU值大于 θ 的预测
  - 重复上述过程直到所有预测要么被选中，要么被去掉

## 迁移学习

深度学习的常见问题

- 训练数据量过少。导致欠拟合、过拟合
  - 使用数据增强
- 参数调节过多、时间成本大

**迁移学习** 就是使用 **他人的** 与自己项1目相似（数据集相似，参数相似）的项目的网络的 **w，b** 参数

- 继续训练 w，b 参数
- 微调网络（特别是 fc），继续训练一部分 w，b 参数

迁移学习的速度非常快、pytorch 官网有迁移学习的例子

## RNN 递归神经网络

RNN 的特点是带有时间序列，常用于 NLP，RNN 网络会将之前的结果全部记下来（h~0~，h~1~ .......），即输入过多，效果并不好

![image-20240826143426351](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240826143426351.png)

## LSTM 网络

相较于 RNN，添加控制参数 c，决定什么样的信息会被保留，什么样的会被遗忘

## 词向量模型 Word2Vec

文本向量化：

- 使用一定维度的向量描述词语
- 数据维度越高，能提供的信息越多
- 相似的词在特征表达中比较相似

词向量模型训练过程：

- 输入： 一个一个词，根据词向量大表（随机初始化）转换为向量
- 输出：一个词，输出可能性最高的词，类似多分类任务
- 更新词向量大表中输入数据的词向量

不同架构模型：

- CBOW
- Skip-gram

如果一个语料库很大，可能的结果就很多，最后一层 softmax 计算起来十分耗时。解决方案:

- 输入两个单词，看他们是不是前后对应的输入和输出，相当于一个二分类任务
- 由于训练数据来自文本，上述方法的结果都为 1，因此引入 **负采样**，即认为添加结果为 0 的输入

## GAN 对抗生成网络

对抗生成网络有 **生成器** 与 **判别器**

- 生成器生成想要的东西
- 判别器用于判别

![image-20240828214817355](E:\Study\=my repo\vuepress-hope-bloc\my-docs\src\code\python\Machine Learning\强化学习\assets\image-20240828214817355.png)

## CycleGan

PatchGAN 的作用：

- 输入一个 N * N 的矩阵个，基于感受野来计算损失
- 基于感受野在特征图上的预测结果和标签（也需设置成 N * N）计算损失

## KNN

## 随机森林
