## 4ã€åŠ¨æ€è®¡ç®—å›¾

### 4.1ã€åŠ¨æ€è®¡ç®—å›¾ç®€ä»‹

**è®¡ç®—å›¾ç”±èŠ‚ç‚¹å’Œè¾¹ç»„æˆ**

- èŠ‚ç‚¹ï¼šå¼ é‡æˆ–è€…Function
- è¾¹ï¼šå¼ é‡å’Œ Function ä¹‹é—´çš„ä¾èµ–å…³ç³»

è®¡ç®—å›¾æ˜¯åŠ¨æ€å›¾ï¼ŒåŠ¨æ€å›¾æœ‰ä¸¤å±‚å«ä¹‰ï¼š

- è®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚æ— éœ€ç­‰å¾…å®Œæ•´çš„è®¡ç®—å›¾åˆ›å»ºå®Œæ¯•ï¼Œæ¯æ¡è¯­å¥éƒ½ä¼šåœ¨è®¡ç®—å›¾ä¸­åŠ¨æ€æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ï¼Œå¹¶ç«‹å³æ‰§è¡Œæ­£å‘ä¼ æ’­å¾—åˆ°è®¡ç®—ç»“æœ
- è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ã€‚ä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°æ„å»ºè®¡ç®—å›¾ã€‚å¦‚æœåœ¨ç¨‹åºä¸­ä½¿ç”¨äº† backward æ–¹æ³•æ‰§è¡Œäº†åå‘ä¼ æ’­ï¼Œæˆ–è€…åˆ©ç”¨ torch.autograd.grad æ–¹æ³•è®¡ç®—äº†æ¢¯åº¦ï¼Œé‚£ä¹ˆåˆ›å»ºçš„è®¡ç®—å›¾ä¼šè¢«ç«‹å³é”€æ¯ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´ï¼Œä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°åˆ›å»º

**è®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­ç«‹å³æ‰§è¡Œ**

```py
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.randn(10,2)
Y = torch.randn(10,1)
Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³
loss = torch.mean(torch.pow(Y_hat-Y,2)) # è®¡ç®—å¼ é‡çš„å¹³å‡å€¼

print(loss.data)
print(Y_hat.data)
```

**è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯**

```py
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.randn(10,2)
Y = torch.randn(10,1)
Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³
loss = torch.mean(torch.pow(Y_hat-Y,2))

#è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ï¼Œå¦‚æœéœ€è¦ä¿ç•™è®¡ç®—å›¾, éœ€è¦è®¾ç½®retain_graph = True
loss.backward()  #loss.backward(retain_graph = True) 

#loss.backward() #å¦‚æœå†æ¬¡æ‰§è¡Œåå‘ä¼ æ’­å°†æŠ¥é”™
```

### 4.2ã€è®¡ç®—å›¾ä¸­çš„ Function

è®¡ç®—å›¾ä¸­çš„å¦å¤–ä¸€ç§èŠ‚ç‚¹æ˜¯ Function, å®é™…ä¸Šå°±æ˜¯ Pytorch ä¸­å„ç§å¯¹å¼ é‡æ“ä½œçš„å‡½æ•°ã€‚

è¿™äº› Function å’Œæˆ‘ä»¬ Python ä¸­çš„å‡½æ•°æœ‰ä¸€ä¸ªè¾ƒå¤§çš„åŒºåˆ«ï¼Œé‚£å°±æ˜¯å®ƒåŒæ—¶åŒ…æ‹¬æ­£å‘è®¡ç®—é€»è¾‘å’Œåå‘ä¼ æ’­çš„é€»è¾‘ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿ torch.autograd.Function æ¥åˆ›å»ºè¿™ç§æ”¯æŒåå‘ä¼ æ’­çš„ Function

```py
class MyReLU(torch.autograd.Function):
   
    #æ­£å‘ä¼ æ’­é€»è¾‘ï¼Œå¯ä»¥ç”¨ctxå­˜å‚¨ä¸€äº›å€¼ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    #åå‘ä¼ æ’­é€»è¾‘
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
    
```

```py
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])
Y = torch.tensor([[2.0,3.0]])

relu = MyReLU.apply # reluç°åœ¨ä¹Ÿå¯ä»¥å…·æœ‰æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­åŠŸèƒ½
Y_hat = relu(X@w.t() + b)
loss = torch.mean(torch.pow(Y_hat-Y,2))

loss.backward()

print(w.grad) # tensor([[4.5000, 4.5000]])
print(b.grad) # tensor([[4.5000]])

# Y_hatçš„æ¢¯åº¦å‡½æ•°å³æ˜¯æˆ‘ä»¬è‡ªå·±æ‰€å®šä¹‰çš„ MyReLU.backward
print(Y_hat.grad_fn)
```

### 4.3ã€è®¡ç®—å›¾ä¸åå‘ä¼ æ’­

ç®€å•åœ°ç†è§£ä¸€ä¸‹åå‘ä¼ æ’­çš„åŸç†å’Œè¿‡ç¨‹

```py
import torch 

x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

loss.backward()
```

loss.backward() è¯­å¥è°ƒç”¨åï¼Œä¾æ¬¡å‘ç”Ÿä»¥ä¸‹è®¡ç®—è¿‡ç¨‹ã€‚

1. loss è‡ªå·±çš„ grad æ¢¯åº¦èµ‹å€¼ä¸º 1ï¼Œå³å¯¹è‡ªèº«çš„æ¢¯åº¦ä¸º 1

2. loss æ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„ backward æ–¹æ³•ï¼Œè®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡å³ y1 å’Œ y2 çš„æ¢¯åº¦ï¼Œå°†è¯¥å€¼èµ‹å€¼åˆ° y1.grad å’Œ y2.grad

3. y2 å’Œ y1 æ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„ backward æ–¹æ³•, åˆ†åˆ«è®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡ x çš„æ¢¯åº¦ï¼Œx.grad å°†å…¶æ”¶åˆ°çš„å¤šä¸ªæ¢¯åº¦å€¼ç´¯åŠ 

   ï¼ˆæ³¨æ„ï¼Œ1,2,3æ­¥éª¤çš„æ±‚æ¢¯åº¦é¡ºåºå’Œå¯¹å¤šä¸ªæ¢¯åº¦å€¼çš„ç´¯åŠ è§„åˆ™æ°å¥½æ˜¯æ±‚å¯¼é“¾å¼æ³•åˆ™çš„ç¨‹åºè¡¨è¿°ï¼‰

æ­£å› ä¸ºæ±‚å¯¼é“¾å¼æ³•åˆ™è¡ç”Ÿçš„æ¢¯åº¦ç´¯åŠ è§„åˆ™ï¼Œå¼ é‡çš„ grad æ¢¯åº¦ä¸ä¼šè‡ªåŠ¨æ¸…é›¶ï¼Œåœ¨éœ€è¦çš„æ—¶å€™éœ€è¦æ‰‹åŠ¨ç½®é›¶

### 4.4ã€å¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹

æ‰§è¡Œä¸‹é¢ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç° loss.grad å¹¶ä¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„ 1ï¼Œè€Œæ˜¯ None

ç±»ä¼¼åœ° y1.grad ä»¥åŠ y2.grad ä¹Ÿæ˜¯ None

è¿™æ˜¯ç”±äºå®ƒä»¬ä¸æ˜¯å¶å­èŠ‚ç‚¹å¼ é‡ã€‚

åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œåªæœ‰ is_leaf=True çš„å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦æ±‚å¯¼çš„å¼ é‡çš„å¯¼æ•°ç»“æœæ‰ä¼šè¢«æœ€åä¿ç•™ä¸‹æ¥ã€‚

é‚£ä¹ˆä»€ä¹ˆæ˜¯å¶å­èŠ‚ç‚¹å¼ é‡å‘¢ï¼Ÿå¶å­èŠ‚ç‚¹å¼ é‡éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ã€‚

1. å¶å­èŠ‚ç‚¹å¼ é‡æ˜¯ç”±ç”¨æˆ·ç›´æ¥åˆ›å»ºçš„å¼ é‡ï¼Œè€Œéç”±æŸä¸ª Function é€šè¿‡è®¡ç®—å¾—åˆ°çš„å¼ é‡ã€‚
2. å¶å­èŠ‚ç‚¹å¼ é‡çš„ requires_grad å±æ€§å¿…é¡»ä¸º True.

Pytorch è®¾è®¡è¿™æ ·çš„è§„åˆ™ä¸»è¦æ˜¯ä¸ºäº†èŠ‚çº¦å†…å­˜æˆ–è€…æ˜¾å­˜ç©ºé—´ï¼Œå› ä¸ºå‡ ä¹æ‰€æœ‰çš„æ—¶å€™ï¼Œç”¨æˆ·åªä¼šå…³å¿ƒä»–è‡ªå·±ç›´æ¥åˆ›å»ºçš„å¼ é‡çš„æ¢¯åº¦ã€‚

æ‰€æœ‰ä¾èµ–äºå¶å­èŠ‚ç‚¹å¼ é‡çš„å¼ é‡, å…¶ requires_grad å±æ€§å¿…å®šæ˜¯ True çš„ï¼Œä½†å…¶æ¢¯åº¦å€¼åªåœ¨è®¡ç®—è¿‡ç¨‹ä¸­è¢«ç”¨åˆ°ï¼Œä¸ä¼šæœ€ç»ˆå­˜å‚¨åˆ° grad å±æ€§ä¸­ã€‚

å¦‚æœéœ€è¦ä¿ç•™ä¸­é—´è®¡ç®—ç»“æœçš„æ¢¯åº¦åˆ° grad å±æ€§ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ retain_grad æ–¹æ³•ã€‚ å¦‚æœä»…ä»…æ˜¯ä¸ºäº†è°ƒè¯•ä»£ç æŸ¥çœ‹æ¢¯åº¦å€¼ï¼Œå¯ä»¥åˆ©ç”¨ register_hook æ‰“å°æ—¥å¿—ã€‚

```py
import torch 

x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

loss.backward()
print("loss.grad:", loss.grad) # loss.grad: None
print("y1.grad:", y1.grad) # y1.grad: None
print("y2.grad:", y2.grad) # y2.grad: None
print(x.grad) # tensor(4.)

print(x.is_leaf) # True
print(y1.is_leaf) # False
print(y2.is_leaf) # False
print(loss.is_leaf) # False
```

åˆ©ç”¨ retain_grad å¯ä»¥ä¿ç•™éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ï¼Œåˆ©ç”¨ register_hook å¯ä»¥æŸ¥çœ‹éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼

```py
import torch 

# æ­£å‘ä¼ æ’­
x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

# éå¶å­èŠ‚ç‚¹æ¢¯åº¦æ˜¾ç¤ºæ§åˆ¶
y1.register_hook(lambda grad: print('y1 grad: ', grad))
y2.register_hook(lambda grad: print('y2 grad: ', grad))
loss.retain_grad()

# åå‘ä¼ æ’­
loss.backward()
print("loss.grad:", loss.grad)
print("x.grad:", x.grad)

'''
y2 grad:  tensor(4.)
y1 grad:  tensor(-4.)
loss.grad: tensor(1.)
x.grad: tensor(4.)
'''
```

### 4.5ã€è®¡ç®—å›¾åœ¨ TensorBoard ä¸­çš„å¯è§†åŒ–

```py
from torch import nn 
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.w = nn.Parameter(torch.randn(2,1))
        self.b = nn.Parameter(torch.zeros(1,1))

    def forward(self, x):
        y = x@self.w + self.b
        return y

net = Net()
```

```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter('./data/tensorboard')
writer.add_graph(net,input_to_model = torch.rand(10,2))
writer.close()
```

```python
%load_ext tensorboard
#%tensorboard --logdir ./data/tensorboard
```

```py
from tensorboard import notebook
notebook.list() 
```

```py
# åœ¨tensorboardä¸­æŸ¥çœ‹æ¨¡å‹
notebook.start("--logdir ./data/tensorboard")
```

## 5ã€Pytorch å±‚æ¬¡ç»“æ„

Pytorch ä¸­ 5 ä¸ªä¸åŒçš„å±‚æ¬¡ç»“æ„ï¼š

- ç¡¬ä»¶å±‚ï¼šPytorch æ”¯æŒ CPUã€GPU åŠ å…¥è®¡ç®—èµ„æºæ± 
- å†…æ ¸å±‚ï¼šC++ å®ç°çš„å†…æ ¸
- ä½é˜¶ APIï¼šPython å®ç°çš„æ“ä½œç¬¦ï¼Œæä¾›äº†å°è£… C++ å†…æ ¸çš„ä½çº§ API æŒ‡ä»¤ï¼Œä¸»è¦åŒ…æ‹¬å„ç§å¼ é‡æ“ä½œç®—å­ã€è‡ªåŠ¨å¾®åˆ†ã€å˜é‡ç®¡ç†
- ä¸­é˜¶ APIï¼šPython å®ç°çš„æ¨¡å‹ç»„ä»¶ï¼Œå¯¹ä½çº§APIè¿›è¡Œäº†å‡½æ•°å°è£…ï¼Œä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ç­‰ç­‰
- é«˜é˜¶ APIï¼šPython å®ç°çš„æ¨¡å‹æ¥å£ã€‚Pytorch æ²¡æœ‰å®˜æ–¹çš„é«˜é˜¶APIã€‚ä¸ºäº†ä¾¿äºè®­ç»ƒæ¨¡å‹ï¼Œä½œè€…ä»¿ç…§ keras ä¸­çš„æ¨¡å‹æ¥å£ï¼Œå°è£…äº† pytorch çš„é«˜é˜¶æ¨¡å‹æ¥å£ torchkeras.KerasModelã€‚æ­¤å¤–ï¼Œæœ‰ä¸€ä¸ªéå¸¸æµè¡Œçš„éå®˜æ–¹ Pytorch çš„é«˜é˜¶ API åº“ï¼Œå«åš pytorch_lightningï¼Œä½œè€…é€šè¿‡å¼•ç”¨å’Œå€Ÿé‰´å®ƒçš„ä¸€äº›èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ªå’Œ torchkeras.KerasModel åŠŸèƒ½ç±»ä¼¼çš„é«˜é˜¶æ¨¡å‹æ¥å£ torchkeras.LightModelï¼ŒåŠŸèƒ½æ›´åŠ å¼ºå¤§ã€‚

### 5.1ã€ä½é˜¶ API

ä½é˜¶ API ä¸»è¦åŒ…æ‹¬ **å¼ é‡æ“ä½œ**ï¼Œ**è®¡ç®—å›¾** å’Œ **è‡ªåŠ¨å¾®åˆ†**

å¼ é‡ç»“æ„æ“ä½œä¸»è¦åŒ…æ‹¬ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚

#### åˆ›å»ºå¼ é‡

å¼ é‡åˆ›å»ºçš„è®¸å¤šæ–¹æ³•å’Œ numpy ä¸­åˆ›å»º array çš„æ–¹æ³•å¾ˆåƒ

```py
import numpy as np
import torch 

a = torch.tensor([1,2,3],dtype = torch.float) # tensor([1., 2., 3.])
b = torch.arange(1,10,step = 2) # tensor([1, 3, 5, 7, 9])
c = torch.linspace(0.0,2*3.14,10) # tensor([0.0000, 0.6978, 1.3956, 2.0933, 2.7911, 3.4889, 4.1867, 4.8844, 5.5822,6.2800])
'''
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
'''
d = torch.zeros((3,3))
'''
tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.int32)
'''
a = torch.ones((3,3),dtype = torch.int)
'''
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
'''
b = torch.zeros_like(a,dtype = torch.float)
'''
tensor([[5., 5., 5.],
        [5., 5., 5.],
        [5., 5., 5.]])
'''
torch.fill_(b,5)

# å‡åŒ€éšæœºåˆ†å¸ƒ
torch.manual_seed(0)
minval,maxval = 0,10
a = minval + (maxval-minval)*torch.rand([5]) # tensor([4.9626, 7.6822, 0.8848, 1.3203, 3.0742])

# æ­£æ€åˆ†å¸ƒéšæœº
'''
tensor([[ 0.5507,  0.2704,  0.6472],
        [ 0.2490, -0.3354,  0.4564],
        [-0.6255,  0.4539, -1.3740]])
'''
b = torch.normal(mean = torch.zeros(3,3), std = torch.ones(3,3))

# æ­£æ€åˆ†å¸ƒéšæœº
'''
tensor([[16.2371, -1.6612,  3.9163],
        [ 7.4999,  1.5616,  4.0768],
        [ 5.2128, -8.9407,  6.4601]])
'''
mean,std = 2,5
c = std * torch.randn((3,3)) + mean

# æ•´æ•°éšæœºæ’åˆ—
d = torch.randperm(20) # tensor([ 3, 17,  9, 19,  1, 18,  4, 13, 15, 12,  0, 16,  7, 11,  2,  5,  8, 10, 6, 14])

# ç‰¹æ®ŠçŸ©é˜µ
I = torch.eye(3,3) #å•ä½çŸ©é˜µ
print(I)
t = torch.diag(torch.tensor([1,2,3])) #å¯¹è§’çŸ©é˜µ
```

#### ç´¢å¼•åˆ‡ç‰‡

å¼ é‡çš„ç´¢å¼•åˆ‡ç‰‡æ–¹å¼å’Œ numpy å‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚åˆ‡ç‰‡æ—¶æ”¯æŒç¼ºçœå‚æ•°å’Œçœç•¥å·

å¯ä»¥é€šè¿‡ç´¢å¼•å’Œåˆ‡ç‰‡å¯¹éƒ¨åˆ†å…ƒç´ è¿›è¡Œä¿®æ”¹ã€‚

æ­¤å¤–ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨ `torch.index_select`, `torch.masked_select`, `torch.take`

å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„æŸäº›å…ƒç´ å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨ `torch.where`ï¼Œ`torch.masked_fill`ï¼Œ`torch.index_fill`

```py
# å‡åŒ€éšæœºåˆ†å¸ƒ
torch.manual_seed(0)
minval,maxval = 0,10
t = torch.floor(minval + (maxval-minval)*torch.rand([5,5])).int()
'''
tensor([[4, 7, 0, 1, 3],
        [6, 4, 8, 4, 6],
        [3, 4, 0, 1, 2],
        [5, 6, 8, 1, 2],
        [6, 9, 3, 8, 4]], dtype=torch.int32)
'''
# ç¬¬ 0 è¡Œ
print(t[0]) # tensor([4, 7, 0, 1, 3], dtype=torch.int32)
# ç¬¬ 1 è¡Œç¬¬ 3 åˆ—
print(t[1,3]) # tensor(4, dtype=torch.int32)
print(t[1][3]) # tensor(4, dtype=torch.int32)
# ç¬¬ 1 è¡Œè‡³ç¬¬ 3 è¡Œ
print(t[1:4,:])
# ç¬¬ 1 è¡Œè‡³æœ€åä¸€è¡Œï¼Œç¬¬0åˆ—åˆ°æœ€åä¸€åˆ—æ¯éš”ä¸¤åˆ—å–ä¸€åˆ—
print(t[1:4,:4:2])
#å¯ä»¥ä½¿ç”¨ç´¢å¼•å’Œåˆ‡ç‰‡ä¿®æ”¹éƒ¨åˆ†å…ƒç´ 
x = torch.Tensor([[1,2],[3,4]])
x.data[1,:] = torch.tensor([0.0,0.0]) # tensor([[1., 2.],[0., 0.]])
# çœç•¥å·å¯ä»¥è¡¨ç¤ºå¤šä¸ªå†’å·
print(a[...,1])
```

ä»¥ä¸Šåˆ‡ç‰‡æ–¹å¼ç›¸å¯¹è§„åˆ™ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨ torch.index_select, torch.take, torch.gather, torch.masked_select.

è€ƒè™‘ç­çº§æˆç»©å†Œçš„ä¾‹å­ï¼Œæœ‰ 4 ä¸ªç­çº§ï¼Œæ¯ä¸ªç­çº§ 5 ä¸ªå­¦ç”Ÿï¼Œæ¯ä¸ªå­¦ç”Ÿ 7 é—¨ç§‘ç›®æˆç»©ã€‚å¯ä»¥ç”¨ä¸€ä¸ª 4Ã—5Ã—7 çš„å¼ é‡æ¥è¡¨ç¤ºã€‚

```py
minval=0
maxval=100
scores = torch.floor(minval + (maxval-minval)*torch.rand([4,5,7])).int()

# æŠ½å–æ¯ä¸ªç­çº§ç¬¬ 0 ä¸ªå­¦ç”Ÿï¼Œç¬¬2ä¸ªå­¦ç”Ÿï¼Œç¬¬ 4 ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©
torch.index_select(scores,dim = 1,index = torch.tensor([0,2,4]))

#æŠ½å–æ¯ä¸ªç­çº§ç¬¬ 0 ä¸ªå­¦ç”Ÿï¼Œç¬¬ 2 ä¸ªå­¦ç”Ÿï¼Œç¬¬ 4 ä¸ªå­¦ç”Ÿçš„ç¬¬ 1 é—¨è¯¾ç¨‹ï¼Œç¬¬ 3 é—¨è¯¾ç¨‹ï¼Œç¬¬ 6 é—¨è¯¾ç¨‹æˆç»©
q = torch.index_select(torch.index_select(scores,dim = 1,index = torch.tensor([0,2,4]))
                   ,dim=2,index = torch.tensor([1,3,6]))

# æŠ½å–ç¬¬ 0 ä¸ªç­çº§ç¬¬ 0 ä¸ªå­¦ç”Ÿçš„ç¬¬ 0 é—¨è¯¾ç¨‹ï¼Œç¬¬ 2 ä¸ªç­çº§çš„ç¬¬ 3 ä¸ªå­¦ç”Ÿçš„ç¬¬ 1 é—¨è¯¾ç¨‹ï¼Œç¬¬ 3 ä¸ªç­çº§çš„ç¬¬ 4 ä¸ªå­¦ç”Ÿç¬¬ 6 é—¨è¯¾ç¨‹æˆç»©
# take å°†è¾“å…¥çœ‹æˆä¸€ç»´æ•°ç»„ï¼Œè¾“å‡ºå’Œ index åŒå½¢çŠ¶
s = torch.take(scores,torch.tensor([0*5*7+0,2*5*7+3*7+1,3*5*7+4*7+6]))

# æŠ½å–åˆ†æ•°å¤§äºç­‰äº 80 åˆ†çš„åˆ†æ•°ï¼ˆå¸ƒå°”ç´¢å¼•ï¼‰
# ç»“æœæ˜¯ 1 ç»´å¼ é‡
g = torch.masked_select(scores,scores>=80)


```

ä»¥ä¸Šè¿™äº›æ–¹æ³•ä»…èƒ½æå–å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼ï¼Œä½†ä¸èƒ½æ›´æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ã€‚

å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨ torch.where,torch.index_fill å’Œ torch.masked_fill

- torch.where å¯ä»¥ç†è§£ä¸º if çš„å¼ é‡ç‰ˆæœ¬
- torch.index_fill çš„é€‰å–å…ƒç´ é€»è¾‘å’Œ torch.index_select ç›¸åŒ
- torch.masked_fill çš„é€‰å–å…ƒç´ é€»è¾‘å’Œ torch.masked_select ç›¸åŒã€‚

```py
# å¦‚æœåˆ†æ•°å¤§äº60åˆ†ï¼Œèµ‹å€¼æˆ1ï¼Œå¦åˆ™èµ‹å€¼æˆ0
ifpass = torch.where(scores>60,torch.tensor(1),torch.tensor(0))

# å°†æ¯ä¸ªç­çº§ç¬¬ 0 ä¸ªå­¦ç”Ÿï¼Œç¬¬ 2 ä¸ªå­¦ç”Ÿï¼Œç¬¬ 4 ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©èµ‹å€¼æˆæ»¡åˆ†
torch.index_fill(scores,dim = 1,index = torch.tensor([0,2,4]),value = 100)
# ç­‰ä»·äº scores.index_fill(dim = 1,index = torch.tensor([0,2,4]),value = 100)

# å°†åˆ†æ•°å°äº 60 åˆ†çš„åˆ†æ•°èµ‹å€¼æˆ 60 åˆ†
b = torch.masked_fill(scores,scores<60,60)
# ç­‰ä»·äº b = scores.masked_fill(scores<60,60)
```

#### ç»´åº¦å˜æ¢

ç»´åº¦å˜æ¢ç›¸å…³å‡½æ•°ä¸»è¦æœ‰ torch.reshape(æˆ–è€…è°ƒç”¨å¼ é‡çš„ view æ–¹æ³•), torch.squeeze, torch.unsqueeze, torch.transpose

- torch.reshape å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶
- torch.reshape å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶
- torch.unsqueeze å¯ä»¥å¢åŠ ç»´åº¦
- torch.transpose/torch.permute å¯ä»¥äº¤æ¢ç»´åº¦ã€‚

```py
# å¼ é‡çš„ view æ–¹æ³•æœ‰æ—¶å€™ä¼šè°ƒç”¨å¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨reshapeæ–¹æ³•ã€‚

torch.manual_seed(0)
minval,maxval = 0,255
a = (minval + (maxval-minval)*torch.rand([1,3,3,2])).int()

# åŸ tensor æ”¹æˆ ï¼ˆ3,6ï¼‰å½¢çŠ¶çš„å¼ é‡
b = a.view([3,6]) # torch.reshape(a,[3,6])
# æ”¹å›æˆ [1,3,3,2] å½¢çŠ¶çš„å¼ é‡
c = torch.reshape(b,[1,3,3,2]) # b.view([1,3,3,2]) 
```

å¦‚æœå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šåªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œåˆ©ç”¨ torch.squeeze å¯ä»¥æ¶ˆé™¤è¿™ä¸ªç»´åº¦

torch.unsqueeze çš„ä½œç”¨å’Œ torch.squeeze çš„ä½œç”¨ç›¸å

```py
a = torch.tensor([[1.0,2.0]])
s = torch.squeeze(a)
print(a) # tensor([[1., 2.]])
print(s) # tensor([1., 2.])
print(a.shape) # torch.Size([1, 2])
print(s.shape) # torch.Size([2])

#åœ¨ç¬¬ 0 ç»´æ’å…¥é•¿åº¦ä¸º1çš„ä¸€ä¸ªç»´åº¦
d = torch.unsqueeze(s,axis=0)  
print(s) # tensor([1., 2.])
print(d) # tensor([[1., 2.]])
print(s.shape) # torch.Size([2])
print(d.shape) # torch.Size([1, 2])

```

torch.transpose å¯ä»¥äº¤æ¢å¼ é‡çš„ç»´åº¦ï¼Œtorch.transpose å¸¸ç”¨äºå›¾ç‰‡å­˜å‚¨æ ¼å¼çš„å˜æ¢ä¸Šã€‚permute å¯ä»¥å¯¹ç»´åº¦é¡ºåºåšé‡æ–°ç¼–æ’

å¦‚æœæ˜¯äºŒç»´çš„çŸ©é˜µï¼Œé€šå¸¸ä¼šè°ƒç”¨çŸ©é˜µçš„è½¬ç½®æ–¹æ³• matrix.t()ï¼Œç­‰ä»·äº torch.transpose(matrix,0,1)ã€‚

```py
minval=0
maxval=255
# Batch,Height,Width,Channel
data = torch.floor(minval + (maxval-minval)*torch.rand([100,256,256,4])).int()
print(data.shape) # torch.Size([100, 256, 256, 4])

# è½¬æ¢æˆ Pytorch é»˜è®¤çš„å›¾ç‰‡æ ¼å¼ Batch,Channel,Height,Width 
# éœ€è¦äº¤æ¢ä¸¤æ¬¡
data_t = torch.transpose(torch.transpose(data,1,2),1,3)
print(data_t.shape) # torch.Size([100, 4, 256, 256])
data_p = torch.permute(data,[0,3,1,2]) #å¯¹ç»´åº¦çš„é¡ºåºåšé‡æ–°ç¼–æ’
data_p.shape # torch.Size([100, 4, 256, 256])

matrix = torch.tensor([[1,2,3],[4,5,6]])
print(matrix) # tensor([[1, 2, 3],[4, 5, 6]])
print(matrix.t()) #ç­‰ä»·äºtorch.transpose(matrix,0,1) # tensor([[1, 4],[2, 5],[3, 6]])
```

#### åˆå¹¶åˆ†å‰²

ä»¥ç”¨ torch.cat æ–¹æ³•å’Œ torch.stack æ–¹æ³•å°†å¤šä¸ªå¼ é‡åˆå¹¶ï¼Œå¯ä»¥ç”¨ torch.split æ–¹æ³•æŠŠä¸€ä¸ªå¼ é‡åˆ†å‰²æˆå¤šä¸ªå¼ é‡ã€‚

torch.cat å’Œ torch.stack æœ‰ç•¥å¾®çš„åŒºåˆ«ï¼Œtorch.cat æ˜¯è¿æ¥ï¼Œä¸ä¼šå¢åŠ ç»´åº¦ï¼Œè€Œ torch.stack æ˜¯å †å ï¼Œä¼šå¢åŠ ç»´åº¦ã€‚

```py
a = torch.tensor([[1.0,2.0],[3.0,4.0]])
b = torch.tensor([[5.0,6.0],[7.0,8.0]])
c = torch.tensor([[9.0,10.0],[11.0,12.0]])

abc_cat = torch.cat([a,b,c],dim = 0)
print(abc_cat.shape) # torch.Size([6, 2])
'''
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.],
        [11., 12.]])
'''
print(abc_cat)

abc_stack = torch.stack([a,b,c],axis = 0) # torch ä¸­ dim å’Œ axis å‚æ•°åå¯ä»¥æ··ç”¨
print(abc_stack.shape) # torch.Size([3, 2, 2])
'''
tensor([[[ 1.,  2.],
         [ 3.,  4.]],
        [[ 5.,  6.],
         [ 7.,  8.]],
        [[ 9., 10.],
         [11., 12.]]])
'''
print(abc_stack)

'''
tensor([[ 1.,  2.,  5.,  6.,  9., 10.],
        [ 3.,  4.,  7.,  8., 11., 12.]])
'''
torch.cat([a,b,c],axis = 1)

'''
tensor([[[ 1.,  2.],
         [ 5.,  6.],
         [ 9., 10.]],
         
        [[ 3.,  4.],
         [ 7.,  8.],
         [11., 12.]]])
'''
torch.stack([a,b,c],axis = 1)
```

torch.split æ˜¯ torch.cat çš„é€†è¿ç®—ï¼Œå¯ä»¥æŒ‡å®šåˆ†å‰²ä»½æ•°å¹³å‡åˆ†å‰²ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šæ¯ä»½çš„è®°å½•æ•°é‡è¿›è¡Œåˆ†å‰²

```py
'''
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.],
        [11., 12.]])
'''
print(abc_cat)
a,b,c = torch.split(abc_cat,split_size_or_sections = 2,dim = 0) #æ¯ä»½2ä¸ªè¿›è¡Œåˆ†å‰²
print(a) # tensor([[1., 2.],[3., 4.]])
print(b) # tensor([[5., 6.],[7., 8.]])
print(c) # tensor([[ 9., 10.],[11., 12.]])


p,q,r = torch.split(abc_cat,split_size_or_sections =[4,1,1],dim = 0) #æ¯ä»½åˆ†åˆ«ä¸º[4,1,1]
print(p) # tensor([[1., 2.],[3., 4.],[5., 6.],[7., 8.]])
print(q) # tensor([[ 9., 10.]]) 
print(r) # tensor([[11., 12.]])
```

#### æ•°å­¦è¿ç®—

å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ï¼Œä»¥åŠä½¿ç”¨éå¸¸å¼ºå¤§è€Œçµæ´»çš„çˆ±å› æ–¯å¦æ±‚å’Œå‡½æ•° torch.einsum è¿›è¡Œä»»æ„ç»´çš„å¼ é‡è¿ç®—

##### æ ‡é‡è¿ç®—

å¼ é‡çš„æ•°å­¦è¿ç®—ç¬¦å¯ä»¥åˆ†ä¸ºæ ‡é‡è¿ç®—ç¬¦ã€å‘é‡è¿ç®—ç¬¦ã€ä»¥åŠçŸ©é˜µè¿ç®—ç¬¦ã€‚

åŠ å‡ä¹˜é™¤ä¹˜æ–¹ï¼Œä»¥åŠä¸‰è§’å‡½æ•°ï¼ŒæŒ‡æ•°ï¼Œå¯¹æ•°ç­‰å¸¸è§å‡½æ•°ï¼Œé€»è¾‘æ¯”è¾ƒè¿ç®—ç¬¦ç­‰éƒ½æ˜¯æ ‡é‡è¿ç®—ç¬¦ã€‚

æ ‡é‡è¿ç®—ç¬¦çš„ç‰¹ç‚¹æ˜¯å¯¹å¼ é‡å®æ–½ **é€å…ƒç´ ** è¿ç®—ã€‚

æœ‰äº›æ ‡é‡è¿ç®—ç¬¦å¯¹å¸¸ç”¨çš„æ•°å­¦è¿ç®—ç¬¦è¿›è¡Œäº†é‡è½½ã€‚å¹¶ä¸”æ”¯æŒç±»ä¼¼ numpy çš„å¹¿æ’­ç‰¹æ€§ã€‚

```py
import torch 
import numpy as np 

a = torch.tensor(1.0)
b = torch.tensor(2.0)
a + b # tensor(3.)

a = torch.tensor([[1.0,2],[-3,4.0]])
b = torch.tensor([[5.0,6],[7.0,8.0]])
a+b  # è¿ç®—ç¬¦é‡è½½ tensor([[ 6.,  8.],[ 4., 12.]])
a-b 
a*b 
a/b
a**2
a**(0.5)
a%3 #æ±‚æ¨¡ # tensor([[1., 2.],[-0., 1.]])
torch.div(a, b, rounding_mode='floor')  # åœ°æ¿é™¤æ³• tensor([[ 0.,  0.],[-1.,  0.]])
a >= 2 # torch.ge(a,2)  #ge: greater_equal ç¼©å†™ tensor([[False,  True],[False,  True]])
(a>=2)&(a<=3) # tensor([[False,  True],[False, False]])
(a>=2)|(a<=3) # tensor([[True, True],[True, True]])
a==5 #ã€€torch.eq(a,5)ã€€tensor([[False, False],[False, False]])
torch.sqrt(a) # tensor([[1.0000, 1.4142],[nan, 2.0000]])


a = torch.tensor([1.0,8.0])
b = torch.tensor([5.0,6.0])
c = torch.tensor([6.0,7.0])
torch.max(a,b) # tensor([5., 8.])
torch.min(a,b) # tensor([1., 6.])

x = torch.tensor([2.6,-2.7])
print(torch.round(x)) #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå››èˆäº”å…¥
print(torch.floor(x)) #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå‘ä¸‹å½’æ•´
print(torch.ceil(x))  #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå‘ä¸Šå½’æ•´
print(torch.trunc(x)) #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå‘0å½’æ•´

x = torch.tensor([2.6,-2.7])
print(torch.fmod(x,2)) # ä½œé™¤æ³•å–ä½™æ•°  tensor([ 0.6000, -0.7000])
print(torch.remainder(x,2)) # ä½œé™¤æ³•å–å‰©ä½™çš„éƒ¨åˆ†ï¼Œç»“æœæ’æ­£  tensor([0.6000, 1.3000])

# å¹…å€¼è£å‰ª
x = torch.tensor([0.9,-0.8,100.0,-20.0,0.7])
y = torch.clamp(x,min=-1,max = 1)
z = torch.clamp(x,max = 1)
print(y) # tensor([ 0.9000, -0.8000,  1.0000, -1.0000,  0.7000])
print(z) # tensor([  0.9000,  -0.8000,   1.0000, -20.0000,   0.7000])

relu = lambda x:x.clamp(min=0.0)
relu(torch.tensor(5.0)) # tensor(5.)
```

##### å‘é‡è¿ç®—

åŸåˆ™ä¸Šæ“ä½œçš„å¼ é‡è‡³å°‘æ˜¯ä¸€ç»´å¼ é‡

å‘é‡è¿ç®—ç¬¦åªåœ¨ä¸€ä¸ªç‰¹å®šè½´ä¸Šè¿ç®—ï¼Œå°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ ‡é‡æˆ–è€…å¦å¤–ä¸€ä¸ªå‘é‡ã€‚

```py
# ç»Ÿè®¡å€¼
a = torch.arange(1,10).float().view(3,3) # tensor([[1., 2., 3.],[4., 5., 6.],[7., 8., 9.]])
print(torch.sum(a)) # tensor(45.)
print(torch.mean(a)) # å¹³å‡æ•° tensor(5.)
print(torch.max(a))
print(torch.min(a))
print(torch.prod(a)) #ç´¯ä¹˜ tensor(362880.)
print(torch.std(a))  #æ ‡å‡†å·® tensor(2.7386)
print(torch.var(a))  #æ–¹å·® tensor(7.5000)
print(torch.median(a)) #ä¸­ä½æ•° tensor(5.)

# æŒ‡å®šç»´åº¦è®¡ç®—ç»Ÿè®¡å€¼
b = torch.arange(1,13).float().view(3,4)
print(torch.max(b,dim = 0)) # torch.return_types.max(values=tensor([ 9., 10., 11., 12.]),indices=tensor([2, 2, 2, 2]))
print(torch.max(b,dim = 1)) # torch.return_types.max(values=tensor([ 4.,  8., 12.]),indices=tensor([3, 3, 3]))

# cum æ‰«æ 
a = torch.arange(1,10) # tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
print(torch.cumsum(a,0)) # tensor([ 1, 3,  6, 10, 15, 21, 28, 36, 45])
print(torch.cumprod(a,0)) # tensor([ 1,  2,  6, 24, 120, 720, 5040,  40320, 362880])
print(torch.cummax(a,0).values) # tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
print(torch.cummax(a,0).indices) # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
print(torch.cummin(a,0)) # torch.return_types.cummin(values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))

# torch.sort å’Œ torch.topk å¯ä»¥å¯¹å¼ é‡æ’åº
a = torch.tensor([[9,7,8],[1,3,2],[5,6,4]]).float()
print(torch.topk(a,2,dim = 0),"\n") # torch.return_types.topk(values=tensor([[9., 7., 8.],[5., 6., 4.]]),indices=tensor([[0, 0, 0],[2, 2, 2]])) 
print(torch.topk(a,2,dim = 1),"\n") # torch.return_types.topk(values=tensor([[9., 8.],[3., 2.],[6., 5.]]),indices=tensor([[0, 2],[1, 2],[1, 0]])) 
print(torch.sort(a,dim = 1),"\n") # torch.return_types.sort(values=tensor([[7., 8., 9.],[1., 2., 3.],[4., 5., 6.]]),indices=tensor([[1, 2, 0],[0, 2, 1],[2, 0, 1]])) 

# åˆ©ç”¨ torch.topk å¯ä»¥åœ¨ Pytorch ä¸­å®ç° KNN ç®—æ³•
```

##### çŸ©é˜µè¿ç®—

çŸ©é˜µå¿…é¡»æ˜¯ **äºŒç»´** çš„

çŸ©é˜µè¿ç®—åŒ…æ‹¬ï¼šçŸ©é˜µä¹˜æ³•ï¼ŒçŸ©é˜µé€†ï¼ŒçŸ©é˜µæ±‚è¿¹ï¼ŒçŸ©é˜µèŒƒæ•°ï¼ŒçŸ©é˜µè¡Œåˆ—å¼ï¼ŒçŸ©é˜µæ±‚ç‰¹å¾å€¼ï¼ŒçŸ©é˜µåˆ†è§£ç­‰è¿ç®—ã€‚

```py
# çŸ©é˜µä¹˜æ³•
a = torch.tensor([[1,2],[3,4]])
b = torch.tensor([[2,0],[0,2]])
print(a@b)  #ç­‰ä»·äº torch.matmul(a,b) æˆ– torch.mm(a,b) tensor([[2, 4],[6, 8]])

# é«˜ç»´å¼ é‡çš„çŸ©é˜µä¹˜æ³•åœ¨åé¢çš„ç»´åº¦ä¸Šè¿›è¡Œ
a = torch.randn(5,5,6)
b = torch.randn(5,6,4)
(a@b).shape # torch.Size([5, 5, 4])

# çŸ©é˜µè½¬ç½®
a = torch.tensor([[1.0,2],[3,4]])
print(a.t()) # tensor([[1., 3.],[2., 4.]])

# çŸ©é˜µé€†ï¼ˆé€†çŸ©é˜µï¼‰ï¼Œå¿…é¡»ä¸ºæµ®ç‚¹ç±»å‹
a = torch.tensor([[1.0,2],[3,4]])
print(torch.inverse(a)) # tensor([[-2.0000,  1.0000],[ 1.5000, -0.5000]])

# çŸ©é˜µæ±‚ trace ç§©
a = torch.tensor([[1.0,2],[3,4]])
print(torch.trace(a)) # tensor(5.)

# çŸ©é˜µæ±‚èŒƒæ•°
a = torch.tensor([[1.0,2],[3,4]])
print(torch.norm(a)) # tensor(5.4772)

# çŸ©é˜µè¡Œåˆ—å¼
a = torch.tensor([[1.0,2],[3,4]])
print(torch.det(a)) # tensor(-2.)

# çŸ©é˜µç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
a = torch.tensor([[1.0,2],[-5,4]],dtype = torch.float)
print(torch.linalg.eig(a)) 
'''
ä¸¤ä¸ªç‰¹å¾å€¼åˆ†åˆ«æ˜¯ -2.5+2.7839j, 2.5-2.7839j 
torch.return_types.linalg_eig(
eigenvalues=tensor([2.5000+2.7839j, 2.5000-2.7839j]),
eigenvectors=tensor([[0.2535-0.4706j, 0.2535+0.4706j],
        [0.8452+0.0000j, 0.8452-0.0000j]]))
'''

# çŸ©é˜µ svd åˆ†è§£
# svd åˆ†è§£å¯ä»¥å°†ä»»æ„ä¸€ä¸ªçŸ©é˜µåˆ†è§£ä¸ºä¸€ä¸ªæ­£äº¤çŸ©é˜µ u,ä¸€ä¸ªå¯¹è§’é˜µ s å’Œä¸€ä¸ªæ­£äº¤çŸ©é˜µ v.t() çš„ä¹˜ç§¯
# svd å¸¸ç”¨äºçŸ©é˜µå‹ç¼©å’Œé™ç»´
a=torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]])
u,s,v = torch.linalg.svd(a)
print(u,"\n")
print(s,"\n")
print(v,"\n")

import torch.nn.functional as F 
print(u@F.pad(torch.diag(s),(0,0,0,1))@v.t())
#åˆ©ç”¨ svd åˆ†è§£å¯ä»¥åœ¨Pytorchä¸­å®ç°ä¸»æˆåˆ†åˆ†æé™ç»´
'''
tensor([[-0.2298,  0.8835,  0.4082],
        [-0.5247,  0.2408, -0.8165],
        [-0.8196, -0.4019,  0.4082]]) 

tensor([9.5255, 0.5143]) 

tensor([[-0.6196, -0.7849],
        [-0.7849,  0.6196]]) 

tensor([[1.0000, 2.0000],
        [3.0000, 4.0000],
        [5.0000, 6.0000]])
'''
```

##### ä»»æ„ç»´å¼ é‡è¿ç®—

torch.einsumï¼šçˆ±å› æ–¯å¦æ±‚å’Œå‡½æ•°ã€‚torch.einsum æ”¯æŒæ±‚å¯¼å’Œåå‘ä¼ æ’­ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡éå¸¸é«˜

einsum æä¾›äº†ä¸€å¥—æ—¢ç®€æ´åˆä¼˜é›…çš„è§„åˆ™ï¼Œå¯å®ç°åŒ…æ‹¬ä½†ä¸é™äºï¼šå†…ç§¯ï¼Œå¤–ç§¯ï¼ŒçŸ©é˜µä¹˜æ³•ï¼Œè½¬ç½®å’Œå¼ é‡æ”¶ç¼©ï¼ˆtensor contractionï¼‰ç­‰å¼ é‡æ“ä½œï¼Œç†Ÿç»ƒæŒæ¡ einsum å¯ä»¥å¾ˆæ–¹ä¾¿çš„å®ç°å¤æ‚çš„å¼ é‡æ“ä½œï¼Œè€Œä¸”ä¸å®¹æ˜“å‡ºé”™ã€‚

- **einsum è§„åˆ™åŸç†**

  einsum å‡½æ•°çš„æ€æƒ³èµ·æºäºçˆ±å› æ–¯å¦ï¼Œæ±‚å’Œå¯¼è‡´ç»´åº¦æ”¶ç¼©ï¼Œå› æ­¤æ±‚å’Œç¬¦å·æ“ä½œçš„æŒ‡æ ‡æ€»æ˜¯åªå‡ºç°åœ¨å…¬å¼çš„ä¸€è¾¹ï¼Œä¾‹å¦‚åœ¨æˆ‘ä»¬ç†Ÿæ‚‰çš„çŸ©é˜µä¹˜æ³•ä¸­
  $$
  C_{ij} = \sum_{k}{A_{ik}B_{kj}}
  $$
  

  k è¿™ä¸ªä¸‹æ ‡è¢«æ±‚å’Œäº†ï¼Œæ±‚å’Œå¯¼è‡´äº†è¿™ä¸ªç»´åº¦çš„æ¶ˆå¤±ï¼Œæ‰€ä»¥å®ƒåªå‡ºç°åœ¨å³è¾¹è€Œä¸å‡ºç°åœ¨å·¦è¾¹

  è¿™ç§åªå‡ºç°åœ¨å¼ é‡å…¬å¼çš„ä¸€è¾¹çš„ä¸‹æ ‡è¢«ç§°ä¹‹ä¸ºå“‘æŒ‡æ ‡ï¼Œåä¹‹ä¸ºè‡ªç”±æŒ‡æ ‡

  è¿™ç§åªå‡ºç°åœ¨ä¸€è¾¹çš„å“‘æŒ‡æ ‡ä¸€å®šæ˜¯è¢«æ±‚å’Œæ±‚æ‰çš„ï¼Œå¹²è„†æŠŠå¯¹åº”çš„âˆ‘âˆ‘æ±‚å’Œç¬¦å·çœç•¥

  è¿™å°±æ˜¯çˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šï¼š

  **åªå‡ºç°åœ¨å…¬å¼ä¸€è¾¹çš„æŒ‡æ ‡å«åšå“‘æŒ‡æ ‡ï¼Œé’ˆå¯¹å“‘æŒ‡æ ‡çš„ âˆ‘ æ±‚å’Œç¬¦å·å¯ä»¥çœç•¥**
  $$
  C_{ij} = {A_{ik}B_{kj}}
  $$
  è¿™ä¸ªå…¬å¼è¡¨è¾¾çš„å«ä¹‰å¦‚ä¸‹:

  - C è¿™ä¸ªå¼ é‡çš„ç¬¬ i è¡Œç¬¬jåˆ—ç”± ğ´ è¿™ä¸ªå¼ é‡çš„ç¬¬iè¡Œç¬¬ k åˆ—å’Œ ğµ è¿™ä¸ªå¼ é‡çš„ç¬¬ k è¡Œç¬¬jåˆ—ç›¸ä¹˜ï¼Œè¿™æ ·å¾—åˆ°çš„æ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡ ğ·, å…¶å…ƒç´ ä¸º ğ·~ğ‘–ğ‘˜ğ‘—~ï¼Œç„¶åå¯¹ ğ· åœ¨ç»´åº¦ k ä¸Šæ±‚å’Œå¾—åˆ°

  - å…¬å¼å±•ç°å½¢å¼ä¸­é™¤äº†çœå»äº†æ±‚å’Œç¬¦å·ï¼Œè¿˜çœå»äº†ä¹˜æ³•ç¬¦å·

  - å€Ÿé‰´çˆ±å› æ–¯å¦æ±‚å’Œçº¦å®šè¡¨è¾¾å¼ é‡è¿ç®—çš„æ¸…çˆ½æ•´æ´ï¼Œnumpyã€tensorflow å’Œ torch ç­‰åº“ä¸­éƒ½å¼•å…¥äº† einsum è¿™ä¸ªå‡½æ•°

  - ä¸Šè¿°çŸ©é˜µä¹˜æ³•å¯ä»¥è¢«einsumè¿™ä¸ªå‡½æ•°è¡¨è¿°æˆ

    ```py
    C = torch.einsum("ik,kj->ij",A,B)
    ```

    è¿™ä¸ªå‡½æ•°çš„è§„åˆ™åŸç†éå¸¸ç®€æ´

    1. ç”¨å…ƒç´ è®¡ç®—å…¬å¼æ¥è¡¨è¾¾å¼ é‡è¿ç®—
    2. åªå‡ºç°åœ¨å…ƒç´ è®¡ç®—å…¬å¼ç®­å¤´å·¦è¾¹çš„æŒ‡æ ‡å«åšå“‘æŒ‡æ ‡
    3. çœç•¥å…ƒç´ è®¡ç®—å…¬å¼ä¸­å¯¹å“‘æŒ‡æ ‡çš„æ±‚å’Œç¬¦å·

    ```py
    import torch 
    
    A = torch.tensor([[1,2],[3,4.0]])
    B = torch.tensor([[5,6],[7,8.0]])
    
    C1 = A@B
    print(C1) # tensor([[19., 22.],[43., 50.]])
    
    C2 = torch.einsum("ik,kj->ij",[A,B])
    print(C2) # tensor([[19., 22.],[43., 50.]])
    ```

- **einsum åŸºç¡€èŒƒä¾‹**

  einsum è¿™ä¸ªå‡½æ•°çš„ç²¾é«“å®é™…ä¸Šæ˜¯ç¬¬ä¸€æ¡:

  - ç”¨å…ƒç´ è®¡ç®—å…¬å¼æ¥è¡¨è¾¾å¼ é‡è¿ç®—
  - ç»å¤§éƒ¨åˆ†å¼ é‡è¿ç®—éƒ½å¯ä»¥ç”¨å…ƒç´ è®¡ç®—å…¬å¼å¾ˆæ–¹ä¾¿åœ°æ¥è¡¨è¾¾ï¼Œè¿™ä¹Ÿæ˜¯å®ƒä¸ºä»€ä¹ˆä¼šé‚£ä¹ˆç¥é€šå¹¿å¤§

  ```py
  # ä¾‹1ï¼Œå¼ é‡è½¬ç½®
  A = torch.randn(3,4,5)
  
  # B = torch.permute(A,[0,2,1])
  B = torch.einsum("ijk->ikj",A) 
  
  print("before:",A.shape) # before: torch.Size([3, 4, 5])
  print("after:",B.shape) # after: torch.Size([3, 5, 4])
  
  # ä¾‹2ï¼Œå–å¯¹è§’å…ƒ
  A = torch.randn(5,5)
  # B = torch.diagonal(A)
  B = torch.einsum("ii->i",A)
  print("before:",A.shape) # before: torch.Size([5, 5])
  print("after:",B.shape) # after: torch.Size([5])
  
  # ä¾‹3ï¼Œæ±‚å’Œé™ç»´
  A = torch.randn(4,5)
  # B = torch.sum(A,1)
  B = torch.einsum("ij->i",A)
  print("before:",A.shape) # before: torch.Size([4, 5])
  print("after:",B.shape) # after: torch.Size([4])
  
  # ä¾‹4ï¼Œå“ˆè¾¾ç›ç§¯
  A = torch.randn(5,5)
  B = torch.randn(5,5)
  # C=A*B
  C = torch.einsum("ij,ij->ij",A,B)
  print("before:",A.shape, B.shape) # before: torch.Size([5, 5]) torch.Size([5, 5])
  print("after:",C.shape) # after: torch.Size([5, 5])
  
  # ä¾‹5ï¼Œå‘é‡å†…ç§¯
  A = torch.randn(10)
  B = torch.randn(10)
  # C=torch.dot(A,B)
  C = torch.einsum("i,i->",A,B)
  print("before:",A.shape, B.shape) # before: torch.Size([10]) torch.Size([10])
  print("after:",C.shape) # after: torch.Size([])
  
  # ä¾‹6ï¼Œå‘é‡å¤–ç§¯(ç±»ä¼¼ç¬›å¡å°”ç§¯)
  A = torch.randn(10)
  B = torch.randn(5)
  # C = torch.outer(A,B)
  C = torch.einsum("i,j->ij",A,B)
  print("before:",A.shape, B.shape) # before: torch.Size([10]) torch.Size([5])
  print("after:",C.shape) # after: torch.Size([10, 5])
  
  # ä¾‹7ï¼ŒçŸ©é˜µä¹˜æ³•
  A = torch.randn(5,4)
  B = torch.randn(4,6)
  # C = torch.matmul(A,B)
  C = torch.einsum("ik,kj->ij",A,B)
  print("before:",A.shape, B.shape) # before: torch.Size([5, 4]) torch.Size([4, 6])
  print("after:",C.shape) # after: torch.Size([5, 6])
  
  
  #ä¾‹8ï¼Œå¼ é‡ç¼©å¹¶
  A = torch.randn(3,4,5)
  B = torch.randn(4,3,6)
  # C = torch.tensordot(A,B,dims=[(0,1),(1,0)])
  C = torch.einsum("ijk,jih->kh",A,B)
  print("before:",A.shape, B.shape) # before: torch.Size([3, 4, 5]) torch.Size([4, 3, 6])
  print("after:",C.shape) # after: torch.Size([5, 6])
  ```

- **einsum é«˜çº§èŒƒä¾‹**

  einsum å¯ç”¨äºè¶…è¿‡ä¸¤ä¸ªå¼ é‡çš„è®¡ç®—

  ä¾‹å¦‚ï¼šåŒçº¿æ€§å˜æ¢ã€‚è¿™æ˜¯å‘é‡å†…ç§¯çš„ä¸€ç§æ‰©å±•ï¼Œä¸€ç§å¸¸ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹å¼

  ä¸è€ƒè™‘ batch ç»´åº¦æ—¶ï¼ŒåŒçº¿æ€§å˜æ¢çš„å…¬å¼ï¼š$A=qWk^T$

  è€ƒè™‘ batch ç»´åº¦æ—¶ï¼Œæ— æ³•ç”¨çŸ©é˜µä¹˜æ³•è¡¨ç¤ºï¼Œå¯ä»¥ç”¨å…ƒç´ è®¡ç®—å…¬å¼è¡¨è¾¾ï¼š$A_{ij}=\sum_{k}\sum_{l}{Q_{ik}W_{jkl}K_{il}}=Q_{ik}W_{jkl}K_{il}$

  ```py
  # ä¾‹9ï¼Œbilinear æ³¨æ„åŠ›æœºåˆ¶
  
  #====ä¸è€ƒè™‘ batch ç»´åº¦====
  q = torch.randn(10) # query_features
  k = torch.randn(10) # key_features
  W = torch.randn(5,10,10) # out_features,query_features,key_features
  b = torch.randn(5) # out_features
  
  # a = q@W@k.t()+b  
  a = torch.bilinear(q,k,W,b)
  print("a.shape:",a.shape) # a.shape: torch.Size([5])
  
  
  #=====è€ƒè™‘ batch ç»´åº¦====
  Q = torch.randn(8,10)    #batch_size,query_features
  K = torch.randn(8,10)    #batch_size,key_features
  W = torch.randn(5,10,10) #out_features,query_features,key_features
  b = torch.randn(5)       #out_features
  
  #A = torch.bilinear(Q,K,W,b)
  A = torch.einsum('bq,oqk,bk->bo',Q,W,K) + b
  print("A.shape:",A.shape) # A.shape: torch.Size([8, 5])
  ```

  ä¹Ÿå¯ä»¥ç”¨ einsum æ¥å®ç°æ›´å¸¸è§çš„ scaled-dot-product å½¢å¼çš„ Attention

  ä¸è€ƒè™‘ batch ç»´åº¦æ—¶ï¼Œscaled-dot-product å½¢å¼çš„ Attention ç”¨çŸ©é˜µä¹˜æ³•å…¬å¼è¡¨ç¤ºï¼š$a=softmax(\frac{ak^{T}}{d_k} )$

  è€ƒè™‘ batch ç»´åº¦æ—¶ï¼Œæ— æ³•ç”¨çŸ©é˜µä¹˜æ³•è¡¨ç¤ºï¼Œå¯ä»¥ç”¨å…ƒç´ è®¡ç®—å…¬å¼è¡¨è¾¾ $A_{ij}=softmax(\frac{Q_{in}K_{ijn}}{d_k})$

  ```py
  # ä¾‹10ï¼Œscaled-dot-product æ³¨æ„åŠ›æœºåˆ¶
  
  #====ä¸è€ƒè™‘ batch ç»´åº¦====
  q = torch.randn(10)  # query_features
  k = torch.randn(6,10) # key_size, key_features
  
  d_k = k.shape[-1]
  a = torch.softmax(q@k.t()/d_k,-1) 
  
  print("a.shape=",a.shape )
  
  #====è€ƒè™‘ batch ç»´åº¦====
  Q = torch.randn(8,10)  #batch_size,query_features
  K = torch.randn(8,6,10) #batch_size,key_size,key_features
  
  d_k = K.shape[-1]
  A = torch.softmax(torch.einsum("in,ijn->ij",Q,K)/d_k,-1) 
  
  print("A.shape=",A.shape )
  
  #æ€§èƒ½æµ‹è¯•
  
  #=====è€ƒè™‘ batch ç»´åº¦====
  Q = torch.randn(80,100)    #batch_size,query_features
  K = torch.randn(80,100)    #batch_size,key_features
  W = torch.randn(50,100,100) #out_features,query_features,key_features
  b = torch.randn(50)       #out_features
  
  %%timeit 
  A = torch.bilinear(Q,K,W,b)
  # 1.83 ms Â± 78.1 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
  
  %%timeit 
  A = torch.einsum('bq,oqk,bk->bo',Q,W,K) + b
  # 636 Âµs Â± 27.5 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)ã€
  ```

##### å¹¿æ’­æœºåˆ¶

Pytorch çš„å¹¿æ’­è§„åˆ™å’Œ numpy æ˜¯ä¸€æ ·çš„:

1. å¼ é‡çš„ç»´åº¦ä¸åŒï¼Œå°†ç»´åº¦è¾ƒå°çš„å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œç›´åˆ°ä¸¤ä¸ªå¼ é‡çš„ç»´åº¦éƒ½ä¸€æ ·
2. ä¸¤ä¸ªå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šçš„é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œæˆ–è€…å…¶ä¸­ä¸€ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šçš„é•¿åº¦ä¸º1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¯´è¿™ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šæ˜¯ç›¸å®¹çš„
3. ä¸¤ä¸ªå¼ é‡åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½æ˜¯ç›¸å®¹çš„ï¼Œå®ƒä»¬å°±èƒ½ä½¿ç”¨å¹¿æ’­
4. å¹¿æ’­ä¹‹åï¼Œæ¯ä¸ªç»´åº¦çš„é•¿åº¦å°†å–ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦é•¿åº¦çš„è¾ƒå¤§å€¼
5. åœ¨ä»»ä½•ä¸€ä¸ªç»´åº¦ä¸Šï¼Œå¦‚æœä¸€ä¸ªå¼ é‡çš„é•¿åº¦ä¸º1ï¼Œå¦ä¸€ä¸ªå¼ é‡é•¿åº¦å¤§äº1ï¼Œé‚£ä¹ˆåœ¨è¯¥ç»´åº¦ä¸Šï¼Œå°±å¥½åƒæ˜¯å¯¹ç¬¬ä¸€ä¸ªå¼ é‡è¿›è¡Œäº† **å¤åˆ¶**

torch.broadcast_tensors å¯ä»¥å°†å¤šä¸ªå¼ é‡æ ¹æ®å¹¿æ’­è§„åˆ™è½¬æ¢æˆç›¸åŒçš„ç»´åº¦

ç»´åº¦æ‰©å±•å…è®¸çš„æ“ä½œæœ‰ä¸¤ç§ï¼š

1. å¢åŠ ä¸€ä¸ªç»´åº¦
2. å¯¹é•¿åº¦ä¸º 1 çš„ç»´åº¦è¿›è¡Œå¤åˆ¶æ‰©å±•

```py
a = torch.tensor([1,2,3])
b = torch.tensor([[0,0,0],[1,1,1],[2,2,2]])
print(b + a)  # tensor([[1, 2, 3],[2, 3, 4],[3, 4, 5]])
torch.cat([a[None,:]]*3,dim=0) + b  # tensor([[1, 2, 3],[2, 3, 4],[3, 4, 5]])
a_broad,b_broad = torch.broadcast_tensors(a,b)
print(a_broad,"\n") # tensor([[1, 2, 3],[1, 2, 3],[1, 2, 3]]) 
print(b_broad,"\n") # tensor([[0, 0, 0],[1, 1, 1],[2, 2, 2]]) 
print(a_broad + b_broad) # tensor([[1, 2, 3],[2, 3, 4],[3, 4, 5]])
```

#### nn.functional å’Œ nn.Module

##### ç®€ä»‹

å‰é¢æˆ‘ä»¬ä»‹ç»äº† Pytorch çš„å¼ é‡çš„ç»“æ„æ“ä½œå’Œæ•°å­¦è¿ç®—ä¸­çš„ä¸€äº›å¸¸ç”¨ API

åˆ©ç”¨è¿™äº›å¼ é‡çš„ API æˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºç¥ç»ç½‘ç»œç›¸å…³çš„ç»„ä»¶(å¦‚æ¿€æ´»å‡½æ•°ï¼Œæ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°)

Pytorch å’Œç¥ç»ç½‘ç»œç›¸å…³çš„åŠŸèƒ½ç»„ä»¶å¤§å¤šéƒ½å°è£…åœ¨ **torch.nn** æ¨¡å—ä¸‹

è¿™äº›åŠŸèƒ½ç»„ä»¶çš„ç»å¤§éƒ¨åˆ†æ—¢æœ‰å‡½æ•°å½¢å¼å®ç°ï¼Œä¹Ÿæœ‰ç±»å½¢å¼å®ç°ã€‚

å…¶ä¸­nn.functionalï¼ˆä¸€èˆ¬å¼•å…¥åæ”¹åä¸º **F**ï¼‰æœ‰å„ç§åŠŸèƒ½ç»„ä»¶çš„å‡½æ•°å®ç°

- æ¿€æ´»å‡½æ•°
  - F.relu
  - F.sigmoid
  - F.tanh
  - F.softmax
- æ¨¡å‹å±‚
  - F.linear
  - F.conv2d
  - F.max_pool2d
  - F.dropout2d
  - F.embedding
- æŸå¤±å‡½æ•°
  - F.binary_cross_entropy
  - F.mse_loss
  - F.cross_entropy

ä¸ºäº†ä¾¿äºå¯¹å‚æ•°è¿›è¡Œç®¡ç†ï¼Œä¸€èˆ¬é€šè¿‡ç»§æ‰¿ nn.Module è½¬æ¢æˆä¸ºç±»çš„å®ç°å½¢å¼ï¼Œå¹¶ç›´æ¥å°è£…åœ¨ nn æ¨¡å—ä¸‹ã€‚ä¾‹å¦‚ï¼š

- æ¿€æ´»å‡½æ•°
  - nn.ReLU
  - nn.Sigmoid
  - nn.Tanh
  - nn.Softmax
- æ¨¡å‹å±‚
  - nn.Linear
  - nn.Conv2d
  - nn.MaxPool2d
  - nn.Dropout2d
  - nn.Embedding
- æŸå¤±å‡½æ•°
  - nn.BCELoss
  - nn.MSELoss
  - nn.CrossEntropyLoss

å®é™…ä¸Š nn.Module é™¤äº†å¯ä»¥ç®¡ç†å…¶å¼•ç”¨çš„å„ç§å‚æ•°ï¼Œè¿˜å¯ä»¥ç®¡ç†å…¶å¼•ç”¨çš„å­æ¨¡å—ï¼ŒåŠŸèƒ½ååˆ†å¼ºå¤§

```py
import torch 
import torch.nn.functional as F 
torch.relu(torch.tensor(-1.0))  # tensor(0.)
F.relu(torch.tensor(-1.0)) # tensor(0.)
```

##### ä½¿ç”¨ nn.Module æ¥ç®¡ç†å‚æ•°(é…åˆ nn.Parameter ä½¿ç”¨)

åœ¨ Pytorch ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°æ˜¯éœ€è¦è¢«ä¼˜åŒ–å™¨è®­ç»ƒçš„ï¼Œå› æ­¤ï¼Œé€šå¸¸è¦è®¾ç½®å‚æ•°ä¸º requires_grad = True çš„å¼ é‡ã€‚

åŒæ—¶ï¼Œåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œå¾€å¾€æœ‰è®¸å¤šçš„å‚æ•°ï¼Œè¦æ‰‹åŠ¨ç®¡ç†è¿™äº›å‚æ•°å¹¶ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ã€‚

Pytorch ä¸€èˆ¬å°†å‚æ•°ç”¨ nn.Parameter æ¥è¡¨ç¤ºï¼Œå¹¶ä¸”ç”¨ nn.Module æ¥ç®¡ç†å…¶ç»“æ„ä¸‹çš„æ‰€æœ‰å‚æ•°ã€‚

```py
import torch 
from torch import nn 
import torch.nn.functional  as F

torch.randn(2,2,requires_grad = True) # tensor([[0.1829, 0.0693],[0.0767, 1.2441]], requires_grad=True)

# nn.Parameter å…·æœ‰ requires_grad = True å±æ€§
w = nn.Parameter(torch.randn(2,2))
print(w) # Parameter containing:tensor([[-0.8092, -0.8830],[ 1.6357, -0.1740]], requires_grad=True)
print(w.requires_grad) # True

# nn.ParameterList å¯ä»¥å°†å¤šä¸ª nn.Parameter ç»„æˆä¸€ä¸ªåˆ—è¡¨
params_list = nn.ParameterList([nn.Parameter(torch.rand(8,i)) for i in range(1,3)])
'''
ParameterList(
    (0): Parameter containing: [torch.float32 of size 8x1]
    (1): Parameter containing: [torch.float32 of size 8x2]
)
'''
print(params_list) 
print(params_list[0].requires_grad) # True

# nn.ParameterDict å¯ä»¥å°†å¤šä¸ª nn.Parameter ç»„æˆä¸€ä¸ªå­—å…¸
params_dict = nn.ParameterDict({"a":nn.Parameter(torch.rand(2,2)),
                               "b":nn.Parameter(torch.zeros(2))})
'''
ParameterDict(
    (a): Parameter containing: [torch.FloatTensor of size 2x2]
    (b): Parameter containing: [torch.FloatTensor of size 2]
)
'''
print(params_dict)
print(params_dict["a"].requires_grad) # True

# å¯ä»¥ç”¨ Module å°†å®ƒä»¬ç®¡ç†èµ·æ¥
# module.parameters() è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬å…¶ç»“æ„ä¸‹çš„æ‰€æœ‰ parameters
module = nn.Module()
module.w = nn.Parameter(torch.randn(2,2))
module.params_list = nn.ParameterList([nn.Parameter(torch.rand(8,i)) for i in range(1,3)])
module.params_dict = nn.ParameterDict({"a":nn.Parameter(torch.rand(2,2)),
                               "b":nn.Parameter(torch.zeros(2))})

num_param = 0
for param in module.named_parameters():
    print(param,"\n")
    num_param = num_param + 1
print("number of Parameters =",num_param)
'''
('w', Parameter containing:
tensor([[-1.2390,  0.3316],
        [-0.4232, -0.0090]], requires_grad=True)) 

('params_list.0', Parameter containing:
tensor([[0.8785],
        [0.6456],
        [0.4697],
        [0.8962],
        [0.1122],
        [0.4837],
        [0.8089],
        [0.0515]], requires_grad=True)) 

('params_list.1', Parameter containing:
tensor([[0.7440, 0.5626],
        [0.2430, 0.0113],
        [0.5884, 0.0815],
        [0.7125, 0.4120],
        [0.7275, 0.1608],
        [0.4658, 0.0085],
        [0.8578, 0.7290],
        [0.0327, 0.2239]], requires_grad=True)) 

('params_dict.a', Parameter containing:
tensor([[0.6698, 0.5646],
        [0.2482, 0.8258]], requires_grad=True)) 

('params_dict.b', Parameter containing:
tensor([0., 0.], requires_grad=True)) 

number of Parameters = 5
'''


# å®è·µå½“ä¸­ï¼Œä¸€èˆ¬é€šè¿‡ç»§æ‰¿ nn.Module æ¥æ„å»ºæ¨¡å—ç±»ï¼Œå¹¶å°†æ‰€æœ‰å«æœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°çš„éƒ¨åˆ†æ”¾åœ¨ **æ„é€ å‡½æ•°** ä¸­ã€‚
# ä»¥ä¸‹èŒƒä¾‹ä¸º Pytorch ä¸­ nn.Linear çš„æºç çš„ç®€åŒ–ç‰ˆæœ¬
# å¯ä»¥çœ‹åˆ°å®ƒå°†éœ€è¦å­¦ä¹ çš„å‚æ•°æ”¾åœ¨äº† __init__ æ„é€ å‡½æ•°ä¸­ï¼Œå¹¶åœ¨ forward ä¸­è°ƒç”¨ F.linear å‡½æ•°æ¥å®ç°è®¡ç®—é€»è¾‘ã€‚

class Linear(nn.Module):
    __constants__ = ['in_features', 'out_features']

    def __init__(self, in_features, out_features, bias=True):
        super(Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)

    def forward(self, input):
        return F.linear(input, self.weight, self.bias)
   
```

##### ä½¿ç”¨ nn.Module æ¥ç®¡ç†å­æ¨¡å—

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½å¾ˆå°‘ç›´æ¥ä½¿ç”¨ nn.Parameter æ¥å®šä¹‰å‚æ•°æ„å»ºæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ä¸€äº›æ‹¼è£…ä¸€äº›å¸¸ç”¨çš„æ¨¡å‹å±‚æ¥æ„é€ æ¨¡å‹

è¿™äº›æ¨¡å‹å±‚ä¹Ÿæ˜¯ç»§æ‰¿è‡ª nn.Module çš„å¯¹è±¡ï¼Œæœ¬èº«ä¹ŸåŒ…æ‹¬å‚æ•°ï¼Œå±äºæˆ‘ä»¬è¦å®šä¹‰çš„æ¨¡å—çš„å­æ¨¡å—

nn.Module æä¾›äº†ä¸€äº›æ–¹æ³•å¯ä»¥ç®¡ç†è¿™äº›å­æ¨¡å—

- children() æ–¹æ³•: è¿”å›ç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å­æ¨¡å—ã€‚
- named_children() æ–¹æ³•ï¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å­æ¨¡å—ï¼Œä»¥åŠå®ƒä»¬çš„åå­—ã€‚
- modules() æ–¹æ³•ï¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å„ä¸ªå±‚çº§çš„æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å—æœ¬èº«ã€‚
- named_modules() æ–¹æ³•ï¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å„ä¸ªå±‚çº§çš„æ¨¡å—ä»¥åŠå®ƒä»¬çš„åå­—ï¼ŒåŒ…æ‹¬æ¨¡å—æœ¬èº«ã€‚

å…¶ä¸­ chidren() æ–¹æ³•å’Œ named_children() æ–¹æ³•è¾ƒå¤šä½¿ç”¨ã€‚

modules() æ–¹æ³•å’Œ named_modules() æ–¹æ³•è¾ƒå°‘ä½¿ç”¨ï¼Œå…¶åŠŸèƒ½å¯ä»¥é€šè¿‡å¤šä¸ª named_children() çš„åµŒå¥—ä½¿ç”¨å®ç°ã€‚

```py
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        
        self.embedding = nn.Embedding(num_embeddings = 10000,embedding_dim = 3,padding_idx = 1)
        self.conv = nn.Sequential()
        self.conv.add_module("conv_1",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))
        self.conv.add_module("pool_1",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_1",nn.ReLU())
        self.conv.add_module("conv_2",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))
        self.conv.add_module("pool_2",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_2",nn.ReLU())
        
        self.dense = nn.Sequential()
        self.dense.add_module("flatten",nn.Flatten())
        self.dense.add_module("linear",nn.Linear(6144,1))
        
    def forward(self,x):
        x = self.embedding(x).transpose(1,2)
        x = self.conv(x)
        y = self.dense(x)
        return y
    
net = Net()

i = 0
for child in net.children():
    i+=1
    print(child,"\n")
print("child number",i)
'''
Embedding(10000, 3, padding_idx=1) 

Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
) 

Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
) 

child number 3
'''

i = 0
for name,child in net.named_children():
    i+=1
    print(name,":",child,"\n")
print("child number",i)
'''
embedding : Embedding(10000, 3, padding_idx=1) 

conv : Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
) 

dense : Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
) 

child number 3
'''

i = 0
for module in net.modules():
    i+=1
    print(module)
print("module number:",i)
'''
Net(
  (embedding): Embedding(10000, 3, padding_idx=1)
  (conv): Sequential(
    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_1): ReLU()
    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_2): ReLU()
  )
  (dense): Sequential(
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (linear): Linear(in_features=6144, out_features=1, bias=True)
  )
)
Embedding(10000, 3, padding_idx=1)
Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
)
Conv1d(3, 16, kernel_size=(5,), stride=(1,))
MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
ReLU()
Conv1d(16, 128, kernel_size=(2,), stride=(1,))
MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
ReLU()
Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
)
Flatten(start_dim=1, end_dim=-1)
Linear(in_features=6144, out_features=1, bias=True)
module number: 12
'''
```

ä¸‹é¢æˆ‘ä»¬é€šè¿‡ named_children æ–¹æ³•æ‰¾åˆ° embedding å±‚ï¼Œå¹¶å°†å…¶å‚æ•°è®¾ç½®ä¸ºä¸å¯è®­ç»ƒ(ç›¸å½“äºå†»ç»“ embedding å±‚)ã€‚

```py
children_dict = {name:module for name,module in net.named_children()}

'''
{'embedding': Embedding(10000, 3, padding_idx=1), 'conv': Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
), 'dense': Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
)}
Embedding(10000, 3, padding_idx=1)
'''
print(children_dict)
embedding = children_dict["embedding"]
embedding.requires_grad_(False) #å†»ç»“å…¶å‚æ•°

#å¯ä»¥çœ‹åˆ°å…¶ç¬¬ä¸€å±‚çš„å‚æ•°å·²ç»ä¸å¯ä»¥è¢«è®­ç»ƒäº†ã€‚
for param in embedding.parameters():
    print(param.requires_grad) # False
    print(param.numel()) # 30000
    
from torchkeras import summary
summary(net,input_shape = (200,),input_dtype = torch.LongTensor);
# ä¸å¯è®­ç»ƒå‚æ•°æ•°é‡å¢åŠ 
'''
--------------------------------------------------------------------------
Layer (type)                            Output Shape              Param #
==========================================================================
Embedding-1                             [-1, 200, 3]               30,000
Conv1d-2                               [-1, 16, 196]                  256
MaxPool1d-3                             [-1, 16, 98]                    0
ReLU-4                                  [-1, 16, 98]                    0
Conv1d-5                               [-1, 128, 97]                4,224
MaxPool1d-6                            [-1, 128, 48]                    0
ReLU-7                                 [-1, 128, 48]                    0
Flatten-8                                 [-1, 6144]                    0
Linear-9                                     [-1, 1]                6,145
==========================================================================
Total params: 40,625
Trainable params: 10,625
Non-trainable params: 30,000
--------------------------------------------------------------------------
Input size (MB): 0.000763
Forward/backward pass size (MB): 0.287788
Params size (MB): 0.154972
Estimated Total Size (MB): 0.443523
'''
```

### 5.2ã€ä¸­é˜¶ API

#### Dataset å’Œ DateLoader

Pytorch é€šå¸¸ä½¿ç”¨ Dataset å’Œ DataLoader è¿™ä¸¤ä¸ªå·¥å…·ç±»æ¥æ„å»ºæ•°æ®ç®¡é“

- Dataset å®šä¹‰äº†æ•°æ®é›†çš„å†…å®¹ï¼Œå®ƒç›¸å½“äºä¸€ä¸ªç±»ä¼¼åˆ—è¡¨çš„æ•°æ®ç»“æ„ï¼Œå…·æœ‰ç¡®å®šçš„é•¿åº¦ï¼Œèƒ½å¤Ÿç”¨ç´¢å¼•è·å–æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

- DataLoader å®šä¹‰äº†æŒ‰ batch åŠ è½½æ•°æ®é›†çš„æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ä¸ªå®ç°äº†`__iter__`æ–¹æ³•çš„å¯è¿­ä»£å¯¹è±¡ï¼Œæ¯æ¬¡è¿­ä»£è¾“å‡ºä¸€ä¸ª batch çš„æ•°æ®

  DataLoader èƒ½å¤Ÿæ§åˆ¶ batch çš„å¤§å°ï¼Œbatch ä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°† batch ç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚

åœ¨ç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç”¨æˆ·åªéœ€å®ç° Dataset çš„ `__len__` æ–¹æ³•å’Œ `__getitem__` æ–¹æ³•ï¼Œå°±å¯ä»¥è½»æ¾æ„å»ºè‡ªå·±çš„æ•°æ®é›†ï¼Œå¹¶ç”¨é»˜è®¤æ•°æ®ç®¡é“è¿›è¡ŒåŠ è½½
