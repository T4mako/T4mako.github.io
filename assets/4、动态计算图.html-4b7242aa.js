import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{o as s,c as a,d as p}from"./app-acaf911b.js";const t={},o=p(`<h1 id="_4、动态计算图" tabindex="-1"><a class="header-anchor" href="#_4、动态计算图" aria-hidden="true">#</a> 4、动态计算图</h1><h2 id="_4-1、动态计算图简介" tabindex="-1"><a class="header-anchor" href="#_4-1、动态计算图简介" aria-hidden="true">#</a> 4.1、动态计算图简介</h2><p>计算图（Computational Graph）是一个有向无环图（DAG），计算图由节点和边组成</p><ul><li>节点：张量或者Function</li><li>边：张量和 Function 之间的依赖关系</li></ul><p>计算图是动态图，动态图有两层含义：</p><ul><li>计算图的 <strong>正向传播是立即执行</strong> 的。<br> 无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果</li><li><strong>计算图在反向传播后立即销毁</strong>。<br> 下次调用需要重新构建计算图。<br> 如果在程序中使用了 backward 方法执行了反向传播，或者利用 torch.autograd.grad 方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</li></ul><p>计算图是通过对张量的操作自动构建的。每次操作会创建一个新的节点，并连接到参与操作的张量。</p><p><strong>计算图的正向传播立即执行</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
Y_hat <span class="token operator">=</span> X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment"># Y_hat定义后其正向传播被立即执行，与其后面的 loss 创建语句无关</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 计算张量的平均值</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Y_hat<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>计算图在反向传播后立即销毁</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
Y_hat <span class="token operator">=</span> X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#loss.backward(retain_graph = True) </span>

<span class="token comment"># loss.backward() # 如果再次执行反向传播将报错</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="保存计算图多次反向传播" tabindex="-1"><a class="header-anchor" href="#保存计算图多次反向传播" aria-hidden="true">#</a> 保存计算图多次反向传播</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch

<span class="token comment"># 创建需要梯度的张量</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 第一次计算和反向传播</span>
z <span class="token operator">=</span> x <span class="token operator">*</span> y
w <span class="token operator">=</span> z <span class="token operator">*</span> y
w<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 保留计算图以便后续反向传播</span>

<span class="token comment"># 查看第一次反向传播后的梯度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;x.grad after first backward: </span><span class="token interpolation"><span class="token punctuation">{</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>  <span class="token comment"># 输出：tensor(4.0)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;y.grad after first backward: </span><span class="token interpolation"><span class="token punctuation">{</span>y<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>  <span class="token comment"># 输出：tensor(2.0)</span>

<span class="token comment"># 第二次计算和反向传播</span>
v <span class="token operator">=</span> x <span class="token operator">*</span> y
u <span class="token operator">=</span> v <span class="token operator">*</span> y
u<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 查看第二次反向传播后的梯度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;x.grad after second backward: </span><span class="token interpolation"><span class="token punctuation">{</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>  <span class="token comment"># 输出：tensor(8.0)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;y.grad after second backward: </span><span class="token interpolation"><span class="token punctuation">{</span>y<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>  <span class="token comment"># 输出：tensor(6.0)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_4-2、计算图中的-function" tabindex="-1"><a class="header-anchor" href="#_4-2、计算图中的-function" aria-hidden="true">#</a> 4.2、计算图中的 Function</h2><p>计算图中的另外一种节点是 Function, 实际上就是 Pytorch 中各种对张量操作的函数</p><p>这些 Function 同时包括正向计算逻辑和反向传播的逻辑</p><p>可以通过继承 torch.autograd.Function 来创建这种支持反向传播的 Function</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>
   
    <span class="token comment"># 正向传播逻辑，可以用 ctx 存储一些值，供反向传播使用。</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment"># 反向传播逻辑</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors
        grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
        grad_input<span class="token punctuation">[</span><span class="token builtin">input</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">return</span> grad_input
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span><span class="token builtin">apply</span> <span class="token comment"># relu 现在也可以具有正向传播和反向传播功能</span>
Y_hat <span class="token operator">=</span> relu<span class="token punctuation">(</span>X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># tensor([[4.5000, 4.5000]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># tensor([[4.5000]])</span>

<span class="token comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Y_hat<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_4-3、计算图与反向传播" tabindex="-1"><a class="header-anchor" href="#_4-3、计算图与反向传播" aria-hidden="true">#</a> 4.3、计算图与反向传播</h2><p>简单地理解一下反向传播的原理和过程</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>
y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>x
loss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>loss.backward() 语句调用后，依次发生以下计算过程。</p><ol><li><p>loss 自己的 grad 梯度赋值为 1，即对自身的梯度为 1</p></li><li><p>loss 根据其自身梯度以及关联的 backward 方法，计算出其对应的自变量即 y1 和 y2 的梯度，将该值赋值到 y1.grad 和 y2.grad</p></li><li><p>y2 和 y1 根据其自身梯度以及关联的 backward 方法, 分别计算出其对应的自变量 x 的梯度，x.grad 将其收到的多个梯度值累加</p><p>（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</p></li></ol><p>正因为求导链式法则衍生的梯度累加规则，张量的 grad 梯度不会自动清零，在需要的时候需要手动置零</p><h2 id="_4-4、叶子节点和非叶子节点" tabindex="-1"><a class="header-anchor" href="#_4-4、叶子节点和非叶子节点" aria-hidden="true">#</a> 4.4、叶子节点和非叶子节点</h2><p>举例，下面图中的叶子结点与非叶子结点：</p><ul><li>叶子结点：w，x，b</li><li>非叶子结点：y，z</li></ul><p>![image-20240821141758159](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Pytorch\\assets\\image-20240821141758159.png)</p><p>在反向传播过程中，只有 is_leaf=True 的 <strong>叶子节点</strong>，需要求导的张量的导数结果才会被最后保留下来。</p><blockquote><p><strong>叶子节点张量需要满足两个条件</strong>。</p><ol><li>叶子节点张量是 <strong>由用户直接创建</strong> 的张量，而非由某个 Function 通过计算得到的张量</li><li>叶子节点张量的 <strong>requires_grad 属性必须为 True</strong></li></ol></blockquote><blockquote><p>Pytorch 设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。</p><p>所有依赖于叶子节点张量的张量, 其 requires_grad 属性必定是 True 的，但其梯度值只在计算过程中被用到，不会最终存储到 grad 属性中。</p><p>如果需要保留中间计算结果的梯度到 grad 属性中，可以使用 retain_grad 方法。 如果仅仅是为了调试代码查看梯度值，可以利用 register_hook 打印日志。</p></blockquote><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>
y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>x
loss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;loss.grad:&quot;</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># loss.grad: None</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y1.grad:&quot;</span><span class="token punctuation">,</span> y1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># y1.grad: None</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y2.grad:&quot;</span><span class="token punctuation">,</span> y2<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># y2.grad: None</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># tensor(4.)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># True</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y1<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># False</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y2<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># False</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># False</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>利用 retain_grad 可以保留非叶子节点的梯度值，利用 register_hook 可以查看非叶子节点的梯度值</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 

<span class="token comment"># 正向传播</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>
y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>x
loss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

<span class="token comment"># 非叶子节点梯度显示控制</span>
y1<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;y1 grad: &#39;</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
y2<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;y2 grad: &#39;</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 反向传播</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;loss.grad:&quot;</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;x.grad:&quot;</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39;
y2 grad:  tensor(4.)
y1 grad:  tensor(-4.)
loss.grad: tensor(1.)
x.grad: tensor(4.)
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_4-5、计算图在-tensorboard-中的可视化" tabindex="-1"><a class="header-anchor" href="#_4-5、计算图在-tensorboard-中的可视化" aria-hidden="true">#</a> 4.5、计算图在 TensorBoard 中的可视化</h2><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> x@self<span class="token punctuation">.</span>w <span class="token operator">+</span> self<span class="token punctuation">.</span>b
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token string">&#39;./data/tensorboard&#39;</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>net<span class="token punctuation">,</span>input_to_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token operator">%</span>load_ext tensorboard
<span class="token comment">#%tensorboard --logdir ./data/tensorboard</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> tensorboard <span class="token keyword">import</span> notebook
notebook<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 在tensorboard中查看模型</span>
notebook<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token string">&quot;--logdir ./data/tensorboard&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,41),e=[o];function c(l,i){return s(),a("div",null,e)}const k=n(t,[["render",c],["__file","4、动态计算图.html.vue"]]);export{k as default};
