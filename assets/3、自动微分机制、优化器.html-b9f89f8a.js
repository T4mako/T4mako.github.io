import{_ as t}from"./plugin-vue_export-helper-c27b6911.js";import{r as o,o as e,c,b as n,f as s,e as l,d as a}from"./app-2ac1fdf0.js";const i={},u=a(`<h1 id="_3、自动微分机制、优化器" tabindex="-1"><a class="header-anchor" href="#_3、自动微分机制、优化器" aria-hidden="true">#</a> 3、自动微分机制、优化器</h1><p><strong>求梯度的两种方法</strong></p><ul><li><p>通过反向传播 <strong>backward</strong> 方法实现梯度计算。</p><p>该方法求得的梯度将存在对应 <strong>tenseor 的 grad 属性</strong> 下</p></li><li><p>也可以调用 <strong>torch.autograd.grad</strong> 函数来实现求梯度计算</p></li></ul><p><strong>注意</strong>：使用上述方法求梯度后必须清零，否则（tensor.grad）会造成累加计算</p><ul><li><strong>optimizer.zero_grad()</strong></li></ul><p><strong>创建需要求导的参数</strong></p><ul><li><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>value<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li><li><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>value<span class="token punctuation">)</span>
x<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><p><strong>优化器</strong></p>`,8),r={href:"http://torch.optim.XXX",target:"_blank",rel:"noopener noreferrer"},k=n("li",null,"更新参数：optimizer.step()",-1),d=a(`<h2 id="_3-1、backward-方法求导数" tabindex="-1"><a class="header-anchor" href="#_3-1、backward-方法求导数" aria-hidden="true">#</a> 3.1、backward 方法求导数</h2><ul><li><strong>backward</strong> 在 <strong>标量</strong> 张量上调用，该方法求得的梯度将存在对应自变量张量的 <strong>grad</strong> 属性下。</li><li><strong>backward</strong> 在 <strong>非标量</strong> 张量上调用，则要传入一个和它 <strong>同形</strong> 的 <strong>gradient</strong> 参数张量。</li></ul><p>相当于用该 gradient 参数张量与调用张量作向量点乘，得到的标量结果再反向传播。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># backward  标量的反向传播</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 

<span class="token comment"># f(x) = a*x**2 + b*x + c 的导数</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># x 需要被求导</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>

y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c 

y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
dy_dx <span class="token operator">=</span> x<span class="token punctuation">.</span>grad <span class="token comment"># tensor(-2.)</span>

<span class="token comment"># 非标量的反向传播</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># x 需要被求导</span>
y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c 

gradient <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>gradient <span class="token operator">=</span> gradient<span class="token punctuation">)</span>
x_grad <span class="token operator">=</span> x<span class="token punctuation">.</span>grad <span class="token comment"># # tensor([[-2., -2.],[ 0.,  2.]])</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_3-2、利用-autograd-grad-方法求导数" tabindex="-1"><a class="header-anchor" href="#_3-2、利用-autograd-grad-方法求导数" aria-hidden="true">#</a> 3.2、利用 autograd.grad 方法求导数</h2><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 

<span class="token comment"># f(x) = a*x**2 + b*x + c的导数</span>

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># x 需要被求导</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c

<span class="token comment"># create_graph 设置为 True 将允许创建更高阶的导数(对返回的变量二导，三导)</span>
dy_dx <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>y<span class="token punctuation">,</span>x<span class="token punctuation">,</span>create_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dy_dx<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token comment"># tensor(-2.)</span>

<span class="token comment"># 求二阶导数</span>
dy2_dx2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>dy_dx<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment"># tensor(2.)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 

x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># x 需要被求导</span>
x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

y1 <span class="token operator">=</span> x1<span class="token operator">*</span>x2
y2 <span class="token operator">=</span> x1<span class="token operator">+</span>x2

<span class="token comment"># 允许同时对多个自变量求导数</span>
<span class="token punctuation">(</span>dy1_dx1<span class="token punctuation">,</span>dy1_dx2<span class="token punctuation">)</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>outputs<span class="token operator">=</span>y1<span class="token punctuation">,</span>inputs <span class="token operator">=</span> <span class="token punctuation">[</span>x1<span class="token punctuation">,</span>x2<span class="token punctuation">]</span><span class="token punctuation">,</span>retain_graph <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dy1_dx1<span class="token punctuation">,</span>dy1_dx2<span class="token punctuation">)</span> <span class="token comment"># tensor(2.) tensor(1.)</span>

<span class="token comment"># 如果有多个因变量，相当于把多个因变量的梯度结果求和</span>
<span class="token punctuation">(</span>dy12_dx1<span class="token punctuation">,</span>dy12_dx2<span class="token punctuation">)</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>outputs<span class="token operator">=</span><span class="token punctuation">[</span>y1<span class="token punctuation">,</span>y2<span class="token punctuation">]</span><span class="token punctuation">,</span>inputs <span class="token operator">=</span> <span class="token punctuation">[</span>x1<span class="token punctuation">,</span>x2<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dy12_dx1<span class="token punctuation">,</span>dy12_dx2<span class="token punctuation">)</span> <span class="token comment"># tensor(3.) tensor(2.)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_3-3、利用自动微分和优化器求-optimizer-最小值" tabindex="-1"><a class="header-anchor" href="#_3-3、利用自动微分和优化器求-optimizer-最小值" aria-hidden="true">#</a> 3.3、利用自动微分和优化器求 optimizer 最小值</h2><p>![image-20240730143417803](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Pytorch\\assets\\image-20240730143417803.png)</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 

<span class="token comment"># f(x) = a*x**2 + b*x + c 的最小值</span>

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># x需要被求导</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token operator">=</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.01</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    result <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c 
    <span class="token keyword">return</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 清零所有的梯度信息，不清零，x.grad 实现累加</span>
    y <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播，计算梯度</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 更新参数</span>
   
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y=&quot;</span><span class="token punctuation">,</span>f<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">,</span><span class="token string">&quot;;&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;x=&quot;</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token comment"># y= tensor(0.) ; x= tensor(1.0000)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,10);function m(v,b){const p=o("ExternalLinkIcon");return e(),c("div",null,[u,n("ul",null,[n("li",null,[s("定义："),n("a",r,[s("torch.optim.XXX"),l(p)]),s("(model.parameters(),lr=learning_rate)")]),k]),d])}const x=t(i,[["render",m],["__file","3、自动微分机制、优化器.html.vue"]]);export{x as default};
