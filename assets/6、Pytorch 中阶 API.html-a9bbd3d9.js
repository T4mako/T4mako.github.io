import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as e,o as c,c as l,b as n,f as s,e as p,d as t}from"./app-effcce54.js";const u={},i=t(`<h1 id="_6、pytorch-中阶-api" tabindex="-1"><a class="header-anchor" href="#_6、pytorch-中阶-api" aria-hidden="true">#</a> 6、Pytorch 中阶 API</h1><h2 id="_6-1、dataset-和-dataloader" tabindex="-1"><a class="header-anchor" href="#_6-1、dataset-和-dataloader" aria-hidden="true">#</a> 6.1、Dataset 和 DataLoader</h2><p>Pytorch 使用 Dataset 和 DataLoader 这两个工具类来构建数据管道。它们的作用是将数据整理成适合训练模型的格式，一个 batch 一个 batch 的取出给模型</p><h3 id="_6-1-1、dataset-和-dataloader-原理" tabindex="-1"><a class="header-anchor" href="#_6-1-1、dataset-和-dataloader-原理" aria-hidden="true">#</a> 6.1.1、Dataset 和 DataLoader 原理</h3><h4 id="获取一个-batch-的步骤" tabindex="-1"><a class="header-anchor" href="#获取一个-batch-的步骤" aria-hidden="true">#</a> 获取一个 batch 的步骤</h4><p>假定数据集的特征和标签分别表示为张量<code>X</code>和<code>Y</code>，数据集可以表+示为 <code>(X,Y)</code>, 假定 batch 大小为 <code>m</code></p><ol><li>首先确定数据集长度： <code>n</code></li><li>从 <code>0</code> 到 <code>n-1</code> 的范围中抽样出 <code>m</code> 个数（batch 大小）<br> 假定 <code>m=4</code>, 拿到的结果是一个列表，类似：<code>indices = [1,4,8,9]</code></li><li>从数据集中去取这<code>m</code>个数对应下标的元素<br> 拿到的结果是一个元组列表，类似：<code>samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]</code></li><li>将结果整理成两个张量作为输出<br> 类似 <code>batch = (features,labels)</code><br> 其中 <code>features = torch.stack([X[1],X[4],X[8],X[9]])</code>，<code>labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])</code></li></ol><h4 id="dataset-和-dataloader-的功能分工" tabindex="-1"><a class="header-anchor" href="#dataset-和-dataloader-的功能分工" aria-hidden="true">#</a> Dataset 和 DataLoader 的功能分工</h4><p><code>Dataset</code> 是一个抽象类，用于表示数据集</p><ul><li>上述步骤 1 确定数据集的长度是由 Dataset 的 **<code>__len__</code> ** 方法实现</li><li>步骤 2 从<code>0</code>到 <code>n-1</code> 的范围中抽样出 <code>m</code> 个数的方法是由 DataLoader 的 <code>sampler</code>和 <code>batch_sampler</code>参数指定 <ul><li><code>sampler</code> 参数指定单个元素抽样方法，一般无需用户设置，程序默认在 DataLoader 的参数 <code>shuffle=True</code> 时采用随机抽样，<code>shuffle=False</code> 时采用顺序抽样</li><li><code>batch_sampler</code> 参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在 DataLoader 的参数 <code>drop_last=True</code> 时会丢弃数据集最后一个长度不能被 batch 大小整除的批次，在 <code>drop_last=False</code> 时保留最后一个批次</li></ul></li><li>步骤 3 根据下标取数据集中的元素 是由 Dataset 的 **<code>__getitem__</code> **方法实现</li><li>步骤 4 的逻辑由DataLoader的参数<code>collate_fn</code>指定。一般情况下也无需用户设置。</li></ul><p>Dataset 和 DataLoader 的一般使用方式如下</p><ul><li>TensorDataset(data, labels)</li><li>DataLoader(ds,batch_size=4,drop_last,shuffle...)</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>Dataset<span class="token punctuation">,</span>DataLoader
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> RandomSampler<span class="token punctuation">,</span>BatchSampler 

<span class="token comment"># TensorDataset(data, labels)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token comment"># 获取数据加载器中的第一个批次（features和labels）</span>
features<span class="token punctuation">,</span>labels <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features = &quot;</span><span class="token punctuation">,</span>features <span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;labels = &quot;</span><span class="token punctuation">,</span>labels <span class="token punctuation">)</span>  
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>DataLoader 内部调用方式步骤拆解如下：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># step1: 确定数据集长度 (Dataset 的 __len__ 方法实现)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;n = &quot;</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># len(ds)等价于 ds.__len__()</span>

<span class="token comment"># step2: 确定抽样 indices (DataLoader 中的 Sampler 和 BatchSampler 实现)</span>
sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>data_source <span class="token operator">=</span> ds<span class="token punctuation">)</span> <span class="token comment"># 创建随机采样器</span>
batch_sampler <span class="token operator">=</span> BatchSampler<span class="token punctuation">(</span>sampler <span class="token operator">=</span> sampler<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span> drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># 创建批次采样器</span>
<span class="token comment"># 取出第一个批次的索引 indices</span>
<span class="token keyword">for</span> idxs <span class="token keyword">in</span> batch_sampler<span class="token punctuation">:</span>
    indices <span class="token operator">=</span> idxs
    <span class="token keyword">break</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;indices = &quot;</span><span class="token punctuation">,</span>indices<span class="token punctuation">)</span>

<span class="token comment"># step3: 取出一批样本 batch (Dataset 的 __getitem__ 方法实现)</span>
batch <span class="token operator">=</span> <span class="token punctuation">[</span>ds<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span>  indices<span class="token punctuation">]</span>  <span class="token comment">#  ds[i] 等价于 ds.__getitem__(i)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;batch = &quot;</span><span class="token punctuation">,</span> batch<span class="token punctuation">)</span>

<span class="token comment"># step4: 整理成 features 和 labels (DataLoader 的 collate_fn 方法实现)</span>
<span class="token keyword">def</span> <span class="token function">collate_fn</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    features <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch<span class="token punctuation">]</span><span class="token punctuation">)</span>
    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> features<span class="token punctuation">,</span>labels 

features<span class="token punctuation">,</span>labels <span class="token operator">=</span> collate_fn<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features = &quot;</span><span class="token punctuation">,</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;labels = &quot;</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-1-2、dataset-使用" tabindex="-1"><a class="header-anchor" href="#_6-1-2、dataset-使用" aria-hidden="true">#</a> 6.1.2、Dataset 使用</h3><p>Dataset 创建数据集的方法：</p><ul><li><p>使用 torch.utils.data.<strong>TensorDataset</strong> 根据 Tensor 创建数据集（numpy 的 array，Pandas 的 DataFrame 需要先转换成 Tensor）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset
train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集</p></li><li><p>继承 torch.utils.data.Dataset 创建自定义数据集</p></li></ul><p>常用手法：</p><ul><li>torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集</li><li>调用 Dataset 的加法运算符(<code>+</code>)将多个数据集合并成一个数据集</li></ul><h4 id="根据-tensor-创建数据集" tabindex="-1"><a class="header-anchor" href="#根据-tensor-创建数据集" aria-hidden="true">#</a> 根据 Tensor 创建数据集</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>Dataset<span class="token punctuation">,</span>DataLoader<span class="token punctuation">,</span>random_split 

<span class="token comment"># 根据 Tensor 创建数据集</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets 
iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
ds_iris <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 分割成训练集和预测集</span>
n_train <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_iris<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.8</span><span class="token punctuation">)</span>
n_val <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>ds_iris<span class="token punctuation">)</span> <span class="token operator">-</span> n_train
ds_train<span class="token punctuation">,</span>ds_val <span class="token operator">=</span> random_split<span class="token punctuation">(</span>ds_iris<span class="token punctuation">,</span><span class="token punctuation">[</span>n_train<span class="token punctuation">,</span>n_val<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 使用 DataLoader 加载数据集</span>
dl_train<span class="token punctuation">,</span>dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">break</span>

<span class="token comment"># 演示加法运算符（\`+\`）的合并作用</span>
ds_data <span class="token operator">=</span> ds_train <span class="token operator">+</span> ds_val

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;len(ds_train) = &#39;</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_train<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 120</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;len(ds_valid) = &#39;</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_val<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 30</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;len(ds_train+ds_valid) = &#39;</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds_data<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 150</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>ds_data<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="根据图片目录创建图片数据集" tabindex="-1"><a class="header-anchor" href="#根据图片目录创建图片数据集" aria-hidden="true">#</a> 根据图片目录创建图片数据集</h4><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token punctuation">,</span>datasets 

<span class="token comment"># 常用的图片增强操作</span>
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image
img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&#39;./data/cat.jpeg&#39;</span><span class="token punctuation">)</span>
<span class="token comment"># 随机数值翻转</span>
transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
<span class="token comment"># 随机旋转</span>
transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
<span class="token comment"># 定义图片增强操作，用于将多个图像变换（transforms）组合成一个单一的变换</span>
transform_train <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
   transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机水平翻转</span>
   transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机垂直翻转</span>
   transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 随机在45度角度内旋转</span>
   transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 转换成张量</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
transform_valid <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># 根据图片目录创建数据集</span>
<span class="token keyword">def</span> <span class="token function">transform_label</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

ds_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span><span class="token string">&quot;./eat_pytorch_datasets/cifar2/train/&quot;</span><span class="token punctuation">,</span>transform <span class="token operator">=</span> transform_train<span class="token punctuation">,</span>target_transform<span class="token operator">=</span> transform_label<span class="token punctuation">)</span>
ds_val <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span><span class="token string">&quot;./eat_pytorch_datasets/cifar2/test/&quot;</span><span class="token punctuation">,</span>transform <span class="token operator">=</span> transform_valid<span class="token punctuation">,</span>target_transform<span class="token operator">=</span> transform_label<span class="token punctuation">)</span>


<span class="token keyword">print</span><span class="token punctuation">(</span>ds_train<span class="token punctuation">.</span>class_to_idx<span class="token punctuation">)</span> <span class="token comment"># {&#39;0_airplane&#39;: 0, &#39;1_automobile&#39;: 1}</span>

<span class="token comment"># 使用 DataLoader 加载数据集</span>
dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 3, 32, 32])</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 1])</span>
    <span class="token keyword">break</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="创建自定义数据集" tabindex="-1"><a class="header-anchor" href="#创建自定义数据集" aria-hidden="true">#</a> 创建自定义数据集</h4><p>通过继承 torch.utils.data.Dataset 创建自定义数据集的方式来对 cifar2 构建数据管道</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path 
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image 

<span class="token keyword">class</span> <span class="token class-name">Cifar2Dataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>imgs_dir<span class="token punctuation">,</span>img_transform<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>files <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>Path<span class="token punctuation">(</span>imgs_dir<span class="token punctuation">)</span><span class="token punctuation">.</span>rglob<span class="token punctuation">(</span><span class="token string">&quot;*.jpg&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> img_transform
        
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>
        file_i <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
        img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>file_i<span class="token punctuation">)</span>
        tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span>  <span class="token string">&quot;1_automobile&quot;</span> <span class="token keyword">in</span> file_i <span class="token keyword">else</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tensor<span class="token punctuation">,</span>label 
    
    
train_dir <span class="token operator">=</span> <span class="token string">&quot;./eat_pytorch_datasets/cifar2/train/&quot;</span>
test_dir <span class="token operator">=</span> <span class="token string">&quot;./eat_pytorch_datasets/cifar2/test/&quot;</span>

<span class="token comment"># 定义图片增强</span>
transform_train <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
   transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机水平翻转</span>
   transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 随机垂直翻转</span>
   transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 随机在45度角度内旋转</span>
   transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 转换成张量</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

transform_val <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">)</span>

ds_train <span class="token operator">=</span> Cifar2Dataset<span class="token punctuation">(</span>train_dir<span class="token punctuation">,</span>transform_train<span class="token punctuation">)</span>
ds_val <span class="token operator">=</span> Cifar2Dataset<span class="token punctuation">(</span>test_dir<span class="token punctuation">,</span>transform_val<span class="token punctuation">)</span>

dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 3, 32, 32])</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([50, 1])</span>
    <span class="token keyword">break</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-1-3、使用-dataloader-加载数据集" tabindex="-1"><a class="header-anchor" href="#_6-1-3、使用-dataloader-加载数据集" aria-hidden="true">#</a> 6.1.3、使用 DataLoader 加载数据集</h3><p>DataLoader 能够控制 batch 的大小，batch 中元素的采样方法，以及将 batch 结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据</p><p>DataLoader 的函数签名如下</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>DataLoader<span class="token punctuation">(</span>
    dataset<span class="token punctuation">,</span> <span class="token comment"># 传入 Dataset</span>
    batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 取 Dataset 时，一次取多大</span>
    shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    sampler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    batch_sampler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    collate_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    timeout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    worker_init_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
    multiprocessing_context<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers,pin_memory, drop_last 这六个参数，</p><p>有时候对于一些复杂结构的数据集，还需要自定义 collate_fn 函数，其他参数一般使用默认值即可。</p><p>DataLoader 除了可以加载我们前面讲的 torch.utils.data.Dataset 外，还能够加载另外一种数据集 torch.utils.data.IterableDataset。</p><p>和 Dataset 数据集相当于一种列表结构不同，IterableDataset 相当于一种迭代器结构。 它更加复杂，一般较少使用。</p><ul><li>dataset : 数据集</li><li>batch_size: 批次大小</li><li>shuffle: 是否乱序</li><li>sampler: 样本采样函数，一般无需设置。</li><li>batch_sampler: 批次采样函数，一般无需设置。</li><li>num_workers: 使用多进程读取数据，设置的进程数。</li><li>collate_fn: 整理一个批次数据的函数。</li><li>pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。</li><li>drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。</li><li>timeout: 加载一个数据批次的最长等待时间，一般无需设置。</li><li>worker_init_fn: 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 构建输入数据管道</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span>
                batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span>
                shuffle<span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                drop_last <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 迭代数据</span>
<span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token keyword">in</span> dl<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
    
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([45, 49, 27,  7, 32, 48, 19, 38, 35, 30])
tensor([44, 37, 21, 39, 29, 13,  8, 31, 33,  5])
tensor([34, 28,  2, 23, 15, 42, 43, 40, 22,  6])
tensor([36,  3, 46,  9, 26, 16, 12, 17, 18,  1])
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_6-2、模型层-torch-nn" tabindex="-1"><a class="header-anchor" href="#_6-2、模型层-torch-nn" aria-hidden="true">#</a> 6.2、模型层 torch.nn</h2><p>深度学习模型由各种模型层组合</p><p><strong>torch.nn</strong> 中内置了非常丰富的各种模型层。它们都属于 nn.Module 的子类，具备参数管理功能</p><p>也可以通过继承 nn.Module 基类构建自定义的模型层</p><p>pytorch 不区分模型和模型层，都是通过继承 nn.Module 进行构建</p><p>只要继承 nn.Module 基类并实现 forward 方法即可自定义模型层</p><blockquote><p>torch.nn.function 中有很多功能，与 nn.Module 一样。<br> 一般情况下，如果模型有可学习的参数（w，b），最好用 nn.Module，其他情况（激活函数，loss function） nn.function 相对更简单些</p></blockquote><h3 id="_6-2-1、基础层" tabindex="-1"><a class="header-anchor" href="#_6-2-1、基础层" aria-hidden="true">#</a> 6.2.1、基础层</h3><ul><li><p>nn.Linear：全连接层。参数个数 = 输入层特征数 × 输出层特征数(weight)＋ 输出层特征数(bias)</p></li><li><p>nn.Embedding：嵌入层。一种比 Onehot 更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</p></li><li><p>nn.Flatten：压平层，用于将多维张量样本压成一维张量样本。</p></li><li><p>nn.BatchNorm1d：一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。可以用 afine 参数设置该层是否含有可以训练的参数。</p></li><li><p>nn.BatchNorm2d：二维批标准化层。 常用于 CV 领域。</p></li><li><p>nn.BatchNorm3d：三维批标准化层。</p></li><li><p>nn.Dropout：一维随机丢弃层。一种正则化手段。</p></li><li><p>nn.Dropout2d：二维随机丢弃层。</p></li><li><p>nn.Dropout3d：三维随机丢弃层。</p></li><li><p>nn.Threshold：限幅层。当输入大于或小于阈值范围时，截断之。</p></li><li><p>nn.ConstantPad2d： 二维常数填充层。对二维张量样本填充常数扩展长度。</p></li><li><p>nn.ReplicationPad1d： 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。</p></li><li><p>nn.ZeroPad2d：二维零值填充层。对二维张量样本在边缘填充0值.</p></li><li><p>nn.GroupNorm：组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。不受 batch 大小限制。</p></li><li><p>nn.LayerNorm：层归一化。常用于 NLP 领域，不受序列长度不一致影响。</p></li><li><p>nn.InstanceNorm2d: 样本归一化。一般在图像风格迁移任务中效果较好。</p></li></ul><p>各种归一化层：</p><ul><li>结构化数据 BatchNorm1D 归一化</li><li>图片数据的各种归一化（一般常用BatchNorm2D）</li><li>文本数据的 LayerNorm 归一化</li><li>可自适应学习的归一化 SwitchableNorm</li></ul>`,48),r={href:"https://arxiv.org/pdf/1806.10779.pdf",target:"_blank",rel:"noopener noreferrer"},k=t(`<p>对 BatchNorm 需要注意的几点：</p><ul><li>原始论文认为将 BatchNorm 放在激活函数前效果较好，后面的研究一般认为将 BatchNorm 放在激活函数之后更好</li><li>BatchNorm在训练过程和推理过程的逻辑不一样，训练过程 BatchNorm 的均值和方差和根据 mini-batch 中的数据估计的，而推理过程中 BatchNorm 的均值和方差是用的训练过程中的全体样本估计的。因此预测过程是稳定的，相同的样本不会因为所在批次的差异得到不同的结果，但训练过程中则会受到批次中其他样本的影响所以有正则化效果</li><li>如果受到 GPU 内存限制，不得不使用很小的 batch_size，训练阶段时使用的 mini-batch 上的均值和方差的估计和预测阶段时使用的全体样本上的均值和方差的估计差异可能会较大，效果会变差。这时候，可以尝试 LayerNorm 或者 GroupNorm 等归一化方法</li></ul><p>BatchNorm 使用：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
batch_size<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> height<span class="token punctuation">,</span> width <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token operator">*</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">128</span><span class="token operator">*</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 
<span class="token comment"># 创建了 2D 批量归一化层</span>
bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token operator">=</span>channel<span class="token punctuation">,</span>affine<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
bn_out <span class="token operator">=</span> bn<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
channel_mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>bn_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 提取张量中第一个通道的所有像素，计算该通道像素的均值和标准差</span>
channel_std <span class="token operator">=</span> torch<span class="token punctuation">.</span>std<span class="token punctuation">(</span>bn_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;channel mean:&quot;</span><span class="token punctuation">,</span>channel_mean<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 1.043081283569336e-07</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;channel std:&quot;</span><span class="token punctuation">,</span>channel_std<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 1.0000009536743164</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-2、卷积网络相关层" tabindex="-1"><a class="header-anchor" href="#_6-2-2、卷积网络相关层" aria-hidden="true">#</a> 6.2.2、卷积网络相关层</h3><ul><li>nn.Conv1d：普通一维卷积，常用于文本。参数个数 = 输入通道数 × 卷积核尺寸(如3)×卷积核个数 + 卷积核尺寸(如3）</li><li>nn.Conv2d：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数 + 卷积核尺寸(如3乘3)。) 通过调整 dilation 参数大于 1，可以变成空洞卷积，增加感受野。 通过调整 groups 参数不为1，可以变成分组卷积。分组卷积中每个卷积核仅对其对应的一个分组进行操作。 当 groups 参数数量等于输入通道数时，相当于 tensorflow 中的二维深度卷积层tf.keras.layers.DepthwiseConv2D。 利用分组卷积和1乘1卷积的组合操作，可以构造相当于 Keras 中的二维深度可分离卷积层tf.keras.layers.SeparableConv2D。</li><li>nn.Conv3d：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数 + 卷积核尺寸(如3乘3乘3) 。</li><li>nn.MaxPool1d: 一维最大池化。</li><li>nn.MaxPool2d：二维最大池化。一种下采样方式。没有需要训练的参数。</li><li>nn.MaxPool3d：三维最大池化。</li><li>nn.AdaptiveMaxPool2d：二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的尺寸来反向推算池化算子的padding,stride等参数。</li><li>nn.FractionalMaxPool2d：二维分数最大池化。普通最大池化通常输入尺寸是输出的整数倍。而分数最大池化则可以不必是整数。分数最大池化使用了一些随机采样策略，有一定的正则效果，可以用它来代替普通最大池化和 Dropout 层。</li><li>nn.AvgPool2d：二维平均池化。</li><li>nn.AdaptiveAvgPool2d：二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。</li><li>nn.ConvTranspose2d：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。</li><li>nn.Upsample：上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略为&quot;nearest&quot;最邻近策略或&quot;linear&quot;线性插值策略。</li><li>nn.Unfold：滑动窗口提取层。其参数和卷积操作nn.Conv2d相同。实际上，卷积操作可以等价于nn.Unfold和nn.Linear以及nn.Fold的一个组合。 其中nn.Unfold操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。利用nn.Linear将nn.Unfold的输出和卷积核做乘法后，再使用 nn.Fold操作将结果转换成输出图片形状。</li><li>nn.Fold：逆滑动窗口提取层。</li></ul><p>各种常用的卷积层和上采样层：</p><ul><li><p>普通卷积</p><p>![image-20240723134724344](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\人工智能\\Pytorch\\assets\\image-20240723134724344.png)</p></li><li><p>空洞卷积</p><p>![image-20240723134732374](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\人工智能\\Pytorch\\assets\\image-20240723134732374.png)</p></li><li><p>分组卷积</p></li><li><p>深度可分离卷积</p></li><li><p>转置卷积</p></li><li><p>上采样层</p></li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

<span class="token comment"># 卷积输出尺寸计算公式 o = (i + 2*p -k&#39;)//s  + 1 </span>
<span class="token comment"># 对空洞卷积 k&#39; = d(k-1) + 1</span>
<span class="token comment"># o 是输出尺寸，i 是输入尺寸，p 是 padding 大小， k 是卷积核尺寸， s是 stride 步长, d是 dilation 空洞参数</span>

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">25</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># i = 5</span>
filters <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># k = 2</span>

outputs <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> filters<span class="token punctuation">)</span> <span class="token comment"># o = (5+2*0-2)//1+1 = 4</span>
outputs_s2 <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment">#o = (5+2*0-2)//2+1 = 2</span>
outputs_p1 <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> filters<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#o = (5+2*1-2)//1+1 = 6</span>
outputs_d2 <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>filters<span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment">#o = (5+2*0-(2(2-1)+1))//1+1 = 3</span>


<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

features <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features.shape:&quot;</span><span class="token punctuation">,</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 普通卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--conv--&quot;</span><span class="token punctuation">)</span>
conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
conv_out <span class="token operator">=</span> conv<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv_out.shape:&quot;</span><span class="token punctuation">,</span>conv_out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv.weight.shape:&quot;</span><span class="token punctuation">,</span>conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 分组卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--group conv--&quot;</span><span class="token punctuation">)</span>
conv_group <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>
group_out <span class="token operator">=</span> conv_group<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;group_out.shape:&quot;</span><span class="token punctuation">,</span>group_out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv_group.weight.shape:&quot;</span><span class="token punctuation">,</span>conv_group<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 深度可分离卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--separable conv--&quot;</span><span class="token punctuation">)</span>
depth_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>
oneone_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
separable_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>depth_conv<span class="token punctuation">,</span>oneone_conv<span class="token punctuation">)</span>
separable_out <span class="token operator">=</span> separable_conv<span class="token punctuation">(</span>features<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;separable_out.shape:&quot;</span><span class="token punctuation">,</span>separable_out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;depth_conv.weight.shape:&quot;</span><span class="token punctuation">,</span>depth_conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;oneone_conv.weight.shape:&quot;</span><span class="token punctuation">,</span>oneone_conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 转置卷积</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--conv transpose--&quot;</span><span class="token punctuation">)</span>
conv_t <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
features_like <span class="token operator">=</span> conv_t<span class="token punctuation">(</span>conv_out<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;features_like.shape:&quot;</span><span class="token punctuation">,</span>features_like<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;conv_t.weight.shape:&quot;</span><span class="token punctuation">,</span>conv_t<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;inputs:&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

nearest <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">&#39;nearest&#39;</span><span class="token punctuation">)</span>
bilinear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>mode<span class="token operator">=</span><span class="token string">&quot;bilinear&quot;</span><span class="token punctuation">,</span>align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;nearest(inputs)：&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>nearest<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;bilinear(inputs)：&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>bilinear<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-3、循环网络相关层" tabindex="-1"><a class="header-anchor" href="#_6-2-3、循环网络相关层" aria-hidden="true">#</a> 6.2.3、循环网络相关层</h3><ul><li>nn.LSTM：长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置 bidirectional = True 时可以得到双向 LSTM。需要注意的时，默认的输入和输出形状是(seq,batch,feature), 如果需要将 batch 维度放在第 0 维，则要设置 batch_first 参数设置为 True</li><li>nn.GRU：门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快</li><li>nn.RNN：简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用</li><li>nn.LSTMCell：长短记忆循环网络单元。和 nn.LSTM 在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用</li><li>nn.GRUCell：门控循环网络单元。和 nn.GRU 在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用</li><li>nn.RNNCell：简单循环网络单元。和 nn.RNN 在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用</li></ul>`,11),d=n("p",null,[s("各种 RNN 序列模型层(RNN,GRU,LSTM 等)可以用函数表示如下: "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("msub",null,[n("mi",null,"h"),n("mi",null,"t")]),n("mo",null,"="),n("mi",null,"f"),n("mo",{stretchy:"false"},"("),n("msub",null,[n("mi",null,"h"),n("mrow",null,[n("mi",null,"t"),n("mo",null,"−"),n("mn",null,"1")])]),n("mo",{separator:"true"},","),n("msub",null,[n("mi",null,"x"),n("mi",null,"t")]),n("mo",{stretchy:"false"},")")]),n("annotation",{encoding:"application/x-tex"},"h_t = f(h_{t-1},x_t)")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.8444em","vertical-align":"-0.15em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"h"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2806em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mathnormal mtight"},"t")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),n("span",{class:"mopen"},"("),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"h"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3011em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"t"),n("span",{class:"mbin mtight"},"−"),n("span",{class:"mord mtight"},"1")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2083em"}},[n("span")])])])])]),n("span",{class:"mpunct"},","),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"x"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2806em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mathnormal mtight"},"t")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mclose"},")")])])])],-1),m=n("p",null,[s("公式的含义：t 时刻循环神经网络的输出向量 ℎ𝑡 由 t-1 时刻的输出向量 ℎ"),n("sub",null,"𝑡−1"),s(" 和 t 时刻的输入 𝑖"),n("sub",null,"𝑡"),s(" 变换而来")],-1),v={href:"https://zhuanlan.zhihu.com/p/32085405%EF%BC%89",target:"_blank",rel:"noopener noreferrer"},b=n("p",null,"LSTM 通过引入了三个门来控制信息的传递，分别是遗忘门，输入门 和输出门 。三个门的作用为：",-1),h=n("ol",null,[n("li",null,[s("遗忘门: 遗忘门 𝑓"),n("sub",null,"𝑡"),s(" 控制上一时刻的内部状态 需要遗忘多少信息；")]),n("li",null,[s("输入门: 输入门 𝑖"),n("sub",null,"𝑡"),s(" 控制当前时刻的候选状态 有多少信息需要保存；")]),n("li",null,[s("输出门: 输出门 𝑜"),n("sub",null,"𝑡"),s(" 控制当前时刻的内部状态 有多少信息需要输出给外部状态 ；")])],-1),_={href:"https://zhuanlan.zhihu.com/p/32481747%EF%BC%89",target:"_blank",rel:"noopener noreferrer"},y=t("<p>GRU 的结构比 LSTM 更为简单一些，GRU 只有两个门，更新门和重置门</p><ol><li>更新门：更新门用于控制每一步ℎ𝑡ℎ𝑡被更新的比例，更新门越大，ℎ<sub>𝑡</sub> 更新幅度越大。</li><li>重置门：重置门用于控制更新候选向量 ℎ̃<sub>𝑡</sub> 中前一步的状态 ℎ<sub>𝑡−1</sub> 被重新放入的比例，重置门越大，更新候选向量中 ℎ<sub>𝑡−1</sub> 被重新放进来的比例越大</li></ol><p>公式中的小圈表示哈达玛积，也就是两个向量逐位相乘</p><p>其中 1式和 2式计算的是更新门 𝑢<sub>𝑡</sub> 和重置门 𝑟<sub>𝑡</sub>，是两个长度和 ℎ<sub>𝑡</sub> 相同的向量。</p><p>注意到 4式实际上和 ResNet 的残差结构是相似的，都是 f(x) = x + g(x) 的形式，可以有效地防止长序列学习反向传播过程中梯度消失问题。</p>",5),f=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span> <span class="token comment">#batch_size, seq_length, features</span>

gru <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
gru_output<span class="token punctuation">,</span>gru_hn <span class="token operator">=</span> gru<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--GRU--&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;gru_output.shape:&quot;</span><span class="token punctuation">,</span>gru_output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;gru_hn.shape:&quot;</span><span class="token punctuation">,</span>gru_hn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;--LSTM--&quot;</span><span class="token punctuation">)</span>
lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
lstm_output<span class="token punctuation">,</span><span class="token punctuation">(</span>lstm_hn<span class="token punctuation">,</span>lstm_cn<span class="token punctuation">)</span> <span class="token operator">=</span> lstm<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;lstm_output.shape:&quot;</span><span class="token punctuation">,</span>lstm_output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;lstm_hn.shape:&quot;</span><span class="token punctuation">,</span>lstm_hn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;lstm_cn.shape:&quot;</span><span class="token punctuation">,</span>lstm_cn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary
summary<span class="token punctuation">(</span>gru<span class="token punctuation">,</span>input_data<span class="token operator">=</span>inputs<span class="token punctuation">)</span><span class="token punctuation">;</span>
summary<span class="token punctuation">(</span>lstm<span class="token punctuation">,</span>input_data<span class="token operator">=</span>inputs<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-4、transformer-相关层" tabindex="-1"><a class="header-anchor" href="#_6-2-4、transformer-相关层" aria-hidden="true">#</a> 6.2.4、Transformer 相关层</h3><ul><li>nn.Transformer：Transformer网络结构。Transformer网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。它是目前NLP任务的主流模型的主要构成部分。</li><li>nn.TransformerEncoder：Transformer编码器结构。由多个 nn.TransformerEncoderLayer编码器层组成。</li><li>nn.TransformerDecoder：Transformer解码器结构。由多个 nn.TransformerDecoderLayer解码器层组成。</li><li>nn.TransformerEncoderLayer：Transformer的编码器层。主要由Multi-Head self-Attention, Feed-Forward前馈网络, LayerNorm归一化层, 以及残差连接层组成。</li><li>nn.TransformerDecoderLayer：Transformer的解码器层。主要由Masked Multi-Head self-Attention, Multi-Head cross-Attention, Feed-Forward前馈网络, LayerNorm归一化层, 以及残差连接层组成。</li><li>nn.MultiheadAttention：多头注意力层。用于在序列方向上融合特征。使用的是Scaled Dot Production Attention，并引入了多个注意力头。</li></ul>`,3),g=n("br",null,null,-1),w={href:"https://zhuanlan.zhihu.com/p/48508221",target:"_blank",rel:"noopener noreferrer"},L=n("br",null,null,-1),q={href:"http://nlp.seas.harvard.edu/annotated-transformer/",target:"_blank",rel:"noopener noreferrer"},x=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 

<span class="token comment"># 验证 MultiheadAttention 和 head 数量无关</span>
inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span> <span class="token comment"># batch_size, seq_length, features</span>
attention_h8 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>
    embed_dim <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span>
    num_heads <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>
    bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    batch_first<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
attention_h16 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MultiheadAttention<span class="token punctuation">(</span>
    embed_dim <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span>
    num_heads <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>
    bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    batch_first<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
out_h8 <span class="token operator">=</span> attention_h8<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span>
out_h16 <span class="token operator">=</span> attention_h16<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span>

<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary 
summary<span class="token punctuation">(</span>attention_h8<span class="token punctuation">,</span>input_data_args<span class="token operator">=</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
summary<span class="token punctuation">(</span>attention_h16<span class="token punctuation">,</span>input_data_args<span class="token operator">=</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">from</span> copy <span class="token keyword">import</span> deepcopy

<span class="token comment"># 多头注意力的一种简洁实现</span>

<span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Compute &#39;Scaled Dot Product Attention&#39;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        d_k <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        scores <span class="token operator">=</span> query@key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_k<span class="token operator">**</span><span class="token number">0.5</span>     
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e20</span><span class="token punctuation">)</span>
        p_attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            p_attn <span class="token operator">=</span> dropout<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>
        <span class="token keyword">return</span> p_attn@value<span class="token punctuation">,</span> p_attn
    
<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> h<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Take in model size and number of heads.&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> d_model <span class="token operator">%</span> h <span class="token operator">==</span> <span class="token number">0</span>
        <span class="token comment"># We assume d_v always equals d_k</span>
        self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> d_model <span class="token operator">//</span> h
        self<span class="token punctuation">.</span>h <span class="token operator">=</span> h
        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>deepcopy<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Implements Figure 2&quot;</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># Same mask applied to all h heads.</span>
            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        nbatches <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span>
        query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value <span class="token operator">=</span> \\
            <span class="token punctuation">[</span>l<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
             <span class="token keyword">for</span> l<span class="token punctuation">,</span> x <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>linears<span class="token punctuation">,</span> <span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 2) Apply attention on all the projected vectors in batch. </span>
        x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">,</span> 
                                 dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
        
        <span class="token comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> \\
             <span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h <span class="token operator">*</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-5、自定义模型层" tabindex="-1"><a class="header-anchor" href="#_6-2-5、自定义模型层" aria-hidden="true">#</a> 6.2.5、自定义模型层</h3><p>如果 Pytorch 的内置模型层不能够满足需求，我们也可以通过继承 nn.Module 基类构建自定义的模型层</p><p>实际上，pytorch 不区分模型和模型层，都是通过继承 nn.Module 进行构建。</p><p>因此，我们只要继承 nn.Module 基类并实现 forward 方法即可自定义模型层。</p><p>下面是 Pytorch 的 nn.Linear 层的源码，我们可以仿照它来自定义模型层。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    __constants__ <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;in_features&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;out_features&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> bias<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">&#39;bias&#39;</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> a<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            fan_in<span class="token punctuation">,</span> _ <span class="token operator">=</span> nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>_calculate_fan_in_and_fan_out<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
            bound <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>fan_in<span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token operator">-</span>bound<span class="token punctuation">,</span> bound<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">extra_repr</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string">&#39;in_features={}, out_features={}, bias={}&#39;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>in_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>out_features<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_6-3、损失函数-losses" tabindex="-1"><a class="header-anchor" href="#_6-3、损失函数-losses" aria-hidden="true">#</a> 6.3、损失函数 losses</h2><p>一般来说，监督学习的目标函数由损失函数和正则化项组成。(Objective = Loss + Regularization)</p><p>Pytorch 中的损失函数一般在训练模型时候指定</p><blockquote><p>Pytorch 中内置的损失函数的参数和 tensorflow 不同，是 y_pred 在前，y_true 在后，而 Tensorflow 是 y_true 在前，y_pred 在后</p></blockquote><ul><li><p>对于 <strong>回归模型</strong>，通常使用的内置损失函数是 <strong>均方损失函数 nn.MSELoss</strong></p></li><li><p>对于 <strong>二分类模型</strong>，通常使用的是 <strong>二元交叉熵损失函数nn.BCELoss</strong> (输入已经是 sigmoid 激活函数之后的结果) 或者 nn.BCEWithLogitsLoss (输入尚未经过 nn.Sigmoid 激活函数)</p></li><li><p>对于 <strong>多分类模型</strong>，一般推荐使用 <strong>交叉熵损失函数 nn.CrossEntropyLoss</strong> (y_true 需要是一维的，是类别编码。y_pred 未经过 nn.Softmax 激活)</p><p>此外，如果多分类的 y_pred 经过了 nn.LogSoftmax 激活，可以使用 nn.NLLLoss 损失函数(The negative log likelihood loss)。 这种方法和直接使用 nn.CrossEntropyLoss 等价</p></li></ul><p>如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量 y_pred，y_true 作为输入参数，并输出一个标量作为损失函数值</p><p>Pytorch 中的正则化项一般通过自定义的方式和损失函数一起添加作为目标函数</p><p>如果仅仅使用 L2 正则化，也可以利用优化器的 weight_decay 参数来实现相同的效果</p><h3 id="_6-3-1、内置损失函数" tabindex="-1"><a class="header-anchor" href="#_6-3-1、内置损失函数" aria-hidden="true">#</a> 6.3.1、内置损失函数</h3><p>内置的损失函数一般有类的实现和函数的实现两种形式</p><p>类的实现形式通常是调用函数的实现形式并用 nn.Module 封装后得到的</p><p>我们常用的是类的实现形式。它们封装在 torch.nn 模块下，并且类名以 Loss 结尾</p><p>常用的一些内置损失函数说明如下：</p><ul><li>nn.MSELoss（均方误差损失，也叫做 L2 损失，用于回归）</li><li>nn.L1Loss （L1 损失，也叫做绝对值误差损失，用于回归）</li><li>nn.SmoothL1Loss (平滑 L1 损失，当输入在 -1 到 1 之间时，平滑为 L2 损失，用于回归)</li><li>nn.BCELoss (二元交叉熵，用于二分类，输入已经过 nn.Sigmoid 激活，对不平衡数据集可以用 weigths 参数调整类别权重)</li><li>nn.BCEWithLogitsLoss (二元交叉熵，用于二分类，输入未经过 nn.Sigmoid 激活)</li><li>nn.CrossEntropyLoss (交叉熵，用于多分类，要求label为稀疏编码，输入未经过 nn.Softmax 激活，对不平衡数据集可以用 weigths 参数调整类别权重)</li><li>nn.NLLLoss (负对数似然损失，用于多分类，要求 label 为稀疏编码，输入经过 nn.LogSoftmax 激活)</li><li>nn.KLDivLoss (KL 散度损失，也叫相对熵，等于交叉熵减去信息熵，用于标签为概率值的多分类，要求输入经过 nn.LogSoftmax 激活)</li><li>nn.CosineSimilarity(余弦相似度，可用于多分类)</li><li>nn.AdaptiveLogSoftmaxWithLoss (一种适合非常多类别且类别分布很不均衡的损失函数，会自适应地将多个小类别合成一个 cluster)</li></ul><p>二元交叉熵、多元交叉熵、对数损失 LogLoss、负对数似然损失 NLLLoss、KL 散度之间的区别和联系：略</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

<span class="token comment"># nn.BCELoss() 和 nn.BCEWithLogitsLoss() 关系</span>
y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

bce <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span><span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>bce<span class="token punctuation">)</span>


bce_logits <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>bce_logits<span class="token punctuation">)</span>

<span class="token comment"># nn.CrossEntropyLoss() 和  nn.NLLLoss() 关系</span>
y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 直接调用交叉熵损失</span>
ce <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ce<span class="token punctuation">)</span>

<span class="token comment"># 等价于先计算 nn.LogSoftmax 激活，再调用 nn.NLLLoss</span>
y_pred_logsoftmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span>
nll <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred_logsoftmax<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>nll<span class="token punctuation">)</span>

<span class="token comment"># nn.CrossEntropyLoss() 和  KLDivLoss 关系</span>
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y_true <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

ce <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">&quot;mean&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ce<span class="token punctuation">)</span>


<span class="token comment">#KLDivLoss要求target为向量形式编码且preds经过LogSoftmax激活</span>
pred <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
target <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>y_true<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
kl <span class="token operator">=</span> nn<span class="token punctuation">.</span>KLDivLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">&quot;batchmean&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>pred<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>kl<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-2、自定义损失函数" tabindex="-1"><a class="header-anchor" href="#_6-3-2、自定义损失函数" aria-hidden="true">#</a> 6.3.2、自定义损失函数</h3><p>自定义损失函数接收两个张量 y_pred , y_true 作为输入参数，并输出一个标量作为损失函数值</p><p>也可以对 nn.Module 进行子类化，重写 forward 方法实现损失的计算逻辑，从而得到损失函数的类的实现</p><h4 id="自定义损失函数-focalloss-范例" tabindex="-1"><a class="header-anchor" href="#自定义损失函数-focalloss-范例" aria-hidden="true">#</a> 自定义损失函数 FocalLoss 范例</h4><p>Focal Loss是一种对 binary_crossentropy 的改进损失函数形式</p><p>它在样本不均衡和存在较多易分类的样本时相比 binary_crossentropy 具有明显的优势</p><p>它有两个可调参数，alpha 参数和 gamma 参数。其中 alpha 参数主要用于衰减负样本的权重，gamma 参数主要用于衰减容易训练样本的权重。从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做 Focal Loss。</p><p>![image-20240724160609623](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\人工智能\\Pytorch\\assets\\image-20240724160609623.png)</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">class</span> <span class="token class-name">FocalLoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> gamma
        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> alpha

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span><span class="token punctuation">:</span>
        bce <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>reduction <span class="token operator">=</span> <span class="token string">&quot;none&quot;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span>
        p_t <span class="token operator">=</span> <span class="token punctuation">(</span>y_true <span class="token operator">*</span> y_pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_true<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        alpha_factor <span class="token operator">=</span> y_true <span class="token operator">*</span> self<span class="token punctuation">.</span>alpha <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_true<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>alpha<span class="token punctuation">)</span>
        modulating_factor <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> p_t<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gamma<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>alpha_factor <span class="token operator">*</span> modulating_factor <span class="token operator">*</span> bce<span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss
    
    
<span class="token comment"># 困难样本</span>
y_pred_hard <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true_hard <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 容易样本</span>
y_pred_easy <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y_true_easy <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

focal_loss <span class="token operator">=</span> FocalLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
bce_loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;focal_loss(easy samples):&quot;</span><span class="token punctuation">,</span> focal_loss<span class="token punctuation">(</span>y_pred_easy<span class="token punctuation">,</span>y_true_easy<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.0005)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;bce_loss(easy samples):&quot;</span><span class="token punctuation">,</span> bce_loss<span class="token punctuation">(</span>y_pred_easy<span class="token punctuation">,</span>y_true_easy<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.1054)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;focal_loss(hard samples):&quot;</span><span class="token punctuation">,</span> focal_loss<span class="token punctuation">(</span>y_pred_hard<span class="token punctuation">,</span>y_true_hard<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.0866)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;bce_loss(hard samples):&quot;</span><span class="token punctuation">,</span> bce_loss<span class="token punctuation">(</span>y_pred_hard<span class="token punctuation">,</span>y_true_hard<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.6931)</span>


<span class="token comment">#可见 focal_loss 让容易样本的权重衰减到原来的 0.0005/0.1054 = 0.00474</span>
<span class="token comment">#而让困难样本的权重只衰减到原来的 0.0866/0.6931=0.12496</span>

<span class="token comment"># 因此相对而言，focal_loss可以衰减容易样本的权重。</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="sceloss-案例" tabindex="-1"><a class="header-anchor" href="#sceloss-案例" aria-hidden="true">#</a> SCELoss 案例</h4><p>Symmetric Cross Entropy Loss 也是一种对交叉熵损失的改进损失，主要用在标签中存在明显噪声的场景。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">ce</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span>
    p <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>p<span class="token punctuation">,</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">1e-4</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>y<span class="token punctuation">,</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span><span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1</span><span class="token operator">-</span><span class="token number">1e-4</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>y<span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>y<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>p<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">rce</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ce<span class="token punctuation">(</span>p<span class="token punctuation">,</span>y<span class="token punctuation">)</span>

<span class="token comment"># 正常标签</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.8</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token operator">/</span>ce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(8.2502)</span>


<span class="token comment"># 噪声标签</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.8</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token operator">/</span>ce<span class="token punctuation">(</span>y<span class="token punctuation">,</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(4.5786)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span>  torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 

<span class="token keyword">class</span> <span class="token class-name">SCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> a<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SCELoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes
        self<span class="token punctuation">.</span>a <span class="token operator">=</span> a <span class="token comment">#两个超参数</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b
        self<span class="token punctuation">.</span>cross_entropy <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># CE 部分，正常的交叉熵损失</span>
        ce <span class="token operator">=</span> self<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
        <span class="token comment"># RCE</span>
        pred <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
        label_one_hot <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_classes<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        label_one_hot <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>label_one_hot<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span> <span class="token comment">#最小设为 1e-4，即 A 取 -4</span>
        rce <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>pred <span class="token operator">*</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>label_one_hot<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>a <span class="token operator">*</span> ce <span class="token operator">+</span> self<span class="token punctuation">.</span>b <span class="token operator">*</span> rce<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss
    
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-3、l1-和-l2正则化项" tabindex="-1"><a class="header-anchor" href="#_6-3-3、l1-和-l2正则化项" aria-hidden="true">#</a> 6.3.3、L1 和 L2正则化项</h3><p>L1 正则、L2 正则、Dropout、Early_stopping 是神经网络常用的正则化方法（正则化：防止模型在训练数据上过拟合）</p><p>通常认为 L1 正则化可以产生稀疏权值矩阵，即产生一个参数稀疏的模型。而 L2 正则化可以让模型的参数取绝对值较小的数</p><p>考虑两种正则化函数的等值面与原始 Loss 函数的等值面的关系。</p><ul><li><p>以二维情况为例，L1 正则化函数的等值面是个菱形，L2 正则化函数的等值面是个圆形。最优参数必定取在正则化函数的某条等值面和原始Loss函数的某条等值面的切点处。</p></li><li><p>从求导角度考虑，最优参数是个极值点，要求该处 正则化函数的梯度等于 原始Loss函数的梯度的负数。</p><p>而梯度方向必定垂直于等值面的切线方向，所以可以推断必定极值点必定在正则化函数某条等值面和原始Loss函数的某条等值面的切点处。</p></li><li><p>从数值角度考虑，如果该极值点不在两个等值面的切点，那么沿着原始函数Loss的等值面(原始Loss不变)，一定可以找到一个点正则化函数取值更小。</p><p>这样就用反证法证明了最优参数必定取在正则化函数的某条等值面和原始Loss函数的某条等值面的切点处。</p></li></ul><p>由于 L1 正则化函数的等值面是个菱形，更容易和凸的 Loss 函数的等值面相切在坐标轴上，所以倾向于取得参数稀疏的模型，而 L2 正则化则更倾向于使得极小点到坐标原点的距离更近，但不会导致参数稀疏。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token comment"># L2 正则化</span>
<span class="token keyword">def</span> <span class="token function">L2Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>alpha<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l2_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span> <span class="token comment"># 一般不对偏置项使用正则</span>
            l2_loss <span class="token operator">=</span> l2_loss <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> alpha <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l2_loss

<span class="token comment"># L1 正则化</span>
<span class="token keyword">def</span> <span class="token function">L1Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>beta<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l1_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
            l1_loss <span class="token operator">=</span> l1_loss <span class="token operator">+</span>  beta <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l1_loss
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-4、l1l2-正则项使用完整范例" tabindex="-1"><a class="header-anchor" href="#_6-3-4、l1l2-正则项使用完整范例" aria-hidden="true">#</a> 6.3.4、L1L2 正则项使用完整范例</h3><p>以一个二分类问题为例，演示给模型的目标函数添加自定义L1和L2正则化项的方法。</p><p>这个范例同时演示了以下 FocalLoss 的使用。</p><ol><li><p>准备数据</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd 
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span>DataLoader<span class="token punctuation">,</span>TensorDataset
<span class="token keyword">import</span> torchkeras 
<span class="token operator">%</span>matplotlib inline
<span class="token operator">%</span>config InlineBackend<span class="token punctuation">.</span>figure_format <span class="token operator">=</span> <span class="token string">&#39;svg&#39;</span>

<span class="token comment"># 正负样本数量</span>
n_positive<span class="token punctuation">,</span>n_negative <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">6000</span>

<span class="token comment"># 生成正样本, 小圆环分布</span>
r_p <span class="token operator">=</span> <span class="token number">5.0</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">,</span>size <span class="token operator">=</span> <span class="token punctuation">[</span>n_positive<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
theta_p <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>pi<span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span>n_positive<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xp <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>r_p<span class="token operator">*</span>torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>theta_p<span class="token punctuation">)</span><span class="token punctuation">,</span>r_p<span class="token operator">*</span>torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>theta_p<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
Yp <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>r_p<span class="token punctuation">)</span>

<span class="token comment"># 生成负样本, 大圆环分布</span>
r_n <span class="token operator">=</span> <span class="token number">8.0</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">,</span>size <span class="token operator">=</span> <span class="token punctuation">[</span>n_negative<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> 
theta_n <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>pi<span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span>n_negative<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xn <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>r_n<span class="token operator">*</span>torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>theta_n<span class="token punctuation">)</span><span class="token punctuation">,</span>r_n<span class="token operator">*</span>torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>theta_n<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
Yn <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>r_n<span class="token punctuation">)</span>

<span class="token comment"># 汇总样本</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>Xp<span class="token punctuation">,</span>Xn<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>Yp<span class="token punctuation">,</span>Yn<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>


<span class="token comment"># 可视化</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;r&quot;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;positive&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;negative&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment"># 创建 TensorDataset，用于存储特征和标签</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span>Y<span class="token punctuation">)</span>
<span class="token comment"># 将数据集按 7:3 比例划分为训练集和验证集，并创建 DataLoader 用于批量数据加载</span>
ds_train<span class="token punctuation">,</span>ds_val <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>random_split<span class="token punctuation">(</span>ds<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token operator">-</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token comment">#获取一个批次的数据</span>
features<span class="token punctuation">,</span>labels <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>dl_train<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>定义模型</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 简单的三层全连接神经网络。fc1、fc2 和 fc3 分别是三层的全连接层</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y
        
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span> 

<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary

summary<span class="token punctuation">(</span>net<span class="token punctuation">,</span>features<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>训练模型</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># L2正则化</span>
<span class="token keyword">def</span> <span class="token function">L2Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>alpha<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l2_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span> <span class="token comment">#一般不对偏置项使用正则</span>
            l2_loss <span class="token operator">=</span> l2_loss <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">0.5</span> <span class="token operator">*</span> alpha <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l2_loss

<span class="token comment"># L1正则化</span>
<span class="token keyword">def</span> <span class="token function">L1Loss</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span>beta<span class="token punctuation">)</span><span class="token punctuation">:</span>
    l1_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">&#39;bias&#39;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
            l1_loss <span class="token operator">=</span> l1_loss <span class="token operator">+</span>  beta <span class="token operator">*</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> l1_loss
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> KerasModel
<span class="token keyword">from</span> torchkeras<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> AUC

net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 将L2正则和L1正则添加到FocalLoss损失，一起作为目标函数</span>
<span class="token keyword">def</span> <span class="token function">focal_loss_with_regularization</span><span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span><span class="token punctuation">:</span>
    y_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>y_pred<span class="token punctuation">)</span>
    focal <span class="token operator">=</span> FocalLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>y_probs<span class="token punctuation">,</span>y_true<span class="token punctuation">)</span> 
    l2_loss <span class="token operator">=</span> L2Loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span><span class="token number">0.001</span><span class="token punctuation">)</span> <span class="token comment">#注意设置正则化项系数</span>
    l1_loss <span class="token operator">=</span> L1Loss<span class="token punctuation">(</span>net<span class="token punctuation">,</span><span class="token number">0.001</span><span class="token punctuation">)</span>
    total_loss <span class="token operator">=</span> focal <span class="token operator">+</span> l2_loss <span class="token operator">+</span> l1_loss
    <span class="token keyword">return</span> total_loss


optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.002</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> KerasModel<span class="token punctuation">(</span>net<span class="token operator">=</span>net<span class="token punctuation">,</span>
                   loss_fn <span class="token operator">=</span> focal_loss_with_regularization <span class="token punctuation">,</span>
                   metrics_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;auc&quot;</span><span class="token punctuation">:</span>AUC<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                   optimizer<span class="token operator">=</span> optimizer <span class="token punctuation">)</span>


dfhistory <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_data<span class="token operator">=</span>dl_train<span class="token punctuation">,</span>
      val_data<span class="token operator">=</span>dl_val<span class="token punctuation">,</span>
      epochs<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
      ckpt_path<span class="token operator">=</span><span class="token string">&#39;checkpoint&#39;</span><span class="token punctuation">,</span>
      patience<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
      monitor<span class="token operator">=</span><span class="token string">&#39;val_auc&#39;</span><span class="token punctuation">,</span>
      mode<span class="token operator">=</span><span class="token string">&#39;max&#39;</span><span class="token punctuation">,</span>
      plot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
      cpu<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 结果可视化</span>
fig<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax1<span class="token punctuation">,</span>ax2<span class="token punctuation">)</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>ncols<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xp<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span><span class="token string">&quot;r&quot;</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xn<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">)</span>
ax1<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;positive&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;negative&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
ax1<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">&quot;y_true&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

Xp_pred <span class="token operator">=</span> X<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&gt;=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
Xn_pred <span class="token operator">=</span> X<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

ax2<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xp_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xp_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;r&quot;</span><span class="token punctuation">)</span>
ax2<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>Xn_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>Xn_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> <span class="token string">&quot;g&quot;</span><span class="token punctuation">)</span>
ax2<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">&quot;positive&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;negative&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
ax2<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">&quot;y_pred&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol><h3 id="_6-3-5、通过优化器实现-l2-正则化" tabindex="-1"><a class="header-anchor" href="#_6-3-5、通过优化器实现-l2-正则化" aria-hidden="true">#</a> 6.3.5、通过优化器实现 L2 正则化</h3><p>如果仅仅需要使用 L2 正则化，那么也可以利用优化器的 weight_decay 参数来实现。</p><p>weight_decay 参数可以设置参数在训练过程中的衰减，这和 L2 正则化的作用效果等价</p><p>Pytorch 的优化器支持一种称之为 Per-parameter options 的操作，就是对每一个参数进行特定的学习率，权重衰减率指定，以满足更为细致的要求。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>weight_params <span class="token operator">=</span> <span class="token punctuation">[</span>param <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token string">&quot;bias&quot;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> name<span class="token punctuation">]</span>
bias_params <span class="token operator">=</span> <span class="token punctuation">[</span>param <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token string">&quot;bias&quot;</span> <span class="token keyword">in</span> name<span class="token punctuation">]</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">&#39;params&#39;</span><span class="token punctuation">:</span> weight_params<span class="token punctuation">,</span> <span class="token string">&#39;weight_decay&#39;</span><span class="token punctuation">:</span><span class="token number">1e-5</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
                             <span class="token punctuation">{</span><span class="token string">&#39;params&#39;</span><span class="token punctuation">:</span> bias_params<span class="token punctuation">,</span> <span class="token string">&#39;weight_decay&#39;</span><span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                            lr<span class="token operator">=</span><span class="token number">1e-2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_6-4、tensorboard-可视化" tabindex="-1"><a class="header-anchor" href="#_6-4、tensorboard-可视化" aria-hidden="true">#</a> 6.4、TensorBoard 可视化</h2><p>在炼丹过程中，如果能够使用丰富的图像来展示模型的结构，指标的变化，参数的分布，输入的形态等信息，无疑会提升我们对问题的洞察力，并增加许多炼丹的乐趣。</p><p>ensorBoard 正是这样一个神奇的炼丹可视化辅助工具。它原是 TensorFlow 的小弟，但它也能够很好地和 Pytorch 进行配合。甚至在 Pytorch 中使用 TensorBoard 比 TensorFlow 中使用 TensorBoard 还要来的更加简单和自然。</p><h3 id="_6-4-0、tensorboard-可视化概述" tabindex="-1"><a class="header-anchor" href="#_6-4-0、tensorboard-可视化概述" aria-hidden="true">#</a> 6.4.0、Tensorboard 可视化概述</h3><p>pytorch 中利用 TensorBoard 可视化的大概过程：</p><ul><li>首先在 Pytorch 中指定一个目录创建一个 torch.utils.tensorboard.SummaryWriter 日志写入器</li><li>据需要可视化的信息，利用日志写入器将相应信息日志写入我们指定的目录</li><li>最后就可以传入日志目录作为参数启动 TensorBoard</li></ul><p>主要介绍 Pytorch 中利用 TensorBoard 进行如下方面信息的可视化的方法</p><ul><li>可视化模型结构： writer.add_graph</li><li>可视化指标变化： writer.add_scalar</li><li>可视化参数分布： writer.add_histogram</li><li>可视化原始图像： writer.add_image 或 writer.add_images</li><li>可视化人工绘图： writer.add_figure</li></ul><p>作者在 torchkeras 库中集成了一个 torchkeras.callback.TensorBoard 回调函数工具，</p><p>利用该工具配合 torchkeras.LightModel 可以用极少的代码在 TensorBoard 中实现绝大部分常用的可视化功能。</p><p>包括：</p><ul><li>可视化模型结构</li><li>可视化指标变化</li><li>可视化参数分布</li><li>可视化超参调整</li></ul>`,65);function D(T,z){const a=e("ExternalLinkIcon");return c(),l("div",null,[i,n("p",null,[s("参考论文："),n("a",r,[s("https://arxiv.org/pdf/1806.10779.pdf"),p(a)])]),k,d,m,n("ul",null,[n("li",null,[n("p",null,[s("LSTM 结构解析（参考文章：《人人都能看懂的 LSTM》"),n("a",v,[s("https://zhuanlan.zhihu.com/p/32085405）"),p(a)])]),b,h]),n("li",null,[n("p",null,[s("GRU 结构解析（参考文章：《人人都能看懂的 GRU》"),n("a",_,[s("https://zhuanlan.zhihu.com/p/32481747）"),p(a)])]),y])]),f,n("p",null,[s("参考阅读材料："),g,s(" Transformer知乎原理讲解："),n("a",w,[s("https://zhuanlan.zhihu.com/p/48508221"),p(a)]),L,s(" Transformer哈佛博客代码讲解："),n("a",q,[s("http://nlp.seas.harvard.edu/annotated-transformer/"),p(a)])]),x])}const S=o(u,[["render",D],["__file","6、Pytorch 中阶 API.html.vue"]]);export{S as default};
