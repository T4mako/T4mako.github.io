import{_ as p}from"./plugin-vue_export-helper-c27b6911.js";import{r as e,o,c as l,b as n,f as s,e as i,d as a}from"./app-392a5c85.js";const c={},u=a('<h2 id="基础概念" tabindex="-1"><a class="header-anchor" href="#基础概念" aria-hidden="true">#</a> 基础概念</h2><h3 id="机器学习概念" tabindex="-1"><a class="header-anchor" href="#机器学习概念" aria-hidden="true">#</a> 机器学习概念</h3><h4 id="梯度下降" tabindex="-1"><a class="header-anchor" href="#梯度下降" aria-hidden="true">#</a> 梯度下降</h4><h4 id="误差" tabindex="-1"><a class="header-anchor" href="#误差" aria-hidden="true">#</a> 误差</h4><ul><li>训练误差：模型在训练数据上的误差</li><li>泛化误差：模型在新数据上的误差</li></ul><h4 id="k-折交叉验证" tabindex="-1"><a class="header-anchor" href="#k-折交叉验证" aria-hidden="true">#</a> k 折交叉验证</h4><h4 id="过拟合与欠拟合" tabindex="-1"><a class="header-anchor" href="#过拟合与欠拟合" aria-hidden="true">#</a> 过拟合与欠拟合</h4><h4 id="权重衰退" tabindex="-1"><a class="header-anchor" href="#权重衰退" aria-hidden="true">#</a> 权重衰退</h4>',8),r=n("ul",null,[n("li",null,[n("p",null,"使用均方范数作为硬性限制，控制模型复杂度，减少过拟合"),n("p",null,[s("通过限制参数值 w 的选择范围来控制模型容量"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",{mathvariant:"normal"},"∣"),n("mi",{mathvariant:"normal"},"∣"),n("mi",null,"w"),n("mi",{mathvariant:"normal"},"∣"),n("msup",null,[n("mi",{mathvariant:"normal"},"∣"),n("mn",null,"2")]),n("mo",null,"≤"),n("mi",null,"θ")]),n("annotation",{encoding:"application/x-tex"},"||w||^2 \\le \\theta")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.0641em","vertical-align":"-0.25em"}}),n("span",{class:"mord"},"∣∣"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),n("span",{class:"mord"},"∣"),n("span",{class:"mord"},[n("span",{class:"mord"},"∣"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.8141em"}},[n("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},"2")])])])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"≤"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"θ")])])]),s(" ，通常不限制 bias，小的 theta 意味着更强的正则项")])])],-1),m=a('<h4 id="dropout-丢弃法" tabindex="-1"><a class="header-anchor" href="#dropout-丢弃法" aria-hidden="true">#</a> dropout 丢弃法</h4><p>动机：一个好的模型需要对输入数据的扰动鲁棒，减少过拟合</p><ul><li>使用有噪音的数据等价于 Tikhonov 正则</li><li>dropout：在层之间加入噪音，丢弃一部分前一层的输入、后一层的输出</li></ul><p>通常将 dropout 作用在 <strong>隐藏全连接层的输出</strong> 上，将其中的一些值随机设为 <strong>0</strong> 来控制模型复杂度，丢弃概率为 <strong>超参数</strong>，其他数会相应变大，保证均值方差一样</p><p>注意：dropout 用在训练模型上减少复杂性，在使用模型时一般不用 dropout</p><p>![image-20241010195927901](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241010195927901.png)</p><h4 id="数值稳定性" tabindex="-1"><a class="header-anchor" href="#数值稳定性" aria-hidden="true">#</a> 数值稳定性</h4><p>数值稳定性常见的两个问题：</p><ul><li>梯度爆炸 <ul><li>值超出阈值</li><li>对学习率敏感</li></ul></li><li>梯度消失 <ul><li>梯度值变为 0</li><li>训练无进展</li><li>对于底部层尤为严重</li></ul></li></ul><p>因此，合理的权重初始值和激活函数的选取可以提升数值稳定性</p><h3 id="数据集操作" tabindex="-1"><a class="header-anchor" href="#数据集操作" aria-hidden="true">#</a> 数据集操作</h3><p>数据集可分为</p><ul><li>训练集</li><li>验证集</li><li>测试集</li></ul><h4 id="数据增广" tabindex="-1"><a class="header-anchor" href="#数据增广" aria-hidden="true">#</a> 数据增广</h4><p>数据增强：则国家已有数据集，使得有更多的多样性。如在语音中加入背景噪声、改变图片的颜色和形状（翻转、切割、改颜色）</p><p>数据增广一般使用 torchvision</p><h3 id="硬件、计算性能" tabindex="-1"><a class="header-anchor" href="#硬件、计算性能" aria-hidden="true">#</a> 硬件、计算性能</h3>',17),d={href:"https://zh.d2l.ai/chapter_computational-performance/index.html",target:"_blank",rel:"noopener noreferrer"},k=a(`<h3 id="微调" tabindex="-1"><a class="header-anchor" href="#微调" aria-hidden="true">#</a> 微调</h3><p>使用已训练好的模型的特征提取层与权重，调整自己崔侯的全连接层</p><p>![image-20241018164921787](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241018164921787.png)</p><h2 id="全连接层-fc" tabindex="-1"><a class="header-anchor" href="#全连接层-fc" aria-hidden="true">#</a> 全连接层 fc</h2><h3 id="线性回归" tabindex="-1"><a class="header-anchor" href="#线性回归" aria-hidden="true">#</a> 线性回归</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 生成示例数据</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token number">4</span> <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> X <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># y = 4 + 3 * X + 噪声</span>

<span class="token comment"># 将数据转换为 PyTorch 张量</span>
X_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
y_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># 定义线性回归模型</span>
<span class="token keyword">class</span> <span class="token class-name">LinearRegressionModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearRegressionModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#（输入维度，输出维度）输入x，输出y</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment"># 实例化模型</span>
model <span class="token operator">=</span> LinearRegressionModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 定义损失函数和优化器</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 均方误差损失</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>  <span class="token comment"># 随机梯度下降优化器</span>

<span class="token comment"># 训练模型</span>
num_epochs <span class="token operator">=</span> <span class="token number">1000</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 前向传播</span>
    y_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X_tensor<span class="token punctuation">)</span>
    
    <span class="token comment"># 计算损失</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y_tensor<span class="token punctuation">)</span>
    
    <span class="token comment"># 反向传播</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Epoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>num_epochs<span class="token punctuation">}</span></span><span class="token string">], Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>

<span class="token comment"># 绘制结果</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    predicted <span class="token operator">=</span> model<span class="token punctuation">(</span>X_tensor<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">&#39;blue&#39;</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&#39;实际数据&#39;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> predicted<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">&#39;red&#39;</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">&#39;拟合直线&#39;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">&#39;X&#39;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">&#39;y&#39;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">&#39;线性回归示例&#39;</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="softmax-分类" tabindex="-1"><a class="header-anchor" href="#softmax-分类" aria-hidden="true">#</a> Softmax 分类</h3><p>Softmax 是多分类任务，隐藏层为线性层，输出为多个。</p>`,8),h=n("p",null,[s("为了使输出为概率，使用 "),n("strong",null,"softmax"),s(),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"s"),n("mi",null,"i"),n("mi",null,"g"),n("mi",null,"m"),n("mi",null,"a"),n("mo",{stretchy:"false"},"("),n("mi",{mathvariant:"bold"},"z"),n("msub",null,[n("mo",{stretchy:"false"},")"),n("mi",null,"i")]),n("mo",null,"="),n("mfrac",null,[n("msup",null,[n("mi",null,"e"),n("msub",null,[n("mi",null,"z"),n("mi",null,"i")])]),n("mrow",null,[n("msubsup",null,[n("mo",null,"∑"),n("mrow",null,[n("mi",null,"j"),n("mo",null,"="),n("mn",null,"1")]),n("mi",null,"K")]),n("msup",null,[n("mi",null,"e"),n("msub",null,[n("mi",null,"z"),n("mi",null,"j")])])])]),n("mspace",{width:"1em"}),n("mtext",null,"for "),n("mi",null,"i"),n("mo",null,"="),n("mn",null,"1"),n("mo",{separator:"true"},","),n("mn",null,"2"),n("mo",{separator:"true"},","),n("mo",null,"…"),n("mo",{separator:"true"},","),n("mi",null,"K")]),n("annotation",{encoding:"application/x-tex"},"sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } i = 1, 2, \\ldots, K")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mord mathnormal"},"s"),n("span",{class:"mord mathnormal"},"i"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),n("span",{class:"mord mathnormal"},"ma"),n("span",{class:"mopen"},"("),n("span",{class:"mord mathbf"},"z"),n("span",{class:"mclose"},[n("span",{class:"mclose"},")"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3117em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mathnormal mtight"},"i")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.6629em","vertical-align":"-0.7519em"}}),n("span",{class:"mord"},[n("span",{class:"mopen nulldelimiter"}),n("span",{class:"mfrac"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.911em"}},[n("span",{style:{top:"-2.5703em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mop mtight"},[n("span",{class:"mop op-symbol small-op mtight",style:{position:"relative",top:"0em"}},"∑"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.8852em"}},[n("span",{style:{top:"-2.1786em","margin-left":"0em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j"),n("span",{class:"mrel mtight"},"="),n("span",{class:"mord mtight"},"1")])])]),n("span",{style:{top:"-2.8971em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"K")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.4603em"}},[n("span")])])])])]),n("span",{class:"mspace mtight",style:{"margin-right":"0.1952em"}}),n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"e"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.779em"}},[n("span",{style:{top:"-2.9714em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.04398em"}},"z"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3448em"}},[n("span",{style:{top:"-2.3448em","margin-left":"-0.044em","margin-right":"0.1em"}},[n("span",{class:"pstrut",style:{height:"2.6595em"}}),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.5092em"}},[n("span")])])])])])])])])])])])])])])])]),n("span",{style:{top:"-3.23em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),n("span",{style:{top:"-3.394em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"e"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.7385em"}},[n("span",{style:{top:"-2.931em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.04398em"}},"z"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3448em"}},[n("span",{style:{top:"-2.3448em","margin-left":"-0.044em","margin-right":"0.1em"}},[n("span",{class:"pstrut",style:{height:"2.6595em"}}),n("span",{class:"mord mathnormal mtight"},"i")])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3147em"}},[n("span")])])])])])])])])])])])])])])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.7519em"}},[n("span")])])])]),n("span",{class:"mclose nulldelimiter"})]),n("span",{class:"mspace",style:{"margin-right":"1em"}}),n("span",{class:"mord text"},[n("span",{class:"mord"},"for ")]),n("span",{class:"mord mathnormal"},"i"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),n("span",{class:"mord"},"1"),n("span",{class:"mpunct"},","),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord"},"2"),n("span",{class:"mpunct"},","),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"minner"},"…"),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mpunct"},","),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K")])])]),s(" 函数，通过真实值与预测值的概率经过 "),n("strong",null,"交叉熵损失"),s(" 得到 0,1 值")],-1),v=a(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

<span class="token comment"># 1. 数据准备：下载MNIST数据集，并进行预处理</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">&#39;./data&#39;</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>
test_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">&#39;./data&#39;</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token comment"># 2. 定义模型：一个简单的两层神经网络</span>
<span class="token keyword">class</span> <span class="token class-name">SimpleNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># 输入层 (28*28像素)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>     <span class="token comment"># 输出层 (10类)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">)</span>            <span class="token comment"># 将输入展平</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment"># 第一个全连接层和ReLU激活函数</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                  <span class="token comment"># 第二个全连接层</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>   <span class="token comment"># 使用softmax函数生成概率分布</span>

<span class="token comment"># 3. 初始化模型、损失函数和优化器</span>
model <span class="token operator">=</span> SimpleNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 交叉熵损失函数 (softmax和损失结合)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># 4. 训练模型</span>
num_epochs <span class="token operator">=</span> <span class="token number">5</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> images<span class="token punctuation">,</span> labels <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>          <span class="token comment"># 前向传播</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token comment"># 计算损失</span>

        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment"># 梯度清零</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                  <span class="token comment"># 反向传播</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                 <span class="token comment"># 更新模型参数</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Epoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>num_epochs<span class="token punctuation">}</span></span><span class="token string">], Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>

<span class="token comment"># 5. 测试模型</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
correct <span class="token operator">=</span> <span class="token number">0</span>
total <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> images<span class="token punctuation">,</span> labels <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 预测最大概率的类</span>
        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Accuracy of the model on the 10000 test images: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">%&#39;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="多层感知机-mlp" tabindex="-1"><a class="header-anchor" href="#多层感知机-mlp" aria-hidden="true">#</a> 多层感知机 MLP</h3><p>感知机为二分类。给定输入 x，权重 w，偏移 b，感知机输出 0 或 1（有时为 -1 或 1）。它不能拟合 XOR 函数</p><p>多层感知机（MLP）的简单例子：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

<span class="token comment"># 定义MLP模型</span>
<span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 定义第一层全连接层</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        <span class="token comment"># 激活函数ReLU</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 定义第二层全连接层</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 第一层 + 激活函数</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        <span class="token comment"># 输出层</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

<span class="token comment"># 模型实例化</span>
input_size <span class="token operator">=</span> <span class="token number">10</span>   <span class="token comment"># 输入特征数</span>
hidden_size <span class="token operator">=</span> <span class="token number">20</span>  <span class="token comment"># 隐藏层神经元数</span>
output_size <span class="token operator">=</span> <span class="token number">3</span>   <span class="token comment"># 输出类别数</span>

model <span class="token operator">=</span> MLP<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>

<span class="token comment"># 损失函数和优化器</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>

<span class="token comment"># 假设我们有一个大小为10的输入张量</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> input_size<span class="token punctuation">)</span>  <span class="token comment"># 批次大小为5</span>
labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 随机生成5个类别标签</span>

<span class="token comment"># 前向传播</span>
outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment"># 计算损失</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>

<span class="token comment"># 反向传播和优化</span>
optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 梯度清零</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 反向传播</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>       <span class="token comment"># 更新参数</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="卷积神经网络-cnn" tabindex="-1"><a class="header-anchor" href="#卷积神经网络-cnn" aria-hidden="true">#</a> 卷积神经网络 CNN</h2><p>卷积神经网络通常用于 <strong>图像、文本</strong> 处理</p><p>整体架构大致为：conv，relu，conv，relu，pool，conv....，fc</p><blockquote><p>n 层神经网络：有 n 层带参数的层（pool，relu 不算层）</p></blockquote><h3 id="多输入输出通道" tabindex="-1"><a class="header-anchor" href="#多输入输出通道" aria-hidden="true">#</a> 多输入输出通道</h3><p>彩色图片由 RGB 三个通道，每个输入通道通常有独立的二维卷积核</p><p>可以使用多个三维卷积核（下图核函数粘贴复制），每个核生成一个输出通道</p><p>![image-20241018160507227](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241018160507227.png)</p><ul><li>计算的结果为 <strong>特征图</strong></li><li>一次卷积可以有多个 Filter，卷积后的深度就为 Filter 的个数（上述 7*7*3 经过 2 个 3*3*3 的 filter 变为 3*3*2 的特征图）</li></ul><p>1 * 1 的卷积核不识别空间模式，只是融合通道，以 <strong>c<sub>i</sub> 个输入</strong> 值转换为 <strong>c<sub>o</sub> 个输出值</strong></p><p>![image-20241018160829623](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241018160829623.png)</p><h3 id="卷积层-conv-convolution" tabindex="-1"><a class="header-anchor" href="#卷积层-conv-convolution" aria-hidden="true">#</a> 卷积层 conv（convolution）</h3><p>卷积层中的 w 是卷积核，b 是偏置，w、b 是可学习参数</p><ul><li>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</li><li>核矩阵和偏移是可学习的参数</li></ul><p><strong>卷积层</strong> 涉及 <strong>超参数</strong>：</p><ul><li>滑动窗口步长</li><li>卷积核尺寸</li><li>是否边缘填充</li><li>卷积核个数</li></ul><p>卷积层的权重（w + b）参数：所有卷积核的像素数 + 卷积核个数（偏置）</p><p>![image-20240821222130729](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\强化学习\\assets\\image-20240821222130729.png)</p><p>卷积结果计算公式：</p><p>![image-20240822133826745](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\强化学习\\assets\\image-20240822133826745.png)</p><p>可以对一次卷积后的特征图再卷积再卷积</p><h3 id="池化层-pool" tabindex="-1"><a class="header-anchor" href="#池化层-pool" aria-hidden="true">#</a> 池化层 pool</h3><p>用于 <strong>压缩特征图</strong></p><p>例如： MAX POOLING 最大池化层： 2*2 的特征为一组，筛选最大的值。平均池化层，将最大操作替换为平均</p><ul><li>池化层与卷积层类似，都具有填充和步幅</li><li>池化层没有可学习的参数</li><li>在每个输入通道应用池化层以获得相应的输出通道</li><li>输出通道数 = 输入通道数</li></ul><h3 id="全连接层-fc-1" tabindex="-1"><a class="header-anchor" href="#全连接层-fc-1" aria-hidden="true">#</a> 全连接层 fc</h3><p>通过前面 conv 和 pool 得到最后的特征图（假设为 32*32*10），任务为 n 分类任务</p><p>将特征图拉成特征向量 [1,32*32*10] ，则全连接层参数为 [32*32*10,n]</p><h2 id="经典多层神经网路" tabindex="-1"><a class="header-anchor" href="#经典多层神经网路" aria-hidden="true">#</a> 经典多层神经网路</h2><h3 id="lenet-1980s" tabindex="-1"><a class="header-anchor" href="#lenet-1980s" aria-hidden="true">#</a> LeNet （1980s）</h3><p>卷积、全连接</p><h3 id="alexnet-2012-年" tabindex="-1"><a class="header-anchor" href="#alexnet-2012-年" aria-hidden="true">#</a> AlexNet（2012 年）</h3><p>更大卷积，全连接</p><h3 id="vgg-2014-年" tabindex="-1"><a class="header-anchor" href="#vgg-2014-年" aria-hidden="true">#</a> Vgg（2014 年）</h3><p>提出 vgg 块，更大更深的 AlexNet，赋值粘贴 AlexNet</p><p>![image-20241012111726737](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241012111726737.png)</p><h3 id="nin" tabindex="-1"><a class="header-anchor" href="#nin" aria-hidden="true">#</a> NiN</h3><p>提出 NiN 块，一个卷积层后跟两个全连接层（1*1 的卷积核）</p><p>![image-20241012111744823](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241012111744823.png)</p><h3 id="googlenet" tabindex="-1"><a class="header-anchor" href="#googlenet" aria-hidden="true">#</a> GoogLeNet</h3><p>提出 Inception 块</p><h3 id="resnet-2015年" tabindex="-1"><a class="header-anchor" href="#resnet-2015年" aria-hidden="true">#</a> Resnet（2015年）</h3><p>vgg 在层数更多时，训练效果反而不好</p><p>resnet 将 <strong>好的层保留，不好的层跳过</strong>。做法是通过对层数堆叠的值与同等映射的值进行比较</p><ul><li><p>若果层数堆叠的值不好，将层数堆叠权重设为 0</p><p>![image-20240822141714082](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\强化学习\\assets\\image-20240822141714082.png)</p></li></ul><h3 id="densenet" tabindex="-1"><a class="header-anchor" href="#densenet" aria-hidden="true">#</a> DenseNet</h3><h2 id="批量归一化层" tabindex="-1"><a class="header-anchor" href="#批量归一化层" aria-hidden="true">#</a> 批量归一化层</h2><ul><li>损失在最后，后面的层训练比加快，前面的层训练比较慢。</li><li>前面的层一变化，所有层都得跟这边，最后的那些层需要重新学习多次，导致收敛边慢</li></ul><p>方法：</p><ul><li><p>固定小批量 batch 里的均值和方差，然后再做额外调整</p><p>![image-20241016182432423](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241016182432423.png)</p><p>![image-20241016182512399](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\Deep Learning\\assets\\image-20241016182512399.png)</p><ul><li>批量归一化层可学习的参数为 γ 和 β</li><li>作用在 <ul><li>全连接层和卷积层输出上，激活函数前</li><li>全连接层和卷积层输入上</li></ul></li><li>对前连接层，作用在特征维</li><li>对于卷积层，作用在通道维</li></ul></li></ul><h2 id="目标检测算法" tabindex="-1"><a class="header-anchor" href="#目标检测算法" aria-hidden="true">#</a> 目标检测算法</h2><p>锚框：一类目标检测算法是基于锚框</p><ul><li>提出多个被称为锚框的区域(边缘框)</li><li>预测每个锚框里是否含有关注的物体</li><li>如果是，预测从这个锚框到真实边缘框的偏移</li></ul><p>loU 交并比：</p><ul><li>IoU 用来计算两个框之间的相似度</li><li>0 表示无重叠，1 表示重合，具体值通过集合计算</li></ul><p>赋予锚框标号</p><ul><li>每个锚框是一个训练样本</li><li>将每个锚框，要么标注成背景，要么关联上一个真实边缘框</li><li>我们可能会生成大量的锚框，这个导致大量的负类样本</li></ul><p>非极大值抑制输出(NMS)</p><ul><li>每个锚框预测一个边缘框</li><li>NMS可以合并相似的预测 <ul><li>选中是非背景类的最大预测值</li><li>去掉所有其它和它IoU值大于 θ 的预测</li><li>重复上述过程直到所有预测要么被选中，要么被去掉</li></ul></li></ul><h2 id="迁移学习" tabindex="-1"><a class="header-anchor" href="#迁移学习" aria-hidden="true">#</a> 迁移学习</h2><p>深度学习的常见问题</p><ul><li>训练数据量过少。导致欠拟合、过拟合 <ul><li>使用数据增强</li></ul></li><li>参数调节过多、时间成本大</li></ul><p><strong>迁移学习</strong> 就是使用 <strong>他人的</strong> 与自己项1目相似（数据集相似，参数相似）的项目的网络的 <strong>w，b</strong> 参数</p><ul><li>继续训练 w，b 参数</li><li>微调网络（特别是 fc），继续训练一部分 w，b 参数</li></ul><p>迁移学习的速度非常快、pytorch 官网有迁移学习的例子</p><h2 id="rnn-递归神经网络" tabindex="-1"><a class="header-anchor" href="#rnn-递归神经网络" aria-hidden="true">#</a> RNN 递归神经网络</h2><p>RNN 的特点是带有时间序列，常用于 NLP，RNN 网络会将之前的结果全部记下来（h<sub>0</sub>，h<sub>1</sub> .......），即输入过多，效果并不好</p><p>![image-20240826143426351](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\强化学习\\assets\\image-20240826143426351.png)</p><h2 id="lstm-网络" tabindex="-1"><a class="header-anchor" href="#lstm-网络" aria-hidden="true">#</a> LSTM 网络</h2><p>相较于 RNN，添加控制参数 c，决定什么样的信息会被保留，什么样的会被遗忘</p><h2 id="词向量模型-word2vec" tabindex="-1"><a class="header-anchor" href="#词向量模型-word2vec" aria-hidden="true">#</a> 词向量模型 Word2Vec</h2><p>文本向量化：</p><ul><li>使用一定维度的向量描述词语</li><li>数据维度越高，能提供的信息越多</li><li>相似的词在特征表达中比较相似</li></ul><p>词向量模型训练过程：</p><ul><li>输入： 一个一个词，根据词向量大表（随机初始化）转换为向量</li><li>输出：一个词，输出可能性最高的词，类似多分类任务</li><li>更新词向量大表中输入数据的词向量</li></ul><p>不同架构模型：</p><ul><li>CBOW</li><li>Skip-gram</li></ul><p>如果一个语料库很大，可能的结果就很多，最后一层 softmax 计算起来十分耗时。解决方案:</p><ul><li>输入两个单词，看他们是不是前后对应的输入和输出，相当于一个二分类任务</li><li>由于训练数据来自文本，上述方法的结果都为 1，因此引入 <strong>负采样</strong>，即认为添加结果为 0 的输入</li></ul><h2 id="gan-对抗生成网络" tabindex="-1"><a class="header-anchor" href="#gan-对抗生成网络" aria-hidden="true">#</a> GAN 对抗生成网络</h2><p>对抗生成网络有 <strong>生成器</strong> 与 <strong>判别器</strong></p><ul><li>生成器生成想要的东西</li><li>判别器用于判别</li></ul><p>![image-20240828214817355](E:\\Study=my repo\\vuepress-hope-bloc\\my-docs\\src\\code\\python\\Machine Learning\\强化学习\\assets\\image-20240828214817355.png)</p><h2 id="cyclegan" tabindex="-1"><a class="header-anchor" href="#cyclegan" aria-hidden="true">#</a> CycleGan</h2><p>PatchGAN 的作用：</p><ul><li>输入一个 N * N 的矩阵个，基于感受野来计算损失</li><li>基于感受野在特征图上的预测结果和标签（也需设置成 N * N）计算损失</li></ul><h2 id="knn" tabindex="-1"><a class="header-anchor" href="#knn" aria-hidden="true">#</a> KNN</h2><h2 id="随机森林" tabindex="-1"><a class="header-anchor" href="#随机森林" aria-hidden="true">#</a> 随机森林</h2>`,93);function b(g,y){const t=e("ExternalLinkIcon");return o(),l("div",null,[u,r,m,n("p",null,[n("a",d,[s("https://zh.d2l.ai/chapter_computational-performance/index.html"),i(t)])]),k,h,v])}const x=p(c,[["render",b],["__file","网络架构.html.vue"]]);export{x as default};
