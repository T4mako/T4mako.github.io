const t=JSON.parse('{"key":"v-8206932e","path":"/code/python/Machine%20Learning/Pytorch/3%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8.html","title":"3、自动微分机制、优化器","lang":"zh-CN","frontmatter":{"description":"3、自动微分机制、优化器 求梯度的两种方法 通过反向传播 backward 方法实现梯度计算。 该方法求得的梯度将存在对应 tenseor 的 grad 属性 下 也可以调用 torch.autograd.grad 函数来实现求梯度计算 注意：使用上述方法求梯度后必须清零，否则（tensor.grad）会造成累加计算","head":[["meta",{"property":"og:url","content":"https://T4mako.github.io/code/python/Machine%20Learning/Pytorch/3%E3%80%81%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E6%9C%BA%E5%88%B6%E3%80%81%E4%BC%98%E5%8C%96%E5%99%A8.html"}],["meta",{"property":"og:site_name","content":"T4mako"}],["meta",{"property":"og:title","content":"3、自动微分机制、优化器"}],["meta",{"property":"og:description","content":"3、自动微分机制、优化器 求梯度的两种方法 通过反向传播 backward 方法实现梯度计算。 该方法求得的梯度将存在对应 tenseor 的 grad 属性 下 也可以调用 torch.autograd.grad 函数来实现求梯度计算 注意：使用上述方法求梯度后必须清零，否则（tensor.grad）会造成累加计算"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"T4mako"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"3、自动微分机制、优化器\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"T4mako\\",\\"url\\":\\"https://github.com/T4mako/T4mako.github.io\\"}]}"]]},"headers":[{"level":2,"title":"3.1、backward 方法求导数","slug":"_3-1、backward-方法求导数","link":"#_3-1、backward-方法求导数","children":[]},{"level":2,"title":"3.2、利用 autograd.grad 方法求导数","slug":"_3-2、利用-autograd-grad-方法求导数","link":"#_3-2、利用-autograd-grad-方法求导数","children":[]},{"level":2,"title":"3.3、利用自动微分和优化器求 optimizer 最小值","slug":"_3-3、利用自动微分和优化器求-optimizer-最小值","link":"#_3-3、利用自动微分和优化器求-optimizer-最小值","children":[]}],"readingTime":{"minutes":2.27,"words":682},"filePathRelative":"code/python/Machine Learning/Pytorch/3、自动微分机制、优化器.md","excerpt":"<h1> 3、自动微分机制、优化器</h1>\\n<p><strong>求梯度的两种方法</strong></p>\\n<ul>\\n<li>\\n<p>通过反向传播 <strong>backward</strong> 方法实现梯度计算。</p>\\n<p>该方法求得的梯度将存在对应 <strong>tenseor 的 grad 属性</strong> 下</p>\\n</li>\\n<li>\\n<p>也可以调用 <strong>torch.autograd.grad</strong> 函数来实现求梯度计算</p>\\n</li>\\n</ul>\\n<p><strong>注意</strong>：使用上述方法求梯度后必须清零，否则（tensor.grad）会造成累加计算</p>","autoDesc":true}');export{t as data};
