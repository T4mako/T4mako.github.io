import{_ as t}from"./plugin-vue_export-helper-c27b6911.js";import{o as p,c as e,d as a,b as n,f as s}from"./app-00d6fe81.js";const o={},c=a(`<h2 id="_4、动态计算图" tabindex="-1"><a class="header-anchor" href="#_4、动态计算图" aria-hidden="true">#</a> 4、动态计算图</h2><h3 id="_4-1、动态计算图简介" tabindex="-1"><a class="header-anchor" href="#_4-1、动态计算图简介" aria-hidden="true">#</a> 4.1、动态计算图简介</h3><p><strong>计算图由节点和边组成</strong></p><ul><li>节点：张量或者Function</li><li>边：张量和 Function 之间的依赖关系</li></ul><p>计算图是动态图，动态图有两层含义：</p><ul><li>计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果</li><li>计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了 backward 方法执行了反向传播，或者利用 torch.autograd.grad 方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建</li></ul><p><strong>计算图的正向传播立即执行</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
Y_hat <span class="token operator">=</span> X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 计算张量的平均值</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Y_hat<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>计算图在反向传播后立即销毁</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
Y_hat <span class="token operator">=</span> X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment">#计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置retain_graph = True</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#loss.backward(retain_graph = True) </span>

<span class="token comment">#loss.backward() #如果再次执行反向传播将报错</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-2、计算图中的-function" tabindex="-1"><a class="header-anchor" href="#_4-2、计算图中的-function" aria-hidden="true">#</a> 4.2、计算图中的 Function</h3><p>计算图中的另外一种节点是 Function, 实际上就是 Pytorch 中各种对张量操作的函数。</p><p>这些 Function 和我们 Python 中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。</p><p>我们可以通过继承 torch.autograd.Function 来创建这种支持反向传播的 Function</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>
   
    <span class="token comment">#正向传播逻辑，可以用ctx存储一些值，供反向传播使用。</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment">#反向传播逻辑</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors
        grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
        grad_input<span class="token punctuation">[</span><span class="token builtin">input</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">return</span> grad_input
    
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span><span class="token builtin">apply</span> <span class="token comment"># relu现在也可以具有正向传播和反向传播功能</span>
Y_hat <span class="token operator">=</span> relu<span class="token punctuation">(</span>X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># tensor([[4.5000, 4.5000]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># tensor([[4.5000]])</span>

<span class="token comment"># Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Y_hat<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-3、计算图与反向传播" tabindex="-1"><a class="header-anchor" href="#_4-3、计算图与反向传播" aria-hidden="true">#</a> 4.3、计算图与反向传播</h3><p>简单地理解一下反向传播的原理和过程</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>
y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>x
loss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>loss.backward() 语句调用后，依次发生以下计算过程。</p><ol><li><p>loss 自己的 grad 梯度赋值为 1，即对自身的梯度为 1</p></li><li><p>loss 根据其自身梯度以及关联的 backward 方法，计算出其对应的自变量即 y1 和 y2 的梯度，将该值赋值到 y1.grad 和 y2.grad</p></li><li><p>y2 和 y1 根据其自身梯度以及关联的 backward 方法, 分别计算出其对应的自变量 x 的梯度，x.grad 将其收到的多个梯度值累加</p><p>（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）</p></li></ol><p>正因为求导链式法则衍生的梯度累加规则，张量的 grad 梯度不会自动清零，在需要的时候需要手动置零</p><h3 id="_4-4、叶子节点和非叶子节点" tabindex="-1"><a class="header-anchor" href="#_4-4、叶子节点和非叶子节点" aria-hidden="true">#</a> 4.4、叶子节点和非叶子节点</h3><p>执行下面代码，我们会发现 loss.grad 并不是我们期望的 1，而是 None</p><p>类似地 y1.grad 以及 y2.grad 也是 None</p><p>这是由于它们不是叶子节点张量。</p><p>在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。</p><p>那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。</p><ol><li>叶子节点张量是由用户直接创建的张量，而非由某个 Function 通过计算得到的张量。</li><li>叶子节点张量的 requires_grad 属性必须为 True.</li></ol><p>Pytorch 设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。</p><p>所有依赖于叶子节点张量的张量, 其 requires_grad 属性必定是 True 的，但其梯度值只在计算过程中被用到，不会最终存储到 grad 属性中。</p><p>如果需要保留中间计算结果的梯度到 grad 属性中，可以使用 retain_grad 方法。 如果仅仅是为了调试代码查看梯度值，可以利用 register_hook 打印日志。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>
y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>x
loss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;loss.grad:&quot;</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># loss.grad: None</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y1.grad:&quot;</span><span class="token punctuation">,</span> y1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># y1.grad: None</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;y2.grad:&quot;</span><span class="token punctuation">,</span> y2<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># y2.grad: None</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># tensor(4.)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># True</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y1<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># False</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y2<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># False</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span> <span class="token comment"># False</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>利用 retain_grad 可以保留非叶子节点的梯度值，利用 register_hook 可以查看非叶子节点的梯度值</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 

<span class="token comment"># 正向传播</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>
y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>x
loss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

<span class="token comment"># 非叶子节点梯度显示控制</span>
y1<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;y1 grad: &#39;</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
y2<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;y2 grad: &#39;</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 反向传播</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;loss.grad:&quot;</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;x.grad:&quot;</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39;
y2 grad:  tensor(4.)
y1 grad:  tensor(-4.)
loss.grad: tensor(1.)
x.grad: tensor(4.)
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-5、计算图在-tensorboard-中的可视化" tabindex="-1"><a class="header-anchor" href="#_4-5、计算图在-tensorboard-中的可视化" aria-hidden="true">#</a> 4.5、计算图在 TensorBoard 中的可视化</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> x@self<span class="token punctuation">.</span>w <span class="token operator">+</span> self<span class="token punctuation">.</span>b
        <span class="token keyword">return</span> y

net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token string">&#39;./data/tensorboard&#39;</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>net<span class="token punctuation">,</span>input_to_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token operator">%</span>load_ext tensorboard
<span class="token comment">#%tensorboard --logdir ./data/tensorboard</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> tensorboard <span class="token keyword">import</span> notebook
notebook<span class="token punctuation">.</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 在tensorboard中查看模型</span>
notebook<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token string">&quot;--logdir ./data/tensorboard&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_5、pytorch-层次结构" tabindex="-1"><a class="header-anchor" href="#_5、pytorch-层次结构" aria-hidden="true">#</a> 5、Pytorch 层次结构</h2><p>Pytorch 中 5 个不同的层次结构：</p><ul><li>硬件层：Pytorch 支持 CPU、GPU 加入计算资源池</li><li>内核层：C++ 实现的内核</li><li>低阶 API：Python 实现的操作符，提供了封装 C++ 内核的低级 API 指令，主要包括各种张量操作算子、自动微分、变量管理</li><li>中阶 API：Python 实现的模型组件，对低级API进行了函数封装，主要包括各种模型层，损失函数，优化器，数据管道等等</li><li>高阶 API：Python 实现的模型接口。Pytorch 没有官方的高阶API。为了便于训练模型，作者仿照 keras 中的模型接口，封装了 pytorch 的高阶模型接口 torchkeras.KerasModel。此外，有一个非常流行的非官方 Pytorch 的高阶 API 库，叫做 pytorch_lightning，作者通过引用和借鉴它的一些能力，设计了一个和 torchkeras.KerasModel 功能类似的高阶模型接口 torchkeras.LightModel，功能更加强大。</li></ul><h3 id="_5-1、低阶-api" tabindex="-1"><a class="header-anchor" href="#_5-1、低阶-api" aria-hidden="true">#</a> 5.1、低阶 API</h3><p>低阶 API 主要包括 <strong>张量操作</strong>，<strong>计算图</strong> 和 <strong>自动微分</strong></p><p>张量结构操作主要包括：张量创建，索引切片，维度变换，合并分割。</p><h4 id="创建张量" tabindex="-1"><a class="header-anchor" href="#创建张量" aria-hidden="true">#</a> 创建张量</h4><p>张量创建的许多方法和 numpy 中创建 array 的方法很像</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch 

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token comment"># tensor([1., 2., 3.])</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span>step <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># tensor([1, 3, 5, 7, 9])</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token operator">*</span><span class="token number">3.14</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment"># tensor([0.0000, 0.6978, 1.3956, 2.0933, 2.7911, 3.4889, 4.1867, 4.8844, 5.5822,6.2800])</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
&#39;&#39;&#39;</span>
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.int32)
&#39;&#39;&#39;</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
&#39;&#39;&#39;</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>a<span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[5., 5., 5.],
        [5., 5., 5.],
        [5., 5., 5.]])
&#39;&#39;&#39;</span>
torch<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>b<span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>

<span class="token comment"># 均匀随机分布</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
minval<span class="token punctuation">,</span>maxval <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span>
a <span class="token operator">=</span> minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># tensor([4.9626, 7.6822, 0.8848, 1.3203, 3.0742])</span>

<span class="token comment"># 正态分布随机</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[ 0.5507,  0.2704,  0.6472],
        [ 0.2490, -0.3354,  0.4564],
        [-0.6255,  0.4539, -1.3740]])
&#39;&#39;&#39;</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>normal<span class="token punctuation">(</span>mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> std <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 正态分布随机</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[16.2371, -1.6612,  3.9163],
        [ 7.4999,  1.5616,  4.0768],
        [ 5.2128, -8.9407,  6.4601]])
&#39;&#39;&#39;</span>
mean<span class="token punctuation">,</span>std <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span>
c <span class="token operator">=</span> std <span class="token operator">*</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> mean

<span class="token comment"># 整数随机排列</span>
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token comment"># tensor([ 3, 17,  9, 19,  1, 18,  4, 13, 15, 12,  0, 16,  7, 11,  2,  5,  8, 10, 6, 14])</span>

<span class="token comment"># 特殊矩阵</span>
I <span class="token operator">=</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment">#单位矩阵</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>I<span class="token punctuation">)</span>
t <span class="token operator">=</span> torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#对角矩阵</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="索引切片" tabindex="-1"><a class="header-anchor" href="#索引切片" aria-hidden="true">#</a> 索引切片</h4><p>张量的索引切片方式和 numpy 几乎是一样的。切片时支持缺省参数和省略号</p><p>可以通过索引和切片对部分元素进行修改。</p><p>此外，对于不规则的切片提取,可以使用 <code>torch.index_select</code>, <code>torch.masked_select</code>, <code>torch.take</code></p><p>如果要通过修改张量的某些元素得到新的张量，可以使用 <code>torch.where</code>，<code>torch.masked_fill</code>，<code>torch.index_fill</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 均匀随机分布</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
minval<span class="token punctuation">,</span>maxval <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span>
t <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[4, 7, 0, 1, 3],
        [6, 4, 8, 4, 6],
        [3, 4, 0, 1, 2],
        [5, 6, 8, 1, 2],
        [6, 9, 3, 8, 4]], dtype=torch.int32)
&#39;&#39;&#39;</span>
<span class="token comment"># 第 0 行</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># tensor([4, 7, 0, 1, 3], dtype=torch.int32)</span>
<span class="token comment"># 第 1 行第 3 列</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># tensor(4, dtype=torch.int32)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># tensor(4, dtype=torch.int32)</span>
<span class="token comment"># 第 1 行至第 3 行</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 第 1 行至最后一行，第0列到最后一列每隔两列取一列</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment">#可以使用索引和切片修改部分元素</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
x<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 2.],[0., 0.]])</span>
<span class="token comment"># 省略号可以表示多个冒号</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>以上切片方式相对规则，对于不规则的切片提取,可以使用 torch.index_select, torch.take, torch.gather, torch.masked_select.</p><p>考虑班级成绩册的例子，有 4 个班级，每个班级 5 个学生，每个学生 7 门科目成绩。可以用一个 4×5×7 的张量来表示。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>minval<span class="token operator">=</span><span class="token number">0</span>
maxval<span class="token operator">=</span><span class="token number">100</span>
scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 抽取每个班级第 0 个学生，第2个学生，第 4 个学生的全部成绩</span>
torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment">#抽取每个班级第 0 个学生，第 2 个学生，第 4 个学生的第 1 门课程，第 3 门课程，第 6 门课程成绩</span>
q <span class="token operator">=</span> torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                   <span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 抽取第 0 个班级第 0 个学生的第 0 门课程，第 2 个班级的第 3 个学生的第 1 门课程，第 3 个班级的第 4 个学生第 6 门课程成绩</span>
<span class="token comment"># take 将输入看成一维数组，输出和 index 同形状</span>
s <span class="token operator">=</span> torch<span class="token punctuation">.</span>take<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 抽取分数大于等于 80 分的分数（布尔索引）</span>
<span class="token comment"># 结果是 1 维张量</span>
g <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>scores<span class="token operator">&gt;=</span><span class="token number">80</span><span class="token punctuation">)</span>


</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。</p><p>如果要通过修改张量的部分元素值得到新的张量，可以使用 torch.where,torch.index_fill 和 torch.masked_fill</p><ul><li>torch.where 可以理解为 if 的张量版本</li><li>torch.index_fill 的选取元素逻辑和 torch.index_select 相同</li><li>torch.masked_fill 的选取元素逻辑和 torch.masked_select 相同。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 如果分数大于60分，赋值成1，否则赋值成0</span>
ifpass <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>scores<span class="token operator">&gt;</span><span class="token number">60</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 将每个班级第 0 个学生，第 2 个学生，第 4 个学生的全部成绩赋值成满分</span>
torch<span class="token punctuation">.</span>index_fill<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>value <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment"># 等价于 scores.index_fill(dim = 1,index = torch.tensor([0,2,4]),value = 100)</span>

<span class="token comment"># 将分数小于 60 分的分数赋值成 60 分</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>scores<span class="token operator">&lt;</span><span class="token number">60</span><span class="token punctuation">,</span><span class="token number">60</span><span class="token punctuation">)</span>
<span class="token comment"># 等价于 b = scores.masked_fill(scores&lt;60,60)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="维度变换" tabindex="-1"><a class="header-anchor" href="#维度变换" aria-hidden="true">#</a> 维度变换</h4><p>维度变换相关函数主要有 torch.reshape(或者调用张量的 view 方法), torch.squeeze, torch.unsqueeze, torch.transpose</p><ul><li>torch.reshape 可以改变张量的形状</li><li>torch.reshape 可以改变张量的形状</li><li>torch.unsqueeze 可以增加维度</li><li>torch.transpose/torch.permute 可以交换维度。</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 张量的 view 方法有时候会调用失败，可以使用reshape方法。</span>

torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
minval<span class="token punctuation">,</span>maxval <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">255</span>
a <span class="token operator">=</span> <span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 原 tensor 改成 （3,6）形状的张量</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># torch.reshape(a,[3,6])</span>
<span class="token comment"># 改回成 [1,3,3,2] 形状的张量</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># b.view([1,3,3,2]) </span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果张量在某个维度上只有一个元素，利用 torch.squeeze 可以消除这个维度</p><p>torch.unsqueeze 的作用和 torch.squeeze 的作用相反</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
s <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 2.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token comment"># tensor([1., 2.])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([1, 2])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([2])</span>

<span class="token comment">#在第 0 维插入长度为1的一个维度</span>
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>s<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token comment"># tensor([1., 2.])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 2.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([2])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([1, 2])</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>torch.transpose 可以交换张量的维度，torch.transpose 常用于图片存储格式的变换上。permute 可以对维度顺序做重新编排</p><p>如果是二维的矩阵，通常会调用矩阵的转置方法 matrix.t()，等价于 torch.transpose(matrix,0,1)。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>minval<span class="token operator">=</span><span class="token number">0</span>
maxval<span class="token operator">=</span><span class="token number">255</span>
<span class="token comment"># Batch,Height,Width,Channel</span>
data <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([100, 256, 256, 4])</span>

<span class="token comment"># 转换成 Pytorch 默认的图片格式 Batch,Channel,Height,Width </span>
<span class="token comment"># 需要交换两次</span>
data_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>data<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>data_t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([100, 4, 256, 256])</span>
data_p <span class="token operator">=</span> torch<span class="token punctuation">.</span>permute<span class="token punctuation">(</span>data<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#对维度的顺序做重新编排</span>
data_p<span class="token punctuation">.</span>shape <span class="token comment"># torch.Size([100, 4, 256, 256])</span>

matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">)</span> <span class="token comment"># tensor([[1, 2, 3],[4, 5, 6]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#等价于torch.transpose(matrix,0,1) # tensor([[1, 4],[2, 5],[3, 6]])</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="合并分割" tabindex="-1"><a class="header-anchor" href="#合并分割" aria-hidden="true">#</a> 合并分割</h4><p>以用 torch.cat 方法和 torch.stack 方法将多个张量合并，可以用 torch.split 方法把一个张量分割成多个张量。</p><p>torch.cat 和 torch.stack 有略微的区别，torch.cat 是连接，不会增加维度，而 torch.stack 是堆叠，会增加维度。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9.0</span><span class="token punctuation">,</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">11.0</span><span class="token punctuation">,</span><span class="token number">12.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

abc_cat <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>abc_cat<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([6, 2])</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.],
        [11., 12.]])
&#39;&#39;&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>abc_cat<span class="token punctuation">)</span>

abc_stack <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># torch 中 dim 和 axis 参数名可以混用</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>abc_stack<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([3, 2, 2])</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[[ 1.,  2.],
         [ 3.,  4.]],
        [[ 5.,  6.],
         [ 7.,  8.]],
        [[ 9., 10.],
         [11., 12.]]])
&#39;&#39;&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>abc_stack<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[ 1.,  2.,  5.,  6.,  9., 10.],
        [ 3.,  4.,  7.,  8., 11., 12.]])
&#39;&#39;&#39;</span>
torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[[ 1.,  2.],
         [ 5.,  6.],
         [ 9., 10.]],
         
        [[ 3.,  4.],
         [ 7.,  8.],
         [11., 12.]]])
&#39;&#39;&#39;</span>
torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>torch.split 是 torch.cat 的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.],
        [11., 12.]])
&#39;&#39;&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>abc_cat<span class="token punctuation">)</span>
a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>abc_cat<span class="token punctuation">,</span>split_size_or_sections <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#每份2个进行分割</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 2.],[3., 4.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token comment"># tensor([[5., 6.],[7., 8.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token comment"># tensor([[ 9., 10.],[11., 12.]])</span>


p<span class="token punctuation">,</span>q<span class="token punctuation">,</span>r <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>abc_cat<span class="token punctuation">,</span>split_size_or_sections <span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#每份分别为[4,1,1]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 2.],[3., 4.],[5., 6.],[7., 8.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>q<span class="token punctuation">)</span> <span class="token comment"># tensor([[ 9., 10.]]) </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token comment"># tensor([[11., 12.]])</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="数学运算" tabindex="-1"><a class="header-anchor" href="#数学运算" aria-hidden="true">#</a> 数学运算</h4><p>张量数学运算主要有：标量运算，向量运算，矩阵运算，以及使用非常强大而灵活的爱因斯坦求和函数 torch.einsum 进行任意维的张量运算</p><h5 id="标量运算" tabindex="-1"><a class="header-anchor" href="#标量运算" aria-hidden="true">#</a> 标量运算</h5><p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。</p><p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。</p><p>标量运算符的特点是对张量实施 <strong>逐元素</strong> 运算。</p><p>有些标量运算符对常用的数学运算符进行了重载。并且支持类似 numpy 的广播特性。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np 

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">)</span>
a <span class="token operator">+</span> b <span class="token comment"># tensor(3.)</span>

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
a<span class="token operator">+</span>b  <span class="token comment"># 运算符重载 tensor([[ 6.,  8.],[ 4., 12.]])</span>
a<span class="token operator">-</span>b 
a<span class="token operator">*</span>b 
a<span class="token operator">/</span>b
a<span class="token operator">**</span><span class="token number">2</span>
a<span class="token operator">**</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
a<span class="token operator">%</span><span class="token number">3</span> <span class="token comment">#求模 # tensor([[1., 2.],[-0., 1.]])</span>
torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> rounding_mode<span class="token operator">=</span><span class="token string">&#39;floor&#39;</span><span class="token punctuation">)</span>  <span class="token comment"># 地板除法 tensor([[ 0.,  0.],[-1.,  0.]])</span>
a <span class="token operator">&gt;=</span> <span class="token number">2</span> <span class="token comment"># torch.ge(a,2)  #ge: greater_equal 缩写 tensor([[False,  True],[False,  True]])</span>
<span class="token punctuation">(</span>a<span class="token operator">&gt;=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&amp;</span><span class="token punctuation">(</span>a<span class="token operator">&lt;=</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[False,  True],[False, False]])</span>
<span class="token punctuation">(</span>a<span class="token operator">&gt;=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">|</span><span class="token punctuation">(</span>a<span class="token operator">&lt;=</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[True, True],[True, True]])</span>
a<span class="token operator">==</span><span class="token number">5</span> <span class="token comment">#　torch.eq(a,5)　tensor([[False, False],[False, False]])</span>
torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token comment"># tensor([[1.0000, 1.4142],[nan, 2.0000]])</span>


a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6.0</span><span class="token punctuation">,</span><span class="token number">7.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span> <span class="token comment"># tensor([5., 8.])</span>
torch<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span> <span class="token comment"># tensor([1., 6.])</span>

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.6</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#保留整数部分，四舍五入</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#保留整数部分，向下归整</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment">#保留整数部分，向上归整</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>trunc<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#保留整数部分，向0归整</span>

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.6</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>fmod<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 作除法取余数  tensor([ 0.6000, -0.7000])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>remainder<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 作除法取剩余的部分，结果恒正  tensor([0.6000, 1.3000])</span>

<span class="token comment"># 幅值裁剪</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">0.8</span><span class="token punctuation">,</span><span class="token number">100.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">20.0</span><span class="token punctuation">,</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token builtin">max</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
z <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token builtin">max</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token comment"># tensor([ 0.9000, -0.8000,  1.0000, -1.0000,  0.7000])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span> <span class="token comment"># tensor([  0.9000,  -0.8000,   1.0000, -20.0000,   0.7000])</span>

relu <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">5.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(5.)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="向量运算" tabindex="-1"><a class="header-anchor" href="#向量运算" aria-hidden="true">#</a> 向量运算</h5><p>原则上操作的张量至少是一维张量</p><p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 统计值</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 2., 3.],[4., 5., 6.],[7., 8., 9.]])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(45.)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 平均数 tensor(5.)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>prod<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#累乘 tensor(362880.)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>std<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment">#标准差 tensor(2.7386)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>var<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment">#方差 tensor(7.5000)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>median<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#中位数 tensor(5.)</span>

<span class="token comment"># 指定维度计算统计值</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">13</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># torch.return_types.max(values=tensor([ 9., 10., 11., 12.]),indices=tensor([2, 2, 2, 2]))</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># torch.return_types.max(values=tensor([ 4.,  8., 12.]),indices=tensor([3, 3, 3]))</span>

<span class="token comment"># cum 扫描 </span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment"># tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor([ 1, 3,  6, 10, 15, 21, 28, 36, 45])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cumprod<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor([ 1,  2,  6, 24, 120, 720, 5040,  40320, 362880])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cummax<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">)</span> <span class="token comment"># tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cummax<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>indices<span class="token punctuation">)</span> <span class="token comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cummin<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># torch.return_types.cummin(values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))</span>

<span class="token comment"># torch.sort 和 torch.topk 可以对张量排序</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span> <span class="token comment"># torch.return_types.topk(values=tensor([[9., 7., 8.],[5., 6., 4.]]),indices=tensor([[0, 0, 0],[2, 2, 2]])) </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span> <span class="token comment"># torch.return_types.topk(values=tensor([[9., 8.],[3., 2.],[6., 5.]]),indices=tensor([[0, 2],[1, 2],[1, 0]])) </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>a<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span> <span class="token comment"># torch.return_types.sort(values=tensor([[7., 8., 9.],[1., 2., 3.],[4., 5., 6.]]),indices=tensor([[1, 2, 0],[0, 2, 1],[2, 0, 1]])) </span>

<span class="token comment"># 利用 torch.topk 可以在 Pytorch 中实现 KNN 算法</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="矩阵运算" tabindex="-1"><a class="header-anchor" href="#矩阵运算" aria-hidden="true">#</a> 矩阵运算</h5><p>矩阵必须是 <strong>二维</strong> 的</p><p>矩阵运算包括：矩阵乘法，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 矩阵乘法</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a@b<span class="token punctuation">)</span>  <span class="token comment">#等价于 torch.matmul(a,b) 或 torch.mm(a,b) tensor([[2, 4],[6, 8]])</span>

<span class="token comment"># 高维张量的矩阵乘法在后面的维度上进行</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>a@b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape <span class="token comment"># torch.Size([5, 5, 4])</span>

<span class="token comment"># 矩阵转置</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[1., 3.],[2., 4.]])</span>

<span class="token comment"># 矩阵逆（逆矩阵），必须为浮点类型</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>inverse<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[-2.0000,  1.0000],[ 1.5000, -0.5000]])</span>

<span class="token comment"># 矩阵求 trace 秩</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(5.)</span>

<span class="token comment"># 矩阵求范数</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(5.4772)</span>

<span class="token comment"># 矩阵行列式</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>det<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(-2.)</span>

<span class="token comment"># 矩阵特征值和特征向量</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>eig<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> 
<span class="token triple-quoted-string string">&#39;&#39;&#39;
两个特征值分别是 -2.5+2.7839j, 2.5-2.7839j 
torch.return_types.linalg_eig(
eigenvalues=tensor([2.5000+2.7839j, 2.5000-2.7839j]),
eigenvectors=tensor([[0.2535-0.4706j, 0.2535+0.4706j],
        [0.8452+0.0000j, 0.8452-0.0000j]]))
&#39;&#39;&#39;</span>

<span class="token comment"># 矩阵 svd 分解</span>
<span class="token comment"># svd 分解可以将任意一个矩阵分解为一个正交矩阵 u,一个对角阵 s 和一个正交矩阵 v.t() 的乘积</span>
<span class="token comment"># svd 常用于矩阵压缩和降维</span>
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
u<span class="token punctuation">,</span>s<span class="token punctuation">,</span>v <span class="token operator">=</span> torch<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>v<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>

<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 
<span class="token keyword">print</span><span class="token punctuation">(</span>u@F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>@v<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#利用 svd 分解可以在Pytorch中实现主成分分析降维</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([[-0.2298,  0.8835,  0.4082],
        [-0.5247,  0.2408, -0.8165],
        [-0.8196, -0.4019,  0.4082]]) 

tensor([9.5255, 0.5143]) 

tensor([[-0.6196, -0.7849],
        [-0.7849,  0.6196]]) 

tensor([[1.0000, 2.0000],
        [3.0000, 4.0000],
        [5.0000, 6.0000]])
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="任意维张量运算" tabindex="-1"><a class="header-anchor" href="#任意维张量运算" aria-hidden="true">#</a> 任意维张量运算</h5><p>torch.einsum：爱因斯坦求和函数。torch.einsum 支持求导和反向传播，并且计算效率非常高</p><p>einsum 提供了一套既简洁又优雅的规则，可实现包括但不限于：内积，外积，矩阵乘法，转置和张量收缩（tensor contraction）等张量操作，熟练掌握 einsum 可以很方便的实现复杂的张量操作，而且不容易出错。</p>`,98),l=n("ul",null,[n("li",null,[n("p",null,[n("strong",null,"einsum 规则原理")]),n("p",null,"einsum 函数的思想起源于爱因斯坦，求和导致维度收缩，因此求和符号操作的指标总是只出现在公式的一边，例如在我们熟悉的矩阵乘法中"),n("p",{class:"katex-block"},[n("span",{class:"katex-display"},[n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[n("semantics",null,[n("mrow",null,[n("msub",null,[n("mi",null,"C"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"j")])]),n("mo",null,"="),n("munder",null,[n("mo",null,"∑"),n("mi",null,"k")]),n("mrow",null,[n("msub",null,[n("mi",null,"A"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"k")])]),n("msub",null,[n("mi",null,"B"),n("mrow",null,[n("mi",null,"k"),n("mi",null,"j")])])])]),n("annotation",{encoding:"application/x-tex"}," C_{ij} = \\sum_{k}{A_{ik}B_{kj}} ")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3117em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"ij")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"2.3521em","vertical-align":"-1.3021em"}}),n("span",{class:"mop op-limits"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"1.05em"}},[n("span",{style:{top:"-1.8479em","margin-left":"0em"}},[n("span",{class:"pstrut",style:{height:"3.05em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),n("span",{style:{top:"-3.05em"}},[n("span",{class:"pstrut",style:{height:"3.05em"}}),n("span",null,[n("span",{class:"mop op-symbol large-op"},"∑")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"1.3021em"}},[n("span")])])])]),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord"},[n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"A"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"ik")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.05017em"}},"B"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.0502em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"kj")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])])])])])])])]),n("p",null,"k 这个下标被求和了，求和导致了这个维度的消失，所以它只出现在右边而不出现在左边"),n("p",null,"这种只出现在张量公式的一边的下标被称之为哑指标，反之为自由指标"),n("p",null,"这种只出现在一边的哑指标一定是被求和求掉的，干脆把对应的∑∑求和符号省略"),n("p",null,"这就是爱因斯坦求和约定："),n("p",null,[n("strong",null,"只出现在公式一边的指标叫做哑指标，针对哑指标的 ∑ 求和符号可以省略")]),n("p",{class:"katex-block"},[n("span",{class:"katex-display"},[n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[n("semantics",null,[n("mrow",null,[n("msub",null,[n("mi",null,"C"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"j")])]),n("mo",null,"="),n("mrow",null,[n("msub",null,[n("mi",null,"A"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"k")])]),n("msub",null,[n("mi",null,"B"),n("mrow",null,[n("mi",null,"k"),n("mi",null,"j")])])])]),n("annotation",{encoding:"application/x-tex"}," C_{ij} = {A_{ik}B_{kj}} ")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3117em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"ij")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),n("span",{class:"mord"},[n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"A"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"ik")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.05017em"}},"B"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.0502em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"kj")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])])])])])])])]),n("p",null,"这个公式表达的含义如下:"),n("ul",null,[n("li",null,[n("p",null,[s("C 这个张量的第 i 行第j列由 𝐴 这个张量的第i行第 k 列和 𝐵 这个张量的第 k 行第j列相乘，这样得到的是一个三维张量 𝐷, 其元素为 𝐷"),n("sub",null,"𝑖𝑘𝑗"),s("，然后对 𝐷 在维度 k 上求和得到")])]),n("li",null,[n("p",null,"公式展现形式中除了省去了求和符号，还省去了乘法符号")]),n("li",null,[n("p",null,"借鉴爱因斯坦求和约定表达张量运算的清爽整洁，numpy、tensorflow 和 torch 等库中都引入了 einsum 这个函数")]),n("li",null,[n("p",null,"上述矩阵乘法可以被einsum这个函数表述成"),n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[s("C "),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ik,kj->ij"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},")"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"})])]),n("p",null,"这个函数的规则原理非常简洁"),n("ol",null,[n("li",null,"用元素计算公式来表达张量运算"),n("li",null,"只出现在元素计算公式箭头左边的指标叫做哑指标"),n("li",null,"省略元素计算公式中对哑指标的求和符号")]),n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token keyword"},"import"),s(` torch 

A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("tensor"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"["),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"3"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"4.0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("tensor"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"["),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"6"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"7"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"8.0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),s(`

C1 `),n("span",{class:"token operator"},"="),s(` A@B
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),s("C1"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# tensor([[19., 22.],[43., 50.]])"),s(`

C2 `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ik,kj->ij"'),n("span",{class:"token punctuation"},","),n("span",{class:"token punctuation"},"["),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),s("C2"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# tensor([[19., 22.],[43., 50.]])"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])])])])]),n("li",null,[n("p",null,[n("strong",null,"einsum 基础范例")]),n("p",null,"einsum 这个函数的精髓实际上是第一条:"),n("ul",null,[n("li",null,"用元素计算公式来表达张量运算"),n("li",null,"绝大部分张量运算都可以用元素计算公式很方便地来表达，这也是它为什么会那么神通广大")]),n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token comment"},"# 例1，张量转置"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"3"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`

`),n("span",{class:"token comment"},"# B = torch.permute(A,[0,2,1])"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ijk->ikj"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},")"),s(` 

`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([3, 4, 5])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([3, 5, 4])"),s(`

`),n("span",{class:"token comment"},"# 例2，取对角元"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# B = torch.diagonal(A)"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ii->i"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([5, 5])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([5])"),s(`

`),n("span",{class:"token comment"},"# 例3，求和降维"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# B = torch.sum(A,1)"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ij->i"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([4, 5])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([4])"),s(`

`),n("span",{class:"token comment"},"# 例4，哈达玛积"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# C=A*B"),s(`
C `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ij,ij->ij"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},","),s(" B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([5, 5]) torch.Size([5, 5])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("C"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([5, 5])"),s(`

`),n("span",{class:"token comment"},"# 例5，向量内积"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# C=torch.dot(A,B)"),s(`
C `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"i,i->"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},","),s(" B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([10]) torch.Size([10])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("C"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([])"),s(`

`),n("span",{class:"token comment"},"# 例6，向量外积(类似笛卡尔积)"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# C = torch.outer(A,B)"),s(`
C `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"i,j->ij"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},","),s(" B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([10]) torch.Size([5])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("C"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([10, 5])"),s(`

`),n("span",{class:"token comment"},"# 例7，矩阵乘法"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},")"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"6"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# C = torch.matmul(A,B)"),s(`
C `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ik,kj->ij"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},","),s(" B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([5, 4]) torch.Size([4, 6])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("C"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([5, 6])"),s(`


`),n("span",{class:"token comment"},"#例8，张量缩并"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"3"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(`
B `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"3"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"6"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# C = torch.tensordot(A,B,dims=[(0,1),(1,0)])"),s(`
C `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"ijk,jih->kh"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},","),s("B"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"before:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},","),s(" B"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# before: torch.Size([3, 4, 5]) torch.Size([4, 3, 6])"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"after:"'),n("span",{class:"token punctuation"},","),s("C"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# after: torch.Size([5, 6])"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])])]),n("li",null,[n("p",null,[n("strong",null,"einsum 高级范例")]),n("p",null,"einsum 可用于超过两个张量的计算"),n("p",null,"例如：双线性变换。这是向量内积的一种扩展，一种常用的注意力机制实现方式"),n("p",null,[s("不考虑 batch 维度时，双线性变换的公式："),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"A"),n("mo",null,"="),n("mi",null,"q"),n("mi",null,"W"),n("msup",null,[n("mi",null,"k"),n("mi",null,"T")])]),n("annotation",{encoding:"application/x-tex"},"A=qWk^T")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6833em"}}),n("span",{class:"mord mathnormal"},"A"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.0358em","vertical-align":"-0.1944em"}}),n("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.8413em"}},[n("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])])])])])]),n("p",null,[s("考虑 batch 维度时，无法用矩阵乘法表示，可以用元素计算公式表达："),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("msub",null,[n("mi",null,"A"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"j")])]),n("mo",null,"="),n("msub",null,[n("mo",null,"∑"),n("mi",null,"k")]),n("msub",null,[n("mo",null,"∑"),n("mi",null,"l")]),n("mrow",null,[n("msub",null,[n("mi",null,"Q"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"k")])]),n("msub",null,[n("mi",null,"W"),n("mrow",null,[n("mi",null,"j"),n("mi",null,"k"),n("mi",null,"l")])]),n("msub",null,[n("mi",null,"K"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"l")])])]),n("mo",null,"="),n("msub",null,[n("mi",null,"Q"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"k")])]),n("msub",null,[n("mi",null,"W"),n("mrow",null,[n("mi",null,"j"),n("mi",null,"k"),n("mi",null,"l")])]),n("msub",null,[n("mi",null,"K"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"l")])])]),n("annotation",{encoding:"application/x-tex"},"A_{ij}=\\sum_{k}\\sum_{l}{Q_{ik}W_{jkl}K_{il}}=Q_{ik}W_{jkl}K_{il}")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"A"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3117em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"ij")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.0497em","vertical-align":"-0.2997em"}}),n("span",{class:"mop"},[n("span",{class:"mop op-symbol small-op",style:{position:"relative",top:"0em"}},"∑"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.1864em"}},[n("span",{style:{top:"-2.4003em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2997em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mop"},[n("span",{class:"mop op-symbol small-op",style:{position:"relative",top:"0em"}},"∑"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.1864em"}},[n("span",{style:{top:"-2.4003em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.01968em"}},"l")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2997em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),n("span",{class:"mord"},[n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"Q"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"ik")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"jk"),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.01968em"}},"l")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])]),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"i"),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.01968em"}},"l")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"Q"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"ik")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"jk"),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.01968em"}},"l")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])]),n("span",{class:"mord"},[n("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3361em"}},[n("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"i"),n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.01968em"}},"l")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])])])])])]),n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token comment"},"# 例9，bilinear 注意力机制"),s(`

`),n("span",{class:"token comment"},"#====不考虑 batch 维度===="),s(`
q `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# query_features"),s(`
k `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# key_features"),s(`
W `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# out_features,query_features,key_features"),s(`
b `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# out_features"),s(`

`),n("span",{class:"token comment"},"# a = q@W@k.t()+b  "),s(`
a `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("bilinear"),n("span",{class:"token punctuation"},"("),s("q"),n("span",{class:"token punctuation"},","),s("k"),n("span",{class:"token punctuation"},","),s("W"),n("span",{class:"token punctuation"},","),s("b"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"a.shape:"'),n("span",{class:"token punctuation"},","),s("a"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# a.shape: torch.Size([5])"),s(`


`),n("span",{class:"token comment"},"#=====考虑 batch 维度===="),s(`
Q `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"8"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s("    "),n("span",{class:"token comment"},"#batch_size,query_features"),s(`
K `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"8"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s("    "),n("span",{class:"token comment"},"#batch_size,key_features"),s(`
W `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"#out_features,query_features,key_features"),s(`
b `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s("       "),n("span",{class:"token comment"},"#out_features"),s(`

`),n("span",{class:"token comment"},"#A = torch.bilinear(Q,K,W,b)"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},"'bq,oqk,bk->bo'"),n("span",{class:"token punctuation"},","),s("Q"),n("span",{class:"token punctuation"},","),s("W"),n("span",{class:"token punctuation"},","),s("K"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"+"),s(` b
`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"A.shape:"'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# A.shape: torch.Size([8, 5])"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])]),n("p",null,"也可以用 einsum 来实现更常见的 scaled-dot-product 形式的 Attention"),n("p",null,[s("不考虑 batch 维度时，scaled-dot-product 形式的 Attention 用矩阵乘法公式表示："),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"a"),n("mo",null,"="),n("mi",null,"s"),n("mi",null,"o"),n("mi",null,"f"),n("mi",null,"t"),n("mi",null,"m"),n("mi",null,"a"),n("mi",null,"x"),n("mo",{stretchy:"false"},"("),n("mfrac",null,[n("mrow",null,[n("mi",null,"a"),n("msup",null,[n("mi",null,"k"),n("mi",null,"T")])]),n("msub",null,[n("mi",null,"d"),n("mi",null,"k")])]),n("mo",{stretchy:"false"},")")]),n("annotation",{encoding:"application/x-tex"},"a=softmax(\\frac{ak^{T}}{d_k} )")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.4306em"}}),n("span",{class:"mord mathnormal"},"a"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.4882em","vertical-align":"-0.4509em"}}),n("span",{class:"mord mathnormal"},"so"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),n("span",{class:"mord mathnormal"},"t"),n("span",{class:"mord mathnormal"},"ma"),n("span",{class:"mord mathnormal"},"x"),n("span",{class:"mopen"},"("),n("span",{class:"mord"},[n("span",{class:"mopen nulldelimiter"}),n("span",{class:"mfrac"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"1.0374em"}},[n("span",{style:{top:"-2.655em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"d"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3448em"}},[n("span",{style:{top:"-2.3488em","margin-left":"0em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.1512em"}},[n("span")])])])])])])])]),n("span",{style:{top:"-3.23em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),n("span",{style:{top:"-3.394em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"a"),n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.9191em"}},[n("span",{style:{top:"-2.931em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])])])])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.4509em"}},[n("span")])])])]),n("span",{class:"mclose nulldelimiter"})]),n("span",{class:"mclose"},")")])])])]),n("p",null,[s("考虑 batch 维度时，无法用矩阵乘法表示，可以用元素计算公式表达 "),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("msub",null,[n("mi",null,"A"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"j")])]),n("mo",null,"="),n("mi",null,"s"),n("mi",null,"o"),n("mi",null,"f"),n("mi",null,"t"),n("mi",null,"m"),n("mi",null,"a"),n("mi",null,"x"),n("mo",{stretchy:"false"},"("),n("mfrac",null,[n("mrow",null,[n("msub",null,[n("mi",null,"Q"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"n")])]),n("msub",null,[n("mi",null,"K"),n("mrow",null,[n("mi",null,"i"),n("mi",null,"j"),n("mi",null,"n")])])]),n("msub",null,[n("mi",null,"d"),n("mi",null,"k")])]),n("mo",{stretchy:"false"},")")]),n("annotation",{encoding:"application/x-tex"},"A_{ij}=softmax(\\frac{Q_{in}K_{ijn}}{d_k})")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"A"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3117em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"ij")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2861em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.4365em","vertical-align":"-0.4509em"}}),n("span",{class:"mord mathnormal"},"so"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),n("span",{class:"mord mathnormal"},"t"),n("span",{class:"mord mathnormal"},"ma"),n("span",{class:"mord mathnormal"},"x"),n("span",{class:"mopen"},"("),n("span",{class:"mord"},[n("span",{class:"mopen nulldelimiter"}),n("span",{class:"mfrac"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.9857em"}},[n("span",{style:{top:"-2.655em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"d"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3448em"}},[n("span",{style:{top:"-2.3488em","margin-left":"0em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.1512em"}},[n("span")])])])])])])])]),n("span",{style:{top:"-3.23em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),n("span",{style:{top:"-3.5073em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"Q"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3281em"}},[n("span",{style:{top:"-2.357em","margin-left":"0em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"in")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.143em"}},[n("span")])])])])]),n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"K"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3281em"}},[n("span",{style:{top:"-2.357em","margin-left":"-0.0715em","margin-right":"0.0714em"}},[n("span",{class:"pstrut",style:{height:"2.5em"}}),n("span",{class:"sizing reset-size3 size1 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"ijn")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.2819em"}},[n("span")])])])])])])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.4509em"}},[n("span")])])])]),n("span",{class:"mclose nulldelimiter"})]),n("span",{class:"mclose"},")")])])])]),n("div",{class:"language-python line-numbers-mode","data-ext":"py"},[n("pre",{class:"language-python"},[n("code",null,[n("span",{class:"token comment"},"# 例10，scaled-dot-product 注意力机制"),s(`

`),n("span",{class:"token comment"},"#====不考虑 batch 维度===="),s(`
q `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s("  "),n("span",{class:"token comment"},"# query_features"),s(`
k `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"6"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"# key_size, key_features"),s(`

d_k `),n("span",{class:"token operator"},"="),s(" k"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},"["),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},"]"),s(`
a `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("softmax"),n("span",{class:"token punctuation"},"("),s("q@k"),n("span",{class:"token punctuation"},"."),s("t"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("d_k"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(` 

`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"a.shape="'),n("span",{class:"token punctuation"},","),s("a"),n("span",{class:"token punctuation"},"."),s("shape "),n("span",{class:"token punctuation"},")"),s(`

`),n("span",{class:"token comment"},"#====考虑 batch 维度===="),s(`
Q `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"8"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s("  "),n("span",{class:"token comment"},"#batch_size,query_features"),s(`
K `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"8"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"6"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"#batch_size,key_size,key_features"),s(`

d_k `),n("span",{class:"token operator"},"="),s(" K"),n("span",{class:"token punctuation"},"."),s("shape"),n("span",{class:"token punctuation"},"["),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},"]"),s(`
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("softmax"),n("span",{class:"token punctuation"},"("),s("torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"in,ijn->ij"'),n("span",{class:"token punctuation"},","),s("Q"),n("span",{class:"token punctuation"},","),s("K"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("d_k"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(` 

`),n("span",{class:"token keyword"},"print"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"A.shape="'),n("span",{class:"token punctuation"},","),s("A"),n("span",{class:"token punctuation"},"."),s("shape "),n("span",{class:"token punctuation"},")"),s(`

`),n("span",{class:"token comment"},"#性能测试"),s(`

`),n("span",{class:"token comment"},"#=====考虑 batch 维度===="),s(`
Q `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"80"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"100"),n("span",{class:"token punctuation"},")"),s("    "),n("span",{class:"token comment"},"#batch_size,query_features"),s(`
K `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"80"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"100"),n("span",{class:"token punctuation"},")"),s("    "),n("span",{class:"token comment"},"#batch_size,key_features"),s(`
W `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"50"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"100"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"100"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token comment"},"#out_features,query_features,key_features"),s(`
b `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("randn"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"50"),n("span",{class:"token punctuation"},")"),s("       "),n("span",{class:"token comment"},"#out_features"),s(`

`),n("span",{class:"token operator"},"%"),n("span",{class:"token operator"},"%"),s(`timeit 
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("bilinear"),n("span",{class:"token punctuation"},"("),s("Q"),n("span",{class:"token punctuation"},","),s("K"),n("span",{class:"token punctuation"},","),s("W"),n("span",{class:"token punctuation"},","),s("b"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token comment"},"# 1.83 ms ± 78.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"),s(`

`),n("span",{class:"token operator"},"%"),n("span",{class:"token operator"},"%"),s(`timeit 
A `),n("span",{class:"token operator"},"="),s(" torch"),n("span",{class:"token punctuation"},"."),s("einsum"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},"'bq,oqk,bk->bo'"),n("span",{class:"token punctuation"},","),s("Q"),n("span",{class:"token punctuation"},","),s("W"),n("span",{class:"token punctuation"},","),s("K"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"+"),s(` b
`),n("span",{class:"token comment"},"# 636 µs ± 27.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)、"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])])])],-1),i=a(`<h5 id="广播机制" tabindex="-1"><a class="header-anchor" href="#广播机制" aria-hidden="true">#</a> 广播机制</h5><p>Pytorch 的广播规则和 numpy 是一样的:</p><ol><li>张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样</li><li>两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的</li><li>两个张量在所有维度上都是相容的，它们就能使用广播</li><li>广播之后，每个维度的长度将取两个张量在该维度长度的较大值</li><li>在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了 <strong>复制</strong></li></ol><p>torch.broadcast_tensors 可以将多个张量根据广播规则转换成相同的维度</p><p>维度扩展允许的操作有两种：</p><ol><li>增加一个维度</li><li>对长度为 1 的维度进行复制扩展</li></ol><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b <span class="token operator">+</span> a<span class="token punctuation">)</span>  <span class="token comment"># tensor([[1, 2, 3],[2, 3, 4],[3, 4, 5]])</span>
torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment"># tensor([[1, 2, 3],[2, 3, 4],[3, 4, 5]])</span>
a_broad<span class="token punctuation">,</span>b_broad <span class="token operator">=</span> torch<span class="token punctuation">.</span>broadcast_tensors<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a_broad<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[1, 2, 3],[1, 2, 3],[1, 2, 3]]) </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b_broad<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[0, 0, 0],[1, 1, 1],[2, 2, 2]]) </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a_broad <span class="token operator">+</span> b_broad<span class="token punctuation">)</span> <span class="token comment"># tensor([[1, 2, 3],[2, 3, 4],[3, 4, 5]])</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="nn-functional-和-nn-module" tabindex="-1"><a class="header-anchor" href="#nn-functional-和-nn-module" aria-hidden="true">#</a> nn.functional 和 nn.Module</h4><h5 id="简介" tabindex="-1"><a class="header-anchor" href="#简介" aria-hidden="true">#</a> 简介</h5><p>前面我们介绍了 Pytorch 的张量的结构操作和数学运算中的一些常用 API</p><p>利用这些张量的 API 我们可以构建出神经网络相关的组件(如激活函数，模型层，损失函数)</p><p>Pytorch 和神经网络相关的功能组件大多都封装在 <strong>torch.nn</strong> 模块下</p><p>这些功能组件的绝大部分既有函数形式实现，也有类形式实现。</p><p>其中nn.functional（一般引入后改名为 <strong>F</strong>）有各种功能组件的函数实现</p><ul><li>激活函数 <ul><li>F.relu</li><li>F.sigmoid</li><li>F.tanh</li><li>F.softmax</li></ul></li><li>模型层 <ul><li>F.linear</li><li>F.conv2d</li><li>F.max_pool2d</li><li>F.dropout2d</li><li>F.embedding</li></ul></li><li>损失函数 <ul><li>F.binary_cross_entropy</li><li>F.mse_loss</li><li>F.cross_entropy</li></ul></li></ul><p>为了便于对参数进行管理，一般通过继承 nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下。例如：</p><ul><li>激活函数 <ul><li>nn.ReLU</li><li>nn.Sigmoid</li><li>nn.Tanh</li><li>nn.Softmax</li></ul></li><li>模型层 <ul><li>nn.Linear</li><li>nn.Conv2d</li><li>nn.MaxPool2d</li><li>nn.Dropout2d</li><li>nn.Embedding</li></ul></li><li>损失函数 <ul><li>nn.BCELoss</li><li>nn.MSELoss</li><li>nn.CrossEntropyLoss</li></ul></li></ul><p>实际上 nn.Module 除了可以管理其引用的各种参数，还可以管理其引用的子模块，功能十分强大</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F 
torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># tensor(0.)</span>
F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># tensor(0.)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="使用-nn-module-来管理参数-配合-nn-parameter-使用" tabindex="-1"><a class="header-anchor" href="#使用-nn-module-来管理参数-配合-nn-parameter-使用" aria-hidden="true">#</a> 使用 nn.Module 来管理参数(配合 nn.Parameter 使用)</h5><p>在 Pytorch 中，模型的参数是需要被优化器训练的，因此，通常要设置参数为 requires_grad = True 的张量。</p><p>同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。</p><p>Pytorch 一般将参数用 nn.Parameter 来表示，并且用 nn.Module 来管理其结构下的所有参数。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn 
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional  <span class="token keyword">as</span> F

torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># tensor([[0.1829, 0.0693],[0.0767, 1.2441]], requires_grad=True)</span>

<span class="token comment"># nn.Parameter 具有 requires_grad = True 属性</span>
w <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token comment"># Parameter containing:tensor([[-0.8092, -0.8830],[ 1.6357, -0.1740]], requires_grad=True)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># True</span>

<span class="token comment"># nn.ParameterList 可以将多个 nn.Parameter 组成一个列表</span>
params_list <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
ParameterList(
    (0): Parameter containing: [torch.float32 of size 8x1]
    (1): Parameter containing: [torch.float32 of size 8x2]
)
&#39;&#39;&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>params_list<span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span>params_list<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># True</span>

<span class="token comment"># nn.ParameterDict 可以将多个 nn.Parameter 组成一个字典</span>
params_dict <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               <span class="token string">&quot;b&quot;</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
ParameterDict(
    (a): Parameter containing: [torch.FloatTensor of size 2x2]
    (b): Parameter containing: [torch.FloatTensor of size 2]
)
&#39;&#39;&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>params_dict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>params_dict<span class="token punctuation">[</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># True</span>

<span class="token comment"># 可以用 Module 将它们管理起来</span>
<span class="token comment"># module.parameters() 返回一个生成器，包括其结构下的所有 parameters</span>
module <span class="token operator">=</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">(</span><span class="token punctuation">)</span>
module<span class="token punctuation">.</span>w <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
module<span class="token punctuation">.</span>params_list <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
module<span class="token punctuation">.</span>params_dict <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               <span class="token string">&quot;b&quot;</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>

num_param <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> module<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>
    num_param <span class="token operator">=</span> num_param <span class="token operator">+</span> <span class="token number">1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;number of Parameters =&quot;</span><span class="token punctuation">,</span>num_param<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
(&#39;w&#39;, Parameter containing:
tensor([[-1.2390,  0.3316],
        [-0.4232, -0.0090]], requires_grad=True)) 

(&#39;params_list.0&#39;, Parameter containing:
tensor([[0.8785],
        [0.6456],
        [0.4697],
        [0.8962],
        [0.1122],
        [0.4837],
        [0.8089],
        [0.0515]], requires_grad=True)) 

(&#39;params_list.1&#39;, Parameter containing:
tensor([[0.7440, 0.5626],
        [0.2430, 0.0113],
        [0.5884, 0.0815],
        [0.7125, 0.4120],
        [0.7275, 0.1608],
        [0.4658, 0.0085],
        [0.8578, 0.7290],
        [0.0327, 0.2239]], requires_grad=True)) 

(&#39;params_dict.a&#39;, Parameter containing:
tensor([[0.6698, 0.5646],
        [0.2482, 0.8258]], requires_grad=True)) 

(&#39;params_dict.b&#39;, Parameter containing:
tensor([0., 0.], requires_grad=True)) 

number of Parameters = 5
&#39;&#39;&#39;</span>


<span class="token comment"># 实践当中，一般通过继承 nn.Module 来构建模块类，并将所有含有需要学习的参数的部分放在 **构造函数** 中。</span>
<span class="token comment"># 以下范例为 Pytorch 中 nn.Linear 的源码的简化版本</span>
<span class="token comment"># 可以看到它将需要学习的参数放在了 __init__ 构造函数中，并在 forward 中调用 F.linear 函数来实现计算逻辑。</span>

<span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    __constants__ <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;in_features&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;out_features&#39;</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features
        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> bias<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">&#39;bias&#39;</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
   
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="使用-nn-module-来管理子模块" tabindex="-1"><a class="header-anchor" href="#使用-nn-module-来管理子模块" aria-hidden="true">#</a> 使用 nn.Module 来管理子模块</h5><p>一般情况下，我们都很少直接使用 nn.Parameter 来定义参数构建模型，而是通过一些拼装一些常用的模型层来构造模型</p><p>这些模型层也是继承自 nn.Module 的对象，本身也包括参数，属于我们要定义的模块的子模块</p><p>nn.Module 提供了一些方法可以管理这些子模块</p><ul><li>children() 方法: 返回生成器，包括模块下的所有子模块。</li><li>named_children() 方法：返回一个生成器，包括模块下的所有子模块，以及它们的名字。</li><li>modules() 方法：返回一个生成器，包括模块下的所有各个层级的模块，包括模块本身。</li><li>named_modules() 方法：返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。</li></ul><p>其中 chidren() 方法和 named_children() 方法较多使用。</p><p>modules() 方法和 named_modules() 方法较少使用，其功能可以通过多个 named_children() 的嵌套使用实现。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings <span class="token operator">=</span> <span class="token number">10000</span><span class="token punctuation">,</span>embedding_dim <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>padding_idx <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;conv_1&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>out_channels <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;pool_1&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool1d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;relu_1&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;conv_2&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>out_channels <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;pool_2&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool1d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;relu_2&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;flatten&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">&quot;linear&quot;</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">6144</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> y
    
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

i <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> child <span class="token keyword">in</span> net<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    i<span class="token operator">+=</span><span class="token number">1</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>child<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;child number&quot;</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
Embedding(10000, 3, padding_idx=1) 

Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
) 

Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
) 

child number 3
&#39;&#39;&#39;</span>

i <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span>child <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    i<span class="token operator">+=</span><span class="token number">1</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">&quot;:&quot;</span><span class="token punctuation">,</span>child<span class="token punctuation">,</span><span class="token string">&quot;\\n&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;child number&quot;</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
embedding : Embedding(10000, 3, padding_idx=1) 

conv : Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
) 

dense : Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
) 

child number 3
&#39;&#39;&#39;</span>

i <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> module <span class="token keyword">in</span> net<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    i<span class="token operator">+=</span><span class="token number">1</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;module number:&quot;</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
Net(
  (embedding): Embedding(10000, 3, padding_idx=1)
  (conv): Sequential(
    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_1): ReLU()
    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_2): ReLU()
  )
  (dense): Sequential(
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (linear): Linear(in_features=6144, out_features=1, bias=True)
  )
)
Embedding(10000, 3, padding_idx=1)
Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
)
Conv1d(3, 16, kernel_size=(5,), stride=(1,))
MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
ReLU()
Conv1d(16, 128, kernel_size=(2,), stride=(1,))
MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
ReLU()
Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
)
Flatten(start_dim=1, end_dim=-1)
Linear(in_features=6144, out_features=1, bias=True)
module number: 12
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>下面我们通过 named_children 方法找到 embedding 层，并将其参数设置为不可训练(相当于冻结 embedding 层)。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>children_dict <span class="token operator">=</span> <span class="token punctuation">{</span>name<span class="token punctuation">:</span>module <span class="token keyword">for</span> name<span class="token punctuation">,</span>module <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39;
{&#39;embedding&#39;: Embedding(10000, 3, padding_idx=1), &#39;conv&#39;: Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
), &#39;dense&#39;: Sequential(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=6144, out_features=1, bias=True)
)}
Embedding(10000, 3, padding_idx=1)
&#39;&#39;&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>children_dict<span class="token punctuation">)</span>
embedding <span class="token operator">=</span> children_dict<span class="token punctuation">[</span><span class="token string">&quot;embedding&quot;</span><span class="token punctuation">]</span>
embedding<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment">#冻结其参数</span>

<span class="token comment">#可以看到其第一层的参数已经不可以被训练了。</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> embedding<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># False</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 30000</span>
    
<span class="token keyword">from</span> torchkeras <span class="token keyword">import</span> summary
summary<span class="token punctuation">(</span>net<span class="token punctuation">,</span>input_shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>input_dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment"># 不可训练参数数量增加</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
--------------------------------------------------------------------------
Layer (type)                            Output Shape              Param #
==========================================================================
Embedding-1                             [-1, 200, 3]               30,000
Conv1d-2                               [-1, 16, 196]                  256
MaxPool1d-3                             [-1, 16, 98]                    0
ReLU-4                                  [-1, 16, 98]                    0
Conv1d-5                               [-1, 128, 97]                4,224
MaxPool1d-6                            [-1, 128, 48]                    0
ReLU-7                                 [-1, 128, 48]                    0
Flatten-8                                 [-1, 6144]                    0
Linear-9                                     [-1, 1]                6,145
==========================================================================
Total params: 40,625
Trainable params: 10,625
Non-trainable params: 30,000
--------------------------------------------------------------------------
Input size (MB): 0.000763
Forward/backward pass size (MB): 0.287788
Params size (MB): 0.154972
Estimated Total Size (MB): 0.443523
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_5-2、中阶-api" tabindex="-1"><a class="header-anchor" href="#_5-2、中阶-api" aria-hidden="true">#</a> 5.2、中阶 API</h3><h4 id="dataset-和-dateloader" tabindex="-1"><a class="header-anchor" href="#dataset-和-dateloader" aria-hidden="true">#</a> Dataset 和 DateLoader</h4><p>Pytorch 通常使用 Dataset 和 DataLoader 这两个工具类来构建数据管道</p><ul><li><p>Dataset 定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。</p></li><li><p>DataLoader 定义了按 batch 加载数据集的方法，它是一个实现了<code>__iter__</code>方法的可迭代对象，每次迭代输出一个 batch 的数据</p><p>DataLoader 能够控制 batch 的大小，batch 中元素的采样方法，以及将 batch 结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。</p></li></ul><p>在绝大部分情况下，用户只需实现 Dataset 的 <code>__len__</code> 方法和 <code>__getitem__</code> 方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载</p>`,39),u=[c,l,i];function r(k,m){return p(),e("div",null,u)}const b=t(o,[["render",r],["__file","Pytorch-20.html.vue"]]);export{b as default};
